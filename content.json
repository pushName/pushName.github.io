{"meta":{"title":"阿智的个人博客","subtitle":"--进阶之路","description":"一个学计算机的博客 大数据 个人笔记 优质的创作社区  错误记录","author":"阿智","url":"http://www.studyz.club","root":"/"},"pages":[{"title":"关于","date":"2019-06-19T05:45:13.274Z","updated":"2019-06-19T05:45:13.274Z","comments":false,"path":"about/index.html","permalink":"http://www.studyz.club/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"书单","date":"2019-05-21T15:38:30.855Z","updated":"2019-05-13T09:35:35.678Z","comments":true,"path":"books/index.html","permalink":"http://www.studyz.club/books/index.html","excerpt":"","text":""},{"title":"404 Not Found：该页无法显示","date":"2019-05-21T15:38:30.843Z","updated":"2019-05-13T03:03:12.015Z","comments":false,"path":"/404.html","permalink":"http://www.studyz.club/404.html","excerpt":"","text":""},{"title":"Repositories","date":"2019-06-19T05:27:47.186Z","updated":"2019-06-19T05:27:47.186Z","comments":false,"path":"repository/index.html","permalink":"http://www.studyz.club/repository/index.html","excerpt":"","text":""},{"title":"分类","date":"2019-06-19T05:07:06.000Z","updated":"2019-06-19T06:32:15.153Z","comments":true,"path":"categories/index.html","permalink":"http://www.studyz.club/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-06-19T05:42:08.000Z","updated":"2019-06-19T06:28:45.591Z","comments":true,"path":"tags/index.html","permalink":"http://www.studyz.club/tags/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2019-05-21T15:38:30.867Z","updated":"2019-05-13T03:03:12.023Z","comments":true,"path":"links/index.html","permalink":"http://www.studyz.club/links/index.html","excerpt":"","text":""}],"posts":[{"title":"算法练习-二叉树涂色问题","slug":"算法练习","date":"2021-03-04T17:58:17.147Z","updated":"2021-03-04T18:11:36.474Z","comments":true,"path":"posts/b7e858de/","link":"","permalink":"http://www.studyz.club/posts/b7e858de/","excerpt":"","text":"二叉树涂色问题 对二叉树进行涂色，涂一个节点会同时改变自身节点，父节点和两个子节点的颜色计算给整个二叉树上色，需要最少的上色节点数量（实现语言不限） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156package suanfa;import java.util.ArrayList;import java.util.LinkedList;import java.util.List;import java.util.Queue;public class Test4 &#123; public static void main(String[] args) &#123; &#x2F;&#x2F; TODO 自动生成的方法存根 &#x2F;&#x2F; 创建一个二叉树 BinaryTree binaryTree &#x3D; new BinaryTree(); &#x2F;&#x2F; 创建需要的节点 &#x2F;&#x2F; 建立二叉树 TreeNode root &#x3D; new TreeNode(1, &quot;A&quot;); TreeNode node2 &#x3D; new TreeNode(2, &quot;B&quot;); TreeNode node3 &#x3D; new TreeNode(3, &quot;C&quot;); TreeNode node4 &#x3D; new TreeNode(4, &quot;D&quot;); TreeNode node5 &#x3D; new TreeNode(5, &quot;E&quot;); TreeNode node6 &#x3D; new TreeNode(6, &quot;F&quot;); TreeNode node7 &#x3D; new TreeNode(7, &quot;G&quot;); TreeNode node8 &#x3D; new TreeNode(8, &quot;H&quot;); TreeNode node9 &#x3D; new TreeNode(9, &quot;I&quot;); root.setLeft(node2); root.setRight(node3); node2.setLeft(node4); node4.setLeft(node5); node5.setLeft(node6); node6.setLeft(node7); node6.setRight(node8); node8.setLeft(node9); binaryTree.setRoot(root); binaryTree.levelOrder1(); &#125;&#125;class BinaryTree &#123; public TreeNode root; public void setRoot(TreeNode root) &#123; this.root &#x3D; root; &#125; &#x2F;&#x2F; 层序 public void levelOrder1() &#123; if (this.root !&#x3D; null) &#123; this.root.levelOrder(root); ; &#125; else &#123; System.out.println(&quot;二叉树为空！&quot;); &#125; &#125;&#125;class TreeNode &#123; public TreeNode root; public void setRoot(TreeNode root) &#123; this.root &#x3D; root; &#125; int val; TreeNode left; TreeNode right; TreeNode(int val, TreeNode left, TreeNode right) &#123; this.val &#x3D; val; this.left &#x3D; left; this.right &#x3D; right; &#125; public TreeNode(int i, String string) &#123; &#x2F;&#x2F; TODO 自动生成的构造函数存根 &#125; public int getVal() &#123; return val; &#125; public void setVal(int val) &#123; this.val &#x3D; val; &#125; public TreeNode getLeft() &#123; return left; &#125; public void setLeft(TreeNode left) &#123; this.left &#x3D; left; &#125; public TreeNode getRight() &#123; return right; &#125; public void setRight(TreeNode right) &#123; this.right &#x3D; right; &#125; public TreeNode getRoot() &#123; return root; &#125; List&lt;List&lt;Integer&gt;&gt; res &#x3D; new ArrayList(); public List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) &#123; if (root &#x3D;&#x3D; null) return res; &#x2F;&#x2F; 边界条件 Queue&lt;TreeNode&gt; queue &#x3D; new LinkedList(); &#x2F;&#x2F; 创建的队列用来存放结点 queue.add(root); int a &#x3D; 0; int b &#x3D; 0; &#x2F;&#x2F; 记录层数 int sum &#x3D; 0; System.out.println(&quot;该二叉树的层次结构：&quot;); while (!queue.isEmpty()) &#123; &#x2F;&#x2F; 队列为空说明已经遍历完所有元素，while语句用于循环每一个层次 int count &#x3D; queue.size(); a &#x3D; queue.size(); &#x2F;&#x2F; System.out.println(count); List&lt;Integer&gt; list &#x3D; new ArrayList(); while (count &gt; 0) &#123; &#x2F;&#x2F; 遍历当前层次的所有结点，count代表当前层次的结点数 TreeNode temp &#x3D; queue.peek(); queue.poll(); &#x2F;&#x2F; 遍历的每一个结点都需要将其弹出 list.add(temp.val); if (temp.left !&#x3D; null) queue.add(temp.left); &#x2F;&#x2F; 迭代向左探索 if (temp.right !&#x3D; null) queue.add(temp.right); count--; b++; &#x2F;&#x2F; System.out.println(a); &#125; if (b % 3 &#x3D;&#x3D; 0) &#123; sum +&#x3D; a; &#125; if (count&#x3D;&#x3D;1 &amp;&amp; b%3&#x3D;&#x3D;2) &#123; sum+&#x3D;a; &#125; res.add(list); System.out.println(list); &#x2F;&#x2F; System.out.println(count); &#125; System.out.println(&quot;最小涂色结点:&quot;+(sum - 1)); return res; &#125;&#125; 运行结果12345678910该二叉树的层次结构：[0][0, 0][0][0][0][0, 0][0]最小涂色结点:3 其他测试结果112345678该二叉树的层次结构：[0][0, 0][0][0][0]最小涂色结点:2 其他测试结果2123456789该二叉树的层次结构：[0][0, 0][0][0][0, 0][0, 0]最小涂色结点:3","categories":[{"name":"算法练习","slug":"算法练习","permalink":"http://www.studyz.club/categories/%E7%AE%97%E6%B3%95%E7%BB%83%E4%B9%A0/"},{"name":"二叉树","slug":"算法练习/二叉树","permalink":"http://www.studyz.club/categories/%E7%AE%97%E6%B3%95%E7%BB%83%E4%B9%A0/%E4%BA%8C%E5%8F%89%E6%A0%91/"}],"tags":[{"name":"算法练习","slug":"算法练习","permalink":"http://www.studyz.club/tags/%E7%AE%97%E6%B3%95%E7%BB%83%E4%B9%A0/"},{"name":"二叉树","slug":"二叉树","permalink":"http://www.studyz.club/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"}]},{"title":"Python综合项目案例开发--使用GUI框架","slug":"Python综合项目案例开发--使用GUI框架","date":"2020-06-05T00:38:41.099Z","updated":"2020-12-01T01:13:46.946Z","comments":true,"path":"posts/855e4ddc/","link":"","permalink":"http://www.studyz.club/posts/855e4ddc/","excerpt":"","text":"一,初识GUI 1.1 常用的GUI框架 1.2 wxPython 框架的使用 1.3 常用控件 1.4 BoxSizer 布局 一,初识GUI GUI 是 Graphical User Interface(图形用户界面)的缩写。在 GUI 中，并不只是输入文本和返回文本，用户可以看到窗口、按钮、文本框等图形，而且可以用鼠标单击，还可以通过键盘输入。GUI 是与程序交互的一种不同方式。GUI 的程序有三个基本要素：输入、处理和输出，但它们的输入和输出更丰富、更有趣一些。 1.1 常用的GUI框架 Python 的 GUI 开发，有很多工具包供我们选择。其中一些流行的工具包如下表所示：每个工具包都有其优缺点，本章将详细讲解 wxPython 框架和 PyQt 框架的使用。 1.2 wxPython 框架的使用1.2.1 安装wxPython wxPython 是个成熟而且特性丰富的跨平台 GUI 工具包。wxPython 的安装非常简单，使用 pip 工具安装 wxPython 只需要一行命令： 1pip install -U wxPython 创建一个 wx.App 的子类 在开始创建应用程序之前，先来创建一个没有任何功能的子类。创建和使用一个wx.App 子类，需要执行如下 4 个步骤： ①定义这个子类。②在定义的子类中写一个 OnInit 初始化方法。③在程序的主要部分创建这个类的一个实例。④调用应用程序实例的 MainLoop()方法，这个方法将程序的控制权转交给 wxPython。 创建一个没有任何功能的子类，代码如下: 12345678910#导入wx模块import wxclass App(wx.App): #定义子类APP(),继承父类wx.App def OnInit(self): #初始化方法 frame&#x3D;wx.Frame(parent&#x3D;None,title&#x3D;&quot;Hello wyPython&quot;) #创建窗口 frame.Show() #显示窗口 return True #返回值if __name__ &#x3D;&#x3D; &#39;__main__&#39;: #这里一边是两个下划线 app&#x3D;App() app.MainLoop() 直接使用wx.app 通常，如果在系统中只有一个窗口的话，可以不创建 wx.App 子类，直接使用 wx.App。 12345import wx # 导入 wxPythonapp &#x3D; wx.App() # 初始化 wx.App 类frame &#x3D; wx.Frame(None,title&#x3D;&#39;Hello wyPython&#39;) # 定义了一个顶级窗口frame.Show() # 显示窗口app.MainLoop() # 调用 wx.App 类的 MainLoop()主循环方法 1.2.2 使用wx.Frame框架 在 GUI 中框架通常称为窗口。框架是一个容器，用户可以将它在屏幕上任意移动，并可对它进行缩放，它通常包含诸如标题栏、菜单等等。在 wxPython 中，wx.Frame 是所有框架的父类。当创建 wx.Frame 的子类时，子类应该调用其父类的构造器 wx.Frame.init()。wx.Frame 的构造器语法格式如下： 创建一个wx.Frame的子类 123456789import wxclass MyFrame(wx.Frame): def __init__(self,parent,id): wx.Frame.__init__(self,parent,id,title&#x3D;&quot;创建Frame&quot;,pos&#x3D;(100,100),size&#x3D;(300,300))if __name__&#x3D;&#x3D;&#39;__main__&#39;: app&#x3D;wx.App() frame&#x3D;MyFrame(parent&#x3D;None,id&#x3D;-1) frame.Show() app.MainLoop() 1.3 常用控件 创建完窗口以后，我们可以在窗口内添加一些控件，所谓控件，就是经常使用的按钮、文本、输入框、单选框等 Static Text 文本类 对于所有的 UI 工具来说，最基本的任务就是在屏幕上绘制纯文本。在 wxPython 中，可以使用 wx.StaticTest 类来完成。使用 wx.StaticTest 类能够改变文本的对齐方式、字体和颜色等。wx.StaticTest 类的构造函数语法格式如下 在 Python 控制台中输入 import this 后，会输出下图的文本，这些英文语句就是通常所说的 Python 之禅。(Python彩蛋) 1import this 123456789101112131415161718192021The Zen of Python, by Tim PetersBeautiful is better than ugly.Explicit is better than implicit.Simple is better than complex.Complex is better than complicated.Flat is better than nested.Sparse is better than dense.Readability counts.Special cases aren&#39;t special enough to break the rules.Although practicality beats purity.Errors should never pass silently.Unless explicitly silenced.In the face of ambiguity, refuse the temptation to guess.There should be one-- and preferably only one --obvious way to do it.Although that way may not be obvious at first unless you&#39;re Dutch.Now is better than never.Although never is often better than *right* now.If the implementation is hard to explain, it&#39;s a bad idea.If the implementation is easy to explain, it may be a good idea.Namespaces are one honking great idea -- let&#39;s do more of those! 使用 Static Text 类输出中文版的 Python 之禅，代码如 123456789101112131415import wxclass MyFrame(wx.Frame): def __init__(self,parent,id): wx.Frame.__init__(self,parent,id,title&#x3D;&quot;创建StaticText类&quot;,pos&#x3D;(100,100),size&#x3D;(600,400)) panel&#x3D;wx.Panel(self) title &#x3D; wx.StaticText(panel,label&#x3D;&#39;Python之禅--Tim Perters&#39;,pos&#x3D;(100,20)) font &#x3D; wx.Font(16,wx.DEFAULT,wx.FONTSTYLE_NORMAL,wx.NORMAL) title.SetFont(font) wx.StaticText(panel,label&#x3D;&#39;优美胜于丑陋&#39;,pos&#x3D;(50,50)) if __name__&#x3D;&#x3D;&#39;__main__&#39;: app &#x3D; wx.App() frame &#x3D; MyFrame(parent&#x3D;None,id&#x3D;1) frame.Show() app.MainLoop() 上面的例子中，使用 wx.Font 类来设置字体。wx.Font 构造函数语法格式如下： 运行结果如下 TextCtrl 输入文本类 wx.StaticTest 类只能用于显示纯粹的静态文本，但是有时需要输入文本与用户进行交互。此时，就需要使用 wx.TestCtrl 类，它允许输入单行和多行文本，可以作为密码输入控件，掩饰所按下的按键。wx.TestCtrl 类的构造函数语法格式如下： 使用 wx.TestCtrl 类和 wx.StaticTest 类实现一个包含用户和密码的登录界面 1234567891011121314151617import wxclass MyFrame(wx.Frame): def __init__(self,parent,id): wx.Frame.__init__(self,parent,id,title&#x3D;&quot;创建TestCtrl类&quot;,size&#x3D;(400,300)) #创建面板 panel&#x3D;wx.Panel(self) #创建文本和输入框 self.title&#x3D;wx.StaticText(panel,label&#x3D;&quot;请输入用户名和密码:&quot;,pos&#x3D;(140,20)) self.label_user&#x3D;wx.StaticText(panel,label&#x3D;&quot;用户名:&quot;,pos&#x3D;(50,50)) self.text_user&#x3D;wx.TextCtrl(panel,pos&#x3D;(100,50),size&#x3D;(235,25),style&#x3D;wx.TE_LEFT) self.label_pwd&#x3D;wx.StaticText(panel,pos&#x3D;(50,90),label&#x3D;&quot;密 码:&quot;) self.text_password&#x3D;wx.TextCtrl(panel,pos&#x3D;(100,90),size&#x3D;(235,25),style&#x3D;wx.TE_PASSWORD)if __name__&#x3D;&#x3D;&#39;__main__&#39;: app&#x3D;wx.App() #初始化应用 frame&#x3D;MyFrame(parent&#x3D;None,id&#x3D;-1) frame.Show() app.MainLoop() 运行结果如下 Button按钮类 按钮是 GUI 界面中应用最为广泛的控件，它常用于捕获用户生成的单击事件，最为明显的用途是触发绑定到一个处理函数。wxPython 类库提供不同类型的按钮，最简单、常用的是 wx.Button 类。wx.Button 类的构造函数语法格式如下： 使用 wx.Button，在前一个实例的基础上添加“确定”和“取消”按钮。代码如 12345678910111213141516171819202122import wxclass MyFrame(wx.Frame): def __init__(self,parent,id): wx.Frame.__init__(self,parent,id,title&#x3D;&quot;创建TestCtrl类&quot;,size&#x3D;(400,300)) #创建面板 panel&#x3D;wx.Panel(self) #创建文本和输入框 self.title&#x3D;wx.StaticText(panel,label&#x3D;&quot;请输入用户名和密码:&quot;,pos&#x3D;(140,20)) self.label_user&#x3D;wx.StaticText(panel,label&#x3D;&quot;用户名:&quot;,pos&#x3D;(50,50)) self.text_user&#x3D;wx.TextCtrl(panel,pos&#x3D;(100,50),size&#x3D;(235,25),style&#x3D;wx.TE_LEFT) self.label_pwd&#x3D;wx.StaticText(panel,pos&#x3D;(50,90),label&#x3D;&quot;密 码:&quot;) self.text_password&#x3D;wx.TextCtrl(panel,pos&#x3D;(100,90),size&#x3D;(235,25),style&#x3D;wx.TE_PASSWORD) #创建&quot;确定&quot;和&quot;取消&quot;按钮 self.bt_confirm&#x3D;wx.Button(panel,label&#x3D;&quot;确定&quot;,pos&#x3D;(105,130)) self.bt_confirm&#x3D;wx.Button(panel,label&#x3D;&quot;取消&quot;,pos&#x3D;(195,130))if __name__&#x3D;&#x3D;&#39;__main__&#39;: app&#x3D;wx.App() #初始化应用 frame&#x3D;MyFrame(parent&#x3D;None,id&#x3D;-1) #实例化MyFrame类，并传递参数 frame.Show() # 显示窗口 app.MainLoop() # 调用主循环方法 1.4 BoxSizer 布局 前面的例子使用了文本和按钮等控件，并将这些控件通过 pos 参数布置在 pannel 面板上。虽然这种设置坐标的方式很容易理解，但过程很麻烦。此外，控件的位置是绝对位置，也就是固定的。当调整窗口大小时，界面会变得不美观。在 wxPython 中有一种更智能的布局方式——sizer(尺寸器)。sizer 是用于自动布局一组窗口控件的算法。sizer 被附加到一个容器中，通常是一个框架或面板。在父容器中创建的子窗口控件必须被分别添加到 sizer 中。当 sizer 被附加到容器时，它随后就可以管理它所包含的子布局wxPython 提供了 5 个 sizer，定义在下表中 使用BoxSize布局 尺寸器会管理组件的尺寸。只要将部件添加到尺寸器上，再加上一些布局参数，然后让尺寸器自己去管理父组件的尺寸。下面使用 BoxSizer 实现简单的布局。代码如下： 123456789101112131415161718import wxclass MyFrame(wx.Frame): def __init__(self,parent,id): wx.Frame.__init__(self,parent,id,title&#x3D;&quot;用户登录&quot;,size&#x3D;(400,300)) #创建面板 panel&#x3D;wx.Panel(self) #创建文本和输入框 self.title&#x3D;wx.StaticText(panel,label&#x3D;&quot;请输入用户名和密码:&quot;,pos&#x3D;(140,20)) #添加容器，容器中控件按纵向排列 vsizer&#x3D;wx.BoxSizer(wx.VERTICAL) vsizer.Add(self.title,proportion&#x3D;0,flag&#x3D;wx.BOTTOM|wx.Top|wx.ALIGN_CENTER,border&#x3D;15) panel.SetSizer(vsizer) if __name__&#x3D;&#x3D;&#39;__main__&#39;: app&#x3D;wx.App() #初始化应用 frame&#x3D;MyFrame(parent&#x3D;None,id&#x3D;-1) #实例化MyFrame类，并传递参数 frame.Show() # 显示窗口 app.MainLoop() # 调用主循环方法 上述代码中，设置了增加背景控件(wx.Panel)，并创建了一个 wx.BoxSizer，它带有一个决定其是水平还是垂直的参数(wx.HORIZONTAL 或者 wx.VERTICAL)，默认为水平。然后使用Add()方法将控件加入sizer，最后使用面板的 SetSizer()方法设定它的尺寸器。Add()方法的。语法格式如下: proportion=0是说明使用的是默认的控件大小","categories":[{"name":"Pythong实战-使用GUI框架","slug":"Pythong实战-使用GUI框架","permalink":"http://www.studyz.club/categories/Pythong%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8GUI%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"Pythong实战-使用GUI框架","slug":"Pythong实战-使用GUI框架","permalink":"http://www.studyz.club/tags/Pythong%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8GUI%E6%A1%86%E6%9E%B6/"}]},{"title":"Python综合项目案例开发--使用Python操作数据库","slug":"Python综合项目案例开发--使用Python操作数据库","date":"2020-05-23T08:00:55.227Z","updated":"2020-12-01T01:13:46.948Z","comments":true,"path":"posts/fb315820/","link":"","permalink":"http://www.studyz.club/posts/fb315820/","excerpt":"","text":"一，数据库编程接口 1.1 连接接对象 1.2 游标对象 二, 使用SQLite 2.1 创建数据库文件 2.2 操作SQLite 2.2.1 新增用户数据信息 2.2.2 查询数据的三种方式 2.2.3 修改用户数据信息 2.2.4 删除用户信息 2.3 使用MySQL 2.3.1 安装PyMySQL 2.3.2 连接数据库 2.3.3 创建表 2.3.4 操作MySQL数据表 一，数据库编程接口 在项目开发中，数据库的应用必不可少。虽然数据库的种类很多，如：SQLite、MySQL、Oracle 等，但是它们的功能基本都是一样的。为了对数据库进行统一的操作，大多数语言都提供简单的、标准化的数据库接口(API)。在 Python Database API2.0 规范中，定义了模块接口、连接对象、游标对象、类型对象和构造器、DB API 的可选扩展以及可选的错误处理机制等。 1.1 连接接对象 数据库连接对象(Connection Object)主要提供获取数据库游标对象和提交/回滚事物的方法，以及如何关闭数据库连接。 1) 获取连接对象 使用 connect()函数获取连接对象。例如，需要访问 Oracle 数据库和 MySQL 数据库，必须下载 Oracle 和 MySQL 数据库模块。这些模块在获取连接对象时，都要使用 connect()函数,connect()函数常用的参数及说明如下表所示。 例如，使用 PyMySQL 模块连接 MySQL 数据库，代码如下 123import pymysqlconn &#x3D; pymysql.connect (host&#x3D;&#39;localhost&#39;, user&#x3D;&#39;user&#39;, password&#x3D;&#39;passwd&#39;, db&#x3D;&#39;test&#39;, charset&#x3D;&#39;utf8&#39;, cursorclass&#x3D;pymysql.cursors.DictCursor) 2) 连接对象的方法 connect()函数返回连接对象，这个对象表示目前和数据库的会话。连接对象支持的方法如下表所示。使用事物可以维护数据库的完整性. 1.2 游标对象 游标对象(Cursor Object)代表数据库中的游标，用于指示抓取数据的上下文。主要提供执行 SQL 语句、调用存储过程、获取查询结果等方法。使用连接对象的 cursor()方法，可以获取游标对象。游标对象的方法及说明如下表所示。 二, 使用SQLite 与许多数据库管理系统不同，SQLite 不是一个客户端/服务器结构的数据库引擎，而是一种嵌入式数据库，它的数据库就是一个文件。SQLite 将整个数据库，包括定义、表、索引以及数据本身，作为一个独立的、可跨平台使用的文件存储在主机中。由于 SQLite 本身是用 C 语言编写的，体积很小，所以经常被集成到各种应用程序中。Python 就内置了SQLite3，所以在 Python 中使用 SQLite 不需要安装任何模块，可以直接使用。 Python中内置了SQLite在IDLE中导入包，如果不报错则说明成功内置 2.1 创建数据库文件 在 Python 中直接使用 import 语句导入 SQLite3 模块。Python 操作数据库的通用流程图如下图所示 例如，创建一个名称为 mrsoft.db 的 SQLite 数据库文件，然后执行 SQL 语句创建一个 user(用户表)，user 表包含 id 和 name 两个字段，代码如下： 123456789101112131415161718import sqlite3# 连接到 SQLite 数据库，数据库文件是 mrsoft.db，如果文件不存在，会自动在当前目录创建conn &#x3D; sqlite3.connect(&#39;mrsoft.db&#39;)# 创建一个 Cursorcursor &#x3D; conn.cursor()# 执行一条 SQL 语句，创建 user 表cursor.execute(&#39;create table user (id int(10) primary key, name varchar(20))&#39;)# 关闭游标cursor.close()# 关闭 Connectionconn.close()&#96;&#96;&#96;&#96;## &lt;h id&#x3D;2.2&gt;2.2 操作SQLite&lt;&#x2F;h&gt;### &lt;h id&#x3D;2.2.1&gt;1) 新增用户数据信息&lt;&#x2F;h&gt;&gt; 为了向数据表中新增数据，可以使用如下SQL语句 insert into 表名(字段名1,字段名2,…) values (字段值1,字段值2,…) 12例如向user表中插入3条用户信息 #导入模块import sqlite3#创建连接对象conn = sqlite3.connect(‘mrsoft.db’)#创建游标对象cursor = conn.cursor()#执行SQL语句sql = ‘insert into user (id ,name) values(1,”lizhi”)’cursor.execute(sql)#关闭游标cursor.close()#提交事务conn.commit()#关闭连接conn.close() 123456789#### 查看① 用Navicat Premium来查看数据库 ② 新建连接(SQLite是以文件存储的，因此要找到本地文件。也就是新建的mrsoft.db)![](Python综合项目案例开发--使用Python操作数据库&#x2F;221.png)没有用户名密码不需要填。![](Python综合项目案例开发--使用Python操作数据库&#x2F;222.png)#### a, 为了防止SQL注入 #导入模块import sqlite3#创建连接对象conn = sqlite3.connect(‘mrsoft.db’)#创建游标对象cursor = conn.cursor()#执行SQL语句,为了防止SQL注入，将values用?去代替，然后再传值sql = ‘insert into user (id ,name) values(?,?)’cursor.execute(sql,(2,’daer’))#关闭游标cursor.close()#提交事务conn.commit()#关闭连接conn.close() 12#### b,插入多条信息 cursor = conn.cursor()#执行SQL语句,为了防止SQL注入，将values用?去代替，然后再传值sql = ‘insert into user (id ,name) values(?,?)’data = [(3,’erer’),(4,’saner’),(5,’sier’)]cursor.executemany(sql,data) 1234### &lt;h id&#x3D;2.2.2&gt;2) 查询数据的三种方式&lt;&#x2F;h&gt;&gt; 查找user表中的数据可以使用如下SQL语句: select 字段名1,字段名2,字段名3,… from 表名 where 查询条件 12345678910&gt; 查询数据时通常有三种方式 ①fetchone()：获取查询结果集中的下一条记录。 ②fetchmany(size)：获取指定数量的记录。 ③fetchall()：获取查询结果集中的所有记录。 * 表结构![](Python综合项目案例开发--使用Python操作数据库&#x2F;2223.png)* fetchone()获取查询结果集中的下一条记录。 #导入模块import sqlite3#创建连接对象conn = sqlite3.connect(‘mrsoft.db’)#创建游标对象cursor = conn.cursor()#执行SQL语句sql = ‘select * from user’cursor.execute(sql)cursor.fetchone() #这是第一次查询#fetchone()获取查询结果集中的下一条记录。result1 = cursor.fetchone()print(result1)#关闭游标cursor.close()#关闭连接conn.close() 1运行结果 (2, ‘daer’) 12* fetchmany()获取指定数量的记录。 #执行SQL语句sql = ‘select * from user’cursor.execute(sql)result2 = cursor.fetchmany(3)print(result2) 1运行结果 [(1, ‘lizhi’), (2, ‘daer’), (3, ‘erer’)] 12* fetchall()：获取查询结果集中的所有记录。 #执行SQL语句sql = ‘select * from user’cursor.execute(sql)result3 = cursor.fetchall()print(result3) 1运行结果 [(1, ‘lizhi’), (2, ‘daer’), (3, ‘erer’), (4, ‘saner’), (5, ‘sier’)] 1234### &lt;h id&#x3D;2.2.3&gt;2.2.3 修改用户数据信息&lt;&#x2F;h&gt;&gt; 修改user表中的数据可以使用如下SQL语句 update 表名 set 字段名 = 字段值 where 查询条件 12345678910111213141516&#96;&#96;&#96;#导入模块import sqlite3#创建连接对象conn &#x3D; sqlite3.connect(&#39;mrsoft.db&#39;)#创建游标对象cursor &#x3D; conn.cursor()#执行SQL语句sql &#x3D; &#39;update user set name &#x3D; ? where id &#x3D; ?&#39;cursor.execute(sql,(&#39;baba&#39;,1))#关闭游标cursor.close()#提交事务conn.commit()#关闭连接conn.close() 2.2.4 删除用户信息 删除user表中的数据可以使用如下SQL语句 1delete from 表名 where 查询条件 123456789101112131415161718#导入模块import sqlite3#创建连接对象conn &#x3D; sqlite3.connect(&#39;mrsoft.db&#39;)#创建游标对象cursor &#x3D; conn.cursor()#执行SQL语句sql &#x3D; &#39;delete from user where id &#x3D;?&#39;cursor.execute(sql,(1,))cursor.execute(&#39;select * from user&#39;)result4 &#x3D; cursor.fetchall()print(result4)#关闭游标cursor.close()#提交事务conn.commit()#关闭连接conn.close() 运行结果 1[(2, &#39;daer&#39;), (3, &#39;erer&#39;), (4, &#39;saner&#39;), (5, &#39;sier&#39;)] 2.3 使用MySQL 之前的文章已经介绍过MySQL的安装这里就不做过多的介绍了。 2.3.1 安装PyMySQL 由于 MySQL 服务器以独立的进程运行，并通过网络对外服务。所以，需要支持 Python的 MySQL 驱动来连接到MySQL服务器。在Python中支持MySQL 的数据库模块有很多，我们选择 PyMySQL 模块。PyMySQL 的安装比较简单，在 cmd 中运行如下命令： 1pip install PyMySQL pymysql官方文档 2.3.2 连接数据库查看数据库版本号 12345678910111213#导入pymysqlimport pymysql# 调用connect()函数产生connection链接对象db &#x3D; pymysql.connect(host &#x3D; &#39;localhost&#39;,user &#x3D; &#39;root&#39;,password&#x3D;&#39;******&#39;,database &#x3D; &#39;mrsoft&#39;)#调用cursor()方法，创建cursor对象cursor &#x3D; db.cursor()# 执行SQL语句cursor.execute(&#39;select version()&#39;)data &#x3D; cursor.fetchone()print(data)# 关闭连接cursor.close()db.close() 输出结果 1(&#39;8.0.15&#39;,) 2.3.3 创建表 创建book表1234567891011121314151617181920#导入pymysqlimport pymysql# 调用connect()函数产生connection链接对象db &#x3D; pymysql.connect(host &#x3D; &#39;localhost&#39;,user &#x3D; &#39;root&#39;,password&#x3D;&#39;******&#39;,database &#x3D; &#39;mrsoft&#39;)#调用cursor()方法，创建cursor对象cursor &#x3D; db.cursor()# 执行SQL语句sql &#x3D; &quot;&quot;&quot;CREATE TABLE books ( id int(8) NOT NULL AUTO_INCREMENT, name varchar(50) NOT NULL, category varchar(50) NOT NULL, price decimal(10,2) DEFAULT NULL, publish_time date DEFAULT NULL, PRIMARY KEY (id)) ENGINE&#x3D;MyISAM AUTO_INCREMENT&#x3D;1 DEFAULT CHARSET&#x3D;UTF8MB4;&quot;&quot;&quot;cursor.execute(sql)# 关闭连接cursor.close()db.close() 2.3.4 操作MySQL数据表1) 新增 book表中添加数据 1234567891011121314#导入pymysqlimport pymysql# 调用connect()函数产生connection链接对象db &#x3D; pymysql.connect(host &#x3D; &#39;localhost&#39;,user &#x3D; &#39;root&#39;,password&#x3D;&#39;******&#39;,database &#x3D; &#39;mrsoft&#39;)#调用cursor()方法，创建cursor对象cursor &#x3D; db.cursor()data &#x3D; (&#39;零基础学Python&#39;,&#39;python&#39;,&#39;88&#39;,&#39;2020-5-31&#39;)# 执行SQL语句(注意这里sql语句占位符不同，在MySQL中用问号？作为占位符，在pymysql中用%s)sql &#x3D; &quot;insert into books(name,category,price,publish_time) values (%s,%s,%s,%s)&quot;cursor.execute(sql,data)# 关闭连接cursor.close()db.close() 插入多条数据可以使用元组(cursor.executemany()) 1234567891011121314151617#导入pymysqlimport pymysql# 调用connect()函数产生connection链接对象db &#x3D; pymysql.connect(host &#x3D; &#39;localhost&#39;,user &#x3D; &#39;root&#39;,password&#x3D;&#39;******&#39;,database &#x3D; &#39;mrsoft&#39;)#调用cursor()方法，创建cursor对象cursor &#x3D; db.cursor()data &#x3D; [(&#39;零基础学Python&#39;,&#39;python&#39;,&#39;88&#39;,&#39;2020-5-31&#39;), (&#39;零基础学java&#39;,&#39;java&#39;,&#39;99&#39;,&#39;2020-5-31&#39;), (&#39;零基础学jvav&#39;,&#39;jvav&#39;,&#39;100&#39;,&#39;2020-5-31&#39;), (&#39;零基础学从删库到跑路&#39;,&#39;MySQL&#39;,&#39;88&#39;,&#39;2020-5-31&#39;), ]# 执行SQL语句(注意这里sql语句占位符不同，在MySQL中用问号？作为占位符，在pymysql中用%s)sql &#x3D; &quot;insert into books(name,category,price,publish_time) values (%s,%s,%s,%s)&quot;cursor.executemany(sql,data)# 关闭连接cursor.close()db.close() 处理异常 12345678910111213141516171819202122#导入pymysqlimport pymysql# 调用connect()函数产生connection链接对象db &#x3D; pymysql.connect(host &#x3D; &#39;localhost&#39;,user &#x3D; &#39;root&#39;,password&#x3D;&#39;******&#39;,database &#x3D; &#39;mrsoft&#39;)#调用cursor()方法，创建cursor对象cursor &#x3D; db.cursor()data &#x3D; [(&#39;零基础学Python&#39;,&#39;python&#39;,&#39;88&#39;,&#39;2020-5-31&#39;), (&#39;零基础学java&#39;,&#39;java&#39;,&#39;99&#39;,&#39;2020-5-31&#39;), (&#39;零基础学jvav&#39;,&#39;jvav&#39;,&#39;100&#39;,&#39;2020-5-31&#39;), (&#39;零基础学从删库到跑路&#39;,&#39;MySQL&#39;,&#39;88&#39;,&#39;2020-5-31&#39;), ]# 执行SQL语句(注意这里sql语句占位符不同，在MySQL中用问号？作为占位符，在pymysql中用%s)try: sql &#x3D; &quot;insert into books(name,category,price,publish_time) values (%s,%s,%s,%s)&quot; cursor.executemany(sql,data) db.commit()except: #发生错误回滚 db.rollback()# 关闭连接cursor.close()db.close()","categories":[{"name":"Pythong实战-操作数据库","slug":"Pythong实战-操作数据库","permalink":"http://www.studyz.club/categories/Pythong%E5%AE%9E%E6%88%98-%E6%93%8D%E4%BD%9C%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"Pythong实战-操作数据库","slug":"Pythong实战-操作数据库","permalink":"http://www.studyz.club/tags/Pythong%E5%AE%9E%E6%88%98-%E6%93%8D%E4%BD%9C%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"大数据流处理框架--Storm","slug":"大数据流处理框架Storm","date":"2020-05-21T09:23:21.219Z","updated":"2020-12-01T01:13:46.949Z","comments":true,"path":"posts/7c5fe410/","link":"","permalink":"http://www.studyz.club/posts/7c5fe410/","excerpt":"","text":"一，什么是Storm 二，Storm基本概念 一，什么是Storm Storm是一个可以持续运行的流处理工具，它将监听一个流数据，并且在这些数据上执行不同类型的处理。 Storm可以与现有的很多技术一起集成，为流处理提供具有价值的解决方案。 大数据的定义要掌握其四个重要特性：体量（数据量）、速度（数据流入系统的速度）、多样性（不同类型的数据）、真实性（数据的准确性）. 有三个主要类型的工具来处理大数据：批处理、流处理以及基于流的微型批处理。 Storm的优势包括其可扩展性，对每个消息至少处理一次，健壮性，以及支持任何语言来实现开发。 Stom是一个开源的分布式实时计算系统，可以简单、可靠的处理大量的数据流。 Storm有很多使用场景：如实时分析，在线机器学习，持续计算，分布式RPC，ETL等等。 Storm支持水平扩展，具有高容错性，保证每个消息都会得到处理，而且处理速度很快（在一个小集群中，每个结点每秒可以处理数以百万计的消息）. Storm的部署和运维都很便捷，而且更为重要的是可以使用任意编程语言来开发应用。 1.2 Storm特点1) 编程模型简单 在大数据处理方面相信大家对 hadoop已经耳熟能详，基于 Google Map/ Reduce来实现的 Hadoop为开发者提供了map、 reduce原语，使并行批处理程序变得非常地简单和优美。同样， Storm也为大数据的实时计算提供了一些简单优美的原语，这大大降低了开发并行实时处理的任务的复杂性，帮助你快速、高效的开发应用。 2) 可扩展 在 Storm集群中真正运行 topology的主要有三个实体：工作进程（ workers）、线程（ executor）和任务（task）. Storm集群中的每台机器上都可以运行多个工作进程，每个工作进程又可创建多个线程，每个线程可以执行多个任务，任务是真正进行数据处理的实体，我们开发的 spout、bolt就是作为一个或者多个任务的方式执行的。因此，计算任务在多个线程、进程和服务器之间并行进行，支持活的水平扩展。 3) 高可靠性 Storm可以保证 spout发出的每条消息都能被“完全处理”，这也是直接区别于其他实时系统的地方，如S4.请注意， spout发出的消息后续可能会触发产生成千上万条消息可以形象的理解为一棵消息树，其中 spout发出的消息为树根， Storm会跟踪这棵消息树的处理情况，只有当这棵消息树中的所有消息都被处理了， Storm才会认为 spout发出的这个消息已经被“完全处理”如果这棵消息树中的任何一个消息处理失败了，或者整棵消息树在限定的时间内没有“完全处理”，那么 spout发出的消息就会重发。考虑到尽可能减少对内存的消耗， Storm并不会跟踪消息树中的每个消息，而是采用了一些特殊的策略，它把消息树当作一个整体来跟踪，对消息树中所有消息的唯一id进行异或计算，通过是否为零来判定 spout发出的消息是否被“完全处理”，这极大的节约了内存和简化了判定逻辑，后面会对这种机制进行详细介绍。这种模式，每发送一个消息，都会同步发送一个ack/fail，对于网络的带宽会有一定的消耗，如果对于可靠性要求不高，可通过使用不同的emi接口关闭该模式。上面所说的， Storm保证了每个消息至少被处理一次，但是对于有些计算场合，会严格要求每个消息只被处理一次，幸而 Storm的0.7.0引入了事务性拓扑，解决了这个问题，后面会有详述。 4) 高容错性 如果在消息处理过程中出了一些异常，Stom会重新安排这个出问题的 topology。Stom保证一个topology永远运行（除非你显式杀掉这个topology）。当然，如果 topology中存储了中间状态，那么当 topology重新被Storm启动的时候，需要应用自己处理中间状态的恢复。 5) 支持多种编程语言 除了用java实现 spout和bolt，你还可以使用任何你熟悉的编程语言来完成这项工作，这一切得益于Storm所谓的多语言协议。多语言协议是Storm内部的一种特殊协议，允许spout或者bolt使用标准输入和标准输出来进行消息传递,传递的消息为单行文本或者是json编码的多行。Storm支持多语言编程主要是通过 Shellbolt， Shellspout和ShellProcess这些类来实现的，这些类都实现了IBolt和ISpout接口，以及让shell通过java的 ProcessBuilder类来执行脚本或者程序的协议。可以看到，采用这种方式，每个 tuple在处理的时候都需要进行json的编解码，因此在吞吐量上会有较大影响 6) 支持本地模式 Stom有一种“本地模式”，也就是在进程中模拟一个 Storm集群的所有功能，以本地模式运行 topology跟在集群上运行 topology类似，这对于我们开发和测试来说非常有用。 7) 高效 用 ZeroMQ作为底层消息队列，保证消息能快速被处理。 二，Storm基本概念 Topologies Spouts Bolts Stream groupings Reliability Tasks Workers Configuration Storm集群和Hadoop集群表面上看很类似。但是 Hadoop上运行的是 MapReduce jobs，而在 Storm上运行的是拓扑（ topology），这两者之间是非常不一样的。一个关键的区别是：一个 MapReduce job最终会结束，而一个 topology永远会运行（除非你手动ki1l掉）。在 Storm的集群里面有两种节点：控制节点（ master node）和工作节点（ worker node）.控制节点上面运行一个叫Nimbus后台程序，它的作用类似Hadoop里面的JobTracker。Nimbus负责在集群里面分发代码，分配计算任务给机器，并且监控状态。 每一个工作节点上面运行一个叫做 Supervisor的节点。Supervisor会监听分配给它那台机器的工作，根据需要启动/关闭worker进程。每个 supervisor上运行着若干个 worker进程（根据配置文件 supervisor.slots.ports进行配置）.在一个worker进程中，又包含多个Executor线程，一个Executor线程中，可以包含个或多个相同的Task（spout/bolt），默认一个 Executor线程包含一个task. Nimbus和Supervisor之间的所有协调工作都是通过Zookeeper集群完成。另外，Nimbus进程和Supervisor进程都是快速失败（fail-fast）和无状态的。所有的状态要么在zookeeper里面，要么在本地磁盘上。这也就意味着你可以用kil1 -9来杀死 Nimbus和Supervisor进程，然后再重启它们，就好像什么都没有发生过。这个设计使得 Storm异常的稳定。 1、 Nimbus主节点： 主节点通常运行一个后台程序 —— Nimbus，用于响应分布在集群中的节点，分配任务和监测故障。这个很类似于Hadoop中的Job Tracker。2、Supervisor工作节点： 工作节点同样会运行一个后台程序 —— Supervisor，用于收听工作指派并基于要求运行工作进程。每个工作节点都是topology中一个子集的实现。而Nimbus和Supervisor之间的协调则通过Zookeeper系统或者集群。 2.1 Topologies(拓扑) 一个topology是spouts和 bolts组成的图，通过streamgroupings将图中的 spouts和 bolts连接起来，如下图： 一个topology会一直运行直到你手动kill掉，Storm自动重新分配执行失败的任务， 并且Storm可以保证你不会有数据丢失（如果开启了高可靠性的话）。如果一些机器意外停机它上面的所有任务会被转移到其他机器上。运行一个topology很简单。首先，把你所有的代码以及所依赖的jar打进一个jar包。然后运行类似下面的这个命令： 1storm jar all-my-code.jar backtype.storm.MyTopology arg1 arg2 这个命令会运行主类: backtype.strom.MyTopology, 参数是arg1, arg2。这个类的main函数定义这个topology并且把它提交给Nimbus。storm jar负责连接到Nimbus并且上传jar包。Topology的定义是一个Thrift结构，并且Nimbus就是一个Thrift服务， 你可以提交由任何语言创建的topology。上面的方面是用JVM-based语言提交的最简单的方法。 2.2 Spouts 消息源spout是Storm里面一个topology里面的消息生产者。简而言之，Spout从来源处读取数据并放入topology。Spout分成可靠和不可靠两种；当Storm接收失败时，可靠的Spout会对tuple（元组，数据项组成的列表）进行重发；而不可靠的Spout不会考虑接收成功与否只发射一次。 消息源可以发射多条消息流stream。使用OutputFieldsDeclarer.declareStream来定义多个stream，然后使用SpoutOutputCollector来发射指定的stream。而Spout中最主要的方法就是nextTuple（），该方法会发射一个新的tuple到topology，如果没有新tuple发射则会简单的返回。要注意的是nextTuple方法不能阻塞，因为storm在同一个线程上面调用所有消息源spout的方法。另外两个比较重要的spout方法是ack和fail。storm在检测到一个tuple被整个topology成功处理的时候调用ack，否则调用fail。storm只对可靠的spout调用ack和fail。 2.3 Bolts Topology中所有的处理都由Bolt完成。即所有的消息处理逻辑被封装在bolts里面。Bolt可以完成任何事，比如：连接的过滤、聚合、访问文件/数据库、等等。 Bolt从Spout中接收数据并进行处理，如果遇到复杂流的处理也可能将tuple发送给另一个Bolt进行处理。即需要经过很多blots。比如算出一堆图片里面被转发最多的图片就至少需要两步：第一步算出每个图片的转发数量。第二步找出转发最多的前10个图片。（如果要把这个过程做得更具有扩展性那么可能需要更多的步骤）。 Bolts可以发射多条消息流， 使用OutputFieldsDeclarer.declareStream定义stream，使用OutputCollector.emit来选择要发射的stream。 而Bolt中最重要的方法是execute（），以新的tuple作为参数接收。不管是Spout还是Bolt，如果将tuple发射成多个流，这些流都可以通过declareStream（）来声明。 bolts使用OutputCollector来发射tuple，bolts必须要为它处理的每一个tuple调用OutputCollector的ack方法，以通知Storm这个tuple被处理完成了，从而通知这个tuple的发射者spouts。 一般的流程是： bolts处理一个输入tuple, 发射0个或者多个tuple, 然后调用ack通知storm自己已经处理过这个tuple了。storm提供了一个IBasicBolt会自动调用ack。 2.4 Stream groupings Stream Grouping定义了一个流在Bolt任务间该如何被切分。这里有Storm提供的6个Stream Grouping类型：1）. 随机分组（Shuffle grouping）：随机分发tuple到Bolt的任务，保证每个任务获得相等数量的tuple。2）. 字段分组（Fields grouping）：根据指定字段分割数据流，并分组。例如，根据“user-id”字段，相同“user-id”的元组总是分发到同一个任务，不同“user-id”的元组可能分发到不同的任务。3）. 全部分组（All grouping）：tuple被复制到bolt的所有任务。这种类型需要谨慎使用。4）. 全局分组（Global grouping）：全部流都分配到bolt的同一个任务。明确地说，是分配给ID最小的那个task。5）. 无分组（None grouping）：你不需要关心流是如何分组。目前，无分组等效于随机分组。但最终，Storm将把无分组的Bolts放到Bolts或Spouts订阅它们的同一线程去执行（如果可能）。6）. 直接分组（Direct grouping）：这是一个特别的分组类型。元组生产者决定tuple由哪个元组处理者任务接收。当然还可以实现CustomStreamGroupimg接口来定制自己需要的分组。 2.5 Reliability Stom保证每个tuple会被 topology完整的执行。Storm会追踪由每个 spout tuple所产生的 tuple树（一个bolt处理一个 tuple之后可能会发射别的 tuple从而形成树状结构），并且跟踪这棵 tuple树什么时候成功处理完。每个 topology都有一个消息超时的设置，如果 storm在这个超时的时间内检测不到某个 tuple树到底有没有执行成功，那么 topology会把这个 tuple标记为执行失败，并且过会儿重新发射这个 tuple。为了利用 Storm的可靠性特性，在你发出一个新的 tuple以及你完成处理一个 tuple的时候你必须要通知 storm。这一切是由0 Output Collector来完成的。通过emit方法来通知一个新的 tuple产生了，通过ack方法通知一个 tuple处理完成了。 2.6 Tasks worker中每一个spout/bolt的线程称为一个task. 在storm0.8之后，task不再与物理线程对应，同一个spout/bolt的task可能会共享一个物理线程，该线程称为executor。每一个 spout和bolt会被当作很多task在整个集群里执行。每executor对应到一个线程，在这个线程上运行多个task，而stream grouping则是定义怎么从一堆task发射 tuple到另外一堆task。你可以调用 Topology Builder类的 setout和 setBolt来设置并行度（也就是有多少个task）。 2.7 Workers 运行具体处理组件逻辑的进程。一个 topology可能会在一个或者多个 worker（工作进程）里面执行，每个 worker是一个物理JWM并且执行整个 topology的一部分。比如，对于并行度是300的 topology来说，如果我们使用50个工作进程来执行，那么每个工作进程会开启6个线程，默认每个线程处理个 tasks。Storm会尽量均匀的工作分配给所有的 worker。每个supervisor上运行着若干个 worker进（根据配置文件supervisor.slots. ports进行配置）。 2.8 Storm里面有一堆参数可以配置来调整 Nimbus，Supervisor以及正在运行的 topology的行为，一些配置是系统级别的，一些配置是topology级别的。default.yaml里面有所有的默认配置。你可以通过定义个storm.yam1在你的 classpath里来覆盖这些默认配置。并且你也可以在代码里面设置一些 topology相关的配置信息、（使用StormSubmitter）。","categories":[{"name":"大数据流处理框架Storm","slug":"大数据流处理框架Storm","permalink":"http://www.studyz.club/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6Storm/"}],"tags":[{"name":"大数据流处理框架Storm","slug":"大数据流处理框架Storm","permalink":"http://www.studyz.club/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6Storm/"}]},{"title":"内存计算框架--Spark编程模型","slug":"内存计算框架spark2","date":"2020-05-21T07:04:18.159Z","updated":"2020-12-01T01:13:46.944Z","comments":true,"path":"posts/46aea7fc/","link":"","permalink":"http://www.studyz.club/posts/46aea7fc/","excerpt":"","text":"一，Spark编程模型 1.1，RDD弹性分布式数据集 一，Spark编程模型 与 Hadoop相比， Spark最初为提升性能而诞生。 Spark是 Hadoop MapReduce的演化和改进，并兼容了一些数据库的基本思想，可以说， Spark一开始就站在 Hadoop与数据库这两个巨人的肩膀上。同时， Spark依靠 Scala强大的函数式编程 Actor通信模式、闭包、容器、泛型，并借助统一资源调度框架，成为一个简洁、高效、强大的分布式大数据处理框架。Spark在运算期间，将输入数据与中间计算结果保存在内存中，直接在内存中计算另外，用户也可以将重复利用的数据缓存在内存中，缩短数据读写时间，以提高下次计算的效率。显而易见， Spark基于内存计算的特性使其擅长于迭代式与交互式任务，但也不难发现， Spark需要大量内存来完成计算任务。集群规模与 Spark性能之间呈正比关系，随着集群中机器数量的增长， Spark的性能也呈线性增长。接下来介绍 Spark编程模型。 1.1，RDD弹性分布式数据集 通常来讲，数据处理有几种常见模型： Iterative Algorithms、 Relational Queries、Map-Reduce、 Stream Processing.例如， Hadoop MapReduce采用了 MapReduce模型， Storm则采用了 Stream Processing模型。与许多其他大数据处理平台不同， Spark建立在统一抽象的RDD之上，而RDD混合了上述这4种模型，使得 Spark能以基本一致的方式应对不同的大数据处理场景，包括 MapReduce、 Streaming、SQL、 Machine Learning以及 Graph等。这契合了 Matei Zaharia提出的原则：“设计一个通用的编程抽象（Unified Programming Abstraction）”，这也正是 Spark的魅力所在，因此要理解 Spark，先要理解RDD的概念。 RDD（ Resilient Distributed Datasets，弹性分布式数据集）是一个容错的、并行的数据结构，可以让用户显式地将数据存储到磁盘或内存中，并控制数据的分区。RDD还提供了一组丰富的操作来操作这些数据，诸如map、 flatMap、 filter等转换操作实现了 monad模式，很好地契合了 Scala的集合操作。除此之外，RDD还提供诸如join、groupBy、reduceByKey等更为方便的操作，以支持常见的数据运算。 RDD是 Spark的核心数据结构，通过RDD的依赖关系形成 Spark的调度顺序。所谓 Spark应用程序，本质是一组对RDD的操作。 下面介绍RDD的创建方式及操作算子类型。 RDD的两种创建方式 从文件系统输入（如HDFS）创建 从已存在的RDD转换得到新的RDD RDD的两种操作算子 Transformation（变换） Transformation类型的算子不是立刻执行，而是延迟执行。也就是说从一个RDD变换为另一个RDD的操作需要等到 Action操作触发时，才会真正执行。 Action（行动） Action类型的算子会触发 Spark提交作业，并将数据输出到 Spark系统 1.1.1 深入理解RDD RDD从直观上可以看作一个数组，本质上是逻辑分区记录的集合。在集群中个RDD可以包含多个分布在不同节点上的分区，每个分区是一个 dataset片段，如图2-1所示。 在图2-1中，RDD-1含有三个分区（pl、p2和p3），分布存储在两个节点上： node1与node2。RDD-2只有一个分区P4，存储在node3节点上。RDD-3含有两个分区P5和P6，存储在node4节点上。 RDD依赖 RDD可以相互依赖，如果RDD的每个分区最多只能被一个 Child rdd的一个分区使用，则称之为窄依赖（ narrow dependency）；若多个 Child rdd分区都可以依赖，则称之为宽依赖（ wide dependency）.不同的操作依据其特性，可能会产生不同的依赖。例如，map操作会产生窄依赖，join操作则产生宽依赖，如图2-2所示。 RDD支持容错性 支持容错通常采用两种方式：日志记录或者数据复制。对于以数据为中心的系统而言，这两种方式都非常昂贵，因为它需要跨集群网络拷贝大量数据。 RDD天生是支持容错的。首先，它自身是一个不变的（ immutable）数据集，其次，RDD之间通过 lineage产生依赖关系（在下章继续探讨这个话题），因此RDD能够记住构建它的操作图，当执行任务的 Worker失败时，完全可以通过操作图获得之前执行的操作，重新计算。因此无须采用 replication方式支持容错，很好地降低了跨网络的数据传输成本。 RDD的高效性 RDD提供了两方面的特性：persistence（持久化）和partitioning（分区），用户可以通过 persist与 partition By函数来控制这两个特性。RDD的分区特性与并行计算能力（RDD定义了 parallelize函数），使得 Spark可以更好地利用可伸缩的硬件资源。如果将分区与持久化二者结合起来，就能更加高效地处理海量数据。 另外，RDD本质上是一个内存数据集，在访问RDD时，指针只会指向与操作相关的部分。例如，存在一个面向列的数据结构，其中一个实现为Int型数组，另一个实现为Foat型数组。如果只需要访问nt字段，RDD的指针可以只访问Int数组，避免扫描整个数据结构。 再者，如前文所述，RDD将操作分为两类： Transformation与 Action.无论执行了多少次 Transformation操作，RDD都不会真正执行运算，只有当Action操作被执行时运算才会触发。而在RDD的内部实现机制中，底层接口则是基于迭代器的，从而使得数据访问变得更高效，也避免了大量中间结果对内存的消耗。 在实现时，RDD针对Transformation操作，提供了对应的继承自RDD的类型例如，map操作会返回MappedRDD,flatMap则返回FlatMappedRDD.执行map或flatMap操作时，不过是将当前RDD对象传递给对应的RDD对象而已。 RDD特性总结 RDD是 Spark的核心，也是整个 Spark的架构基础。它的特性可以总结如下： RDD是不变的（ immutable）数据结构存储。 RDD将数据存储在内存中，从而提供了低延迟性。 RDD是支持跨集群的分布式数据结构。 RDD可以根据记录的Key对结构分区。 RDD提供了粗粒度的操作，并且都支持分区。","categories":[{"name":"内存计算框架spark","slug":"内存计算框架spark","permalink":"http://www.studyz.club/categories/%E5%86%85%E5%AD%98%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6spark/"}],"tags":[{"name":"内存计算框架spark","slug":"内存计算框架spark","permalink":"http://www.studyz.club/tags/%E5%86%85%E5%AD%98%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6spark/"}]},{"title":"内存计算框架--spark简介","slug":"内存计算框架spark","date":"2020-05-18T04:27:57.992Z","updated":"2020-12-01T01:13:46.951Z","comments":true,"path":"posts/b4cafdd/","link":"","permalink":"http://www.studyz.club/posts/b4cafdd/","excerpt":"","text":"Spark架构与集群环境 一，Spark架构与集群环境1.1 概述 Spark是一种与 Hadoop MapReduce类似的开源集群大数据计算分析框架。 Spark基于内存计算，整合了内存计算的单元，所以相对于 hadoop的集群处理方法， Spark在性能方面更具优势。 Spark启用了弹性内存分布式数据集，除了能够提供交互式查询外，还可以优化迭代工作负载。从另一角度来看， Spark可以看作 MapReduce的一种扩展。 MapReduce之所以不擅长迭代式、交互式和流式的计算工作，主要因为它缺乏在计算的各个阶段进行有效的资源共享，针对这一点， Spark创造性地引人了RDD（弹性分布式数据集）来解决这个问题RDD的重要特性之一就是资源共享。Spark基于内存计算，提高了大数据处理的实时性，同时兼具高容错性和可伸缩性，更重要的是， Spark可以部署在大量廉价的硬件之上，形成集群。 Spark是 MapReduce的一种更优的替代方案，可以兼容HDFS等分布式存储层，也可以兼容现有的 Hadoop生态系统，同时弥补 MapReduce的不足。 与 Hadoop MapReduce相比， Spark的优势如下 中间结果：基于 MapReduce的计算引擎通常将中间结果输出到磁盘上，以达到存储和容错的目的。由于任务管道承接的缘故，一切查询操作都会产生很多串联的Stage，这些 Stage输出的中间结果存储于HDFS.而 Spark将执行操作抽象为通用的有向无环图（DAG，可以将多个 Stage的任务串联或者并行执行，而无须Stage中间结果输出到HDFS中。 执行策略： MapReduce在数据 Shuffle之前，需要花费大量时间来排序，而 Spark不需要对所有情景都进行排序。由于采用了DAG的执行计划，每一次输出的中间结果都可以缓存在内存中。 任务调度的开销： MapReduce系统是为了处理长达数小时的批量作业而设计的在某些极端情况下，提交任务的延迟非常高。而 Spark采用了事件驱动的类库AKKA来启动任务，通过线程池复用线程来避免线程启动及切换产生的开销。 更好的容错性：RDD之间维护了血缘关系（ lineage，一且某个RDD失败了，能通过父RDD自动重建，保证了容错性. 高速：基于内存的 Spark计算速度大约是基于磁盘的 Hadoop MapReduce的100倍。 易用：相同的应用程序代码量一般比 Hadoop MapReduce少50%~80%. 提供了丰富的API：与此同时，Spak支持多语言编程，如 Scala、 Python及Java便于开发者在自己熟悉的环境下工作。 Spark自带了80多个算子，同时允许在Spark Shell环境下进行交互式计算，开发者可以像书写单机程序一样开发分布式程序，轻松利用 Spark搭建大数据内存计算平台，并利用内存计算特性，实时处理海量数据。 1.2 Spark生态 Spark大数据计算平台包含许多子模块，构成了整个 Spark的生态系统，其中 Spark为核心。伯克利将整个 Spark的生态系统称为伯克利数据分析栈（BDAS），其结构如图1-1 以下简要介绍BDAS的各个组成部分 1. Spark CoreSpark Core是整个BDAS的核心组件，是一种大数据分布式处理框架，不仅实现了MapReduce的算子map函数和reduce函数及计算模型，还提供如fler、join、group ByKey等更丰富的算子。Spark将分布式数据抽象为弹性分布式数据集（RDD），实现了应用任务调度、RPC、序列化和压缩，并为运行在其上的上层组件提供API.其底层采用 Scala函数式语言书写而成，并且深度借鉴 Scala函数式的编程思想，提供与 Scala类似的编程接口。 2. MesosMesos是 Apache下的开源分布式资源管理框架，被称为分布式系统的内核，提供了类似YARN的功能，实现了高效的资源任务调度。 3. Spark StreamingSpark Streaming是一种构建在 Spark上的实时计算框架，它扩展了Spark处理大规模流式数据的能力。其吞吐量能够超越现有主流流处理框架 Storm，并提供丰富的API用于流数据计算。 4. MLibMLib是 Spark对常用的机器学习算法的实现库，同时包括相关的测试和数据生成器。MLlib目前支持4种常见的机器学习问题：二元分类、回归、聚类以及协同过滤,还包括一个底层的梯度下降优化基础算法。 5. GraphxGraphx是 Spark中用于图和图并行计算的AP，可以认为是 GraphLab和 Pregel在Spark（ Scala）上的重写及优化，与其他分布式图计算框架相比， GraphX最大的贡献是，在 Spark上提供一栈式数据解决方案，可以方便、高效地完成图计算的一整套流水作业。 6. Spark SQLShark是构建在Spark和Hive基础之上的数据仓库。它提供了能够查询Hive中所存储数据的一套SQL接口，兼容现有的 Hive QL语法。熟悉 Hive QL或者SQL的用户可以基于 Shark进行快速的Ad-Hoc,Reporting等类型的SQL查询。由于其底层计算采用了Spatk，性能比 Mapreduce的Hve普遍快2倍以上，当数据全部存储在内存时，要快10倍以上。2014年7月1日， Spark社区推出了 Spark SQL，重新实现了SOL解析等原来Hive完成的工作， Spark SQL在功能上全覆盖了原有的 Shark，且具备更优秀的性能。 7. AlluxioAlluxio（原名 Tachyon）是一个分布式内存文件系统，可以理解为内存中的HDFS为了提供更高的性能，将数据存储剥离 Java Heap.用户可以基于 Alluxio实现RDD或者文件的跨应用共享，并提供高容错机制，保证数据的可靠性。 8. BlinkDBBlinkDB是一个用于在海量数据上进行交互式SQL的近似查询引擎。它允许用户在查询准确性和查询响应时间之间做出权衡，执行相似查询 1.3 Spark架构 传统的单机系统，虽然可以多核共享内存、磁盘等资源，但是当计算与存储能力无法满足大规模数据处理的需要时，面对自身CPU与存储无法扩展的先天限制，单机系统就力不从心了。 1.分布式系统架构所谓的分布式系统，即为在网络互连的多个计算单元执行任务的软件系统，一般包括分布式操作系统、分布式数据库系统、分布式应用程序等。本书介绍的 Spark分布式计算框架，可以看作分布式软件系统的组成部分，基于 Spark，开发者可以编写分布式计算程序 直观来看，大规模分布式系统由许多计算单元构成，每个计算单元之间松耦合。同时，每个计算单元都包含自己的CPU、内存、总线及硬盘等私有计算资源。这种分布式结构的最大特点在于不共享资源，与此同时，计算节点可以无限制扩展，计算能力和存储能力也因而得到巨大增长。但是由于分布式架构在资源共享方面的先天缺陷，开发者在书写和优化程序时应引起注意。分布式系统架构如图1-2所示。 为了减少网络I/O开销，分布式计算的一个核心原则是数据应该尽量做到本地计算。在计算过程中，每个计算单元之间需要传输信息，因此在信息传输较少时，分布式系统可以利用资源无限扩展的优势达到高效率，这也是分布式系统的优势。目前分布式系统在数据挖掘和决策支持等方面有着广泛的应用。 Spark正是基于这种分布式并行架构而产生，也可以利用分布式架构的优势，根据需要，对计算能力和存储能力进行扩展，以应对处理海量数据带来的挑战。同时，Spark的快速及容错等特性，让数据处理分析显得游刃有余。 2. spark架构Spark架构采用了分布式计算中的 Master- Slave模型。集群中运行 Master进程的节点称为 Master，同样，集群中含有 Worker进程的节点为 Slave. Master负责控制整个集群的运行； Worker节点相当于分布式系统中的计算节点，它接收Master节点指令并返回计算进程到 Master； Executor负责任务的执行； Client是用户提交应用的客户端； Driver负责协调提交后的分布式应用。具体架构如图1-3所示 在 Spark应用的执行过程中， Driver和 Worker是相互对应的。 Driver是应用逻辑执行的起点，负责Task任务的分发和调度；Worker负责管理计算节点并创建Executor来并行处理Task任务。Task执行过程中所需的文件和包由 Driver序列化后传输给对应的Worker节点，Executor对相应分区的任务进行处理。 下面介绍 Spark架构中的组件。 1） Client：提交应用的客户端。 2） Driver：执行 Application中的main函数并创建 Spark Contexto 3） ClusterManager：在YARN模式中为资源管理器。在 Standalone模式中为 Maste（主节点），控制整个集群 4） Worker：从节点，负责控制计算节点。启动 Executor或 Driver，在YARN模式中为 NodeManager 5） Executor：在计算节点上执行任务的组件。 6） Spark Context：应用的上下文，控制应用的生命周期 7）RDD：弹性分布式数据集， Spark的基本计算单元，一组RDD可形成有向无环图。 8） DAG Scheduler：根据应用构建基于 Stage的DAG，并将 Stage提交给Tas 9） Task Scheduler：将Task分发给 Executor执行。 10） SparkEnv：线程级别的上下文，存储运行时重要组件的应用，具体如下 ① SparkConf：存储配置信息 ② BroadcastManager：负责广播变量的控制及元信息的存储。 ③ BlockManager：负责 Block的管理、创建和查找。 ④ MetricsSystem：监控运行时的性能指标。 ⑤ MapOutput Tracker：负责 shuffle元信息的存储 Spark架构揭示了 Spark的具体流程如下： 1）用户在 Client提交了应用 2） Master找到 Worker，并启动 Driver. 3） Driver向资源管理器（YARN模式）或者 Master（ Standalone模式）申请资源，并将应用转化为 RDD Graph 4） DAG Scheduler将 RDD Graph转化为Sage的有向无环图提交给 Task Scheduler 5） Task Scheduler提交任务给 Executor执行。 1.4 Spark运行逻辑下面举例说明 Spark的运行逻辑，如图1-4所示，在 Action算子被触发之后，所有累积的算子会形成一个有向无环图DAG. Spark会根据RDD之间不同的依赖关系形成Stage，每个 Stage都包含一系列函数执行流水线。图1-4中A、B、C、D、E、F为不同的RDD，RDD内的方框为RDD的分区。 图1-4中的运行逻辑如下： 1）数据从HDFS输入 Spark 2）RDD A、RDD C经过 flatMap与Map操作后，分别转换为RDD B和RDD D 3）RDDD经过 reduceByKey操作转换为RDD E. 4）RDD B与RDD E进行join操作转换为RDD F. 5）RDD F通过函数 saveAsSequenceFile输出保存到HDFS中。 1.5 在Linux中部署SparkSpark安装部署比较简单，用户可以登录其官方网站（http://spark.apache.org）下载Spak最新版本或历史版本，也可以查阅Spark相关文档作为参考。Spark使用了 Hadoop的HDFS作为持久化存储层，因此安装 Spark时，应先安装与Spark版本相兼容的 Hadoop。 ① 将下载好的安装包放到Linux中1spark-2.4.5-bin-hadoop2.7.tgz ② 解压缩到指定的目录 为了避免权限问题，文件最好放在自己的home文件夹的usr里，不要放在根目录。 1[root@master opt]# tar -zxvf spark-2.4.5-bin-hadoop2.7.tgz -C &#x2F;usr&#x2F;local&#x2F; ③ 配置环境变量(可配可不配)1.5.1 Spark的部署方式在local模式下就可以启动了 运行Spark自带的实例 1[root@master spark2.4.5]# .&#x2F;bin&#x2F;spark-submit --master local[3] --class org.apache.spark.examples.SparkPi examples&#x2F;jars&#x2F;spark-examples_2.11-2.4.5.jar 运行结果 1220&#x2F;05&#x2F;18 19:05:49 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 2.134963 sPi is roughly 3.1410557052785264 如果报错,要检查自己Hadoop能否正常启动使用 1ERROR SparkContext: Error initializing SparkContext. Standalone模式(伪分布式) 即独立模式，自带完整的服务，可单独部署到一个集群中，无需依赖任何其他资源管理系统。从一定程度上说，该模式是其他两种的基础。借鉴Spark开发模式，我们可以得到一种开发新型计算框架的一般思路：先设计出它的standalone模式，为了快速开发，起初不需要考虑服务（比如master/slave）的容错性，之后再开发相应的wrapper，将stanlone模式下的服务原封不动的部署到资源管理系统yarn或者mesos上，由资源管理系统负责服务本身的容错。目前Spark在standalone模式下是没有任何单点故障问题的，这是借助zookeeper实现的，思想类似于Hbase master单点故障解决方案。将Spark standalone与MapReduce比较，会发现它们两个在架构上是完全一致的： 修改配置文件 12[root@master conf]# cp slaves.template slaves[root@master conf]# cp spark-env.sh.template spark-env.sh 修改slaves文件将主机名添加进去(master) 1[root@master conf]# vi slaves 修改spark-env.sh文件添加 1SPARK_MASTER_HOST&#x3D;master 启动服务 这里务必./sbin目录下，要不然启动Hadoop。 1234567[root@master spark2.4.5]# .&#x2F;sbin&#x2F;start-all.shstarting org.apache.spark.deploy.master.Master, logging to &#x2F;usr&#x2F;local&#x2F;spark2.4.5&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.master.Master-1-master.outmaster: starting org.apache.spark.deploy.worker.Worker, logging to &#x2F;usr&#x2F;local&#x2F;spark2.4.5&#x2F;logs&#x2F;spark-root-org.apache.spark.deploy.worker.Worker-1-master.out[root@master spark2.4.5]# jps75619 Master75703 Worker75863 Jps 提交任务 1[root@master spark2.4.5]# .&#x2F;bin&#x2F;spark-submit --master spark:&#x2F;&#x2F;master:7077 --class org.apache.spark.examples.SparkPi examples&#x2F;jars&#x2F;spark-examples_2.11-2.4.5.jar Spark on Yarn模式: 完全分布式 Driver：提交程序之后启动的第一个进程。 client：客户端模式 在哪台节点提交程序，就在那台节点上启动 Driver进程 更适用于测试完全分布式的环境。 cluster：集群模式 不管在哪台节点提交程序，都要经过yarn进行资源调配。 更适用于已经上线的项目。 安装scala 为了避免适配问题这里下载scala2.12.0版本 登录官网下载scala-2.12.0.tgz ① 解压缩 ② 修改配置文件 1[root@slave2 local]# vi &#x2F;etc&#x2F;profile 添加内容 12export SCALA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;scalaexport PATH&#x3D;$PATH:$SCALA_HOME&#x2F;bin 使配置文件重新生效 1[root@master local]# source &#x2F;etc&#x2F;profile 检验 12[root@master scala]# scala -versionScala code runner version 2.12.0 -- Copyright 2002-2016, LAMP&#x2F;EPFL and Lightbend, Inc. 2)配置spark ① 修改配置文件 1[root@master conf]# vi spark-env.sh 添加 12345export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_91export SCALA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;scalaexport SPARK_MASTER_HOST&#x3D;masterexport SPARK_DAEMON_MEMORY&#x3D;1gHADOOP_CONF_DIR&#x3D;&#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;etc&#x2F;hadoop 添加worker节点(不需要添加主节点) 1vi slaves 12slave1slave2 将文件发到各个节点 1scp -r &#x2F;usr&#x2F;local&#x2F;spark2.4.5 slave1:&#x2F;usr&#x2F;local&#x2F; 启动服务 1zkServer.sh start 启动hadoop集群 1start-all.sh 启动spark(在sbin目录下执行命令) 1[root@master sbin]# .&#x2F;start-all.sh 在web界面查看情况master:8080 ④ 修改Spark配置文件(/conf)(在local模式不需要修改配置文件)123[root@master conf]# mv spark-env.sh.template spark-env.sh[root@master conf]# mv spark-defaults.conf.template spark-defaults.conf[root@master conf]# mv slaves.template slaves spark-env.sh 1.6 Windows环境下开发 工具idea(注意scala的安装，版本太高会报错) 将下载的Spark安装包解压提取压缩文件里的jars目录 打开idea文件–&gt;项目结构–&gt;Libraries,点击右侧的加号，找到刚才解压的目录按住Shift全选该目录下的所有jar包，导入即可。 windows环境下的测试实例 123456789101112131415package spark.chapter01import org.apache.spark.&#123;SparkConf, SparkContext&#125;object wordCount &#123; def main(args: Array[String]): Unit &#x3D; &#123; val conf &#x3D; new SparkConf().setAppName(&quot;app&quot;).setMaster(&quot;local[3]&quot;) val sc &#x3D; new SparkContext(conf) sc.textFile(&quot;E:&#x2F;&#x2F;test&#x2F;worldCountTest.txt&quot;).flatMap(_.split(&quot;,&quot;)) .map((_,1)) .reduceByKey(_+_) .foreach(println) &#125;&#125;","categories":[{"name":"内存计算框架spark","slug":"内存计算框架spark","permalink":"http://www.studyz.club/categories/%E5%86%85%E5%AD%98%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6spark/"}],"tags":[{"name":"内存计算框架spark","slug":"内存计算框架spark","permalink":"http://www.studyz.club/tags/%E5%86%85%E5%AD%98%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6spark/"}]},{"title":"数据采集与清洗1","slug":"数据采集与清洗1","date":"2020-05-12T07:48:37.445Z","updated":"2020-05-13T08:26:13.022Z","comments":true,"path":"posts/4c4d006d/","link":"","permalink":"http://www.studyz.club/posts/4c4d006d/","excerpt":"","text":"","categories":[{"name":"数据采集与清洗","slug":"数据采集与清洗","permalink":"http://www.studyz.club/categories/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E4%B8%8E%E6%B8%85%E6%B4%97/"}],"tags":[{"name":"数据采集与清洗","slug":"数据采集与清洗","permalink":"http://www.studyz.club/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E4%B8%8E%E6%B8%85%E6%B4%97/"}]},{"title":"Scala学习之六函数式编程基础","slug":"Scala学习-函数式编程基础","date":"2020-04-02T08:44:15.175Z","updated":"2020-05-13T08:26:13.019Z","comments":true,"path":"posts/7a630bb7/","link":"","permalink":"http://www.studyz.club/posts/7a630bb7/","excerpt":"","text":"一，函数式编程基础 1.1 函数式编程介绍 1.2 函数的定义 一，函数式编程基础1.1 函数式编程介绍 在scala中，函数式编程和面向对象编程融合在一起，学习函数式编程式需要oop的知识，同样学习oop需要函数式编程的基础。 几个概念的说明 在scala中，方法和函数几乎可以等同(比如他们的定义、使用、运行机制都一样的)，只是函数的使用方式更加的灵活多样。 函数式编程是从编程方式(范式)的角度来谈的，可以这样理解：函数式编程把函数当做一等公民，充分利用函数、 支持的函数的多种使用方式。比如： 在Scala当中，函数是一等公民，像变量一样，既可以作为函数的参数使用，也可以将函数赋值给一个变量. ，函数的创建不用依赖于类或者对象，而在Java当中，函数的创建则要依赖于类、抽象类或者接口. 面向对象编程是以对象为基础的编程方式。 在scala中函数式编程和面向对象编程融合在一起了 。 在学习Scala中将方法、函数、函数式编程和面向对象编程关系分析图： 实例1234567891011121314151617181920212223242526object Method2Function &#123; def main(args: Array[String]): Unit &#x3D; &#123; &#x2F;&#x2F;使用方法 &#x2F;&#x2F;先创建一个对象 val dog &#x3D; new Dog println(dog.sum(10,20)) &#x2F;&#x2F;方法转成函数 val f1 &#x3D; dog.sum _ println(&quot;f1 &#x3D; &quot; + f1) println(&quot;f1 &#x3D; &quot; + f1(10,20)) &#x2F;&#x2F;函数,求两个数的和 val f2 &#x3D; (n1:Int,n2:Int) &#x3D;&gt; &#123; n1 + n2 &#125; println(&quot;f2 &#x3D; &quot;+f2) println(&quot;f2 &#x3D; &quot; + f2(5,6)) &#125;&#125;class Dog&#123; &#x2F;&#x2F;方法 def sum(n1:Int,n2:Int):Int &#x3D; &#123; n1 + n2 &#125;&#125; 运行结果 1234530f1 &#x3D; com.hanshu.Method2Function$$$Lambda$3&#x2F;761960786@3930015af1 &#x3D; 30f2 &#x3D; com.hanshu.Method2Function$$$Lambda$4&#x2F;466002798@1ff8b8ff2 &#x3D; 11 函数式编程介绍 “函数式编程”是一种”编程范式”（programming paradigm）。 它属于”结构化编程”的一种，主要思想是把运算过程尽量写成一系列嵌套的函数调用。 函数式编程中，将函数也当做数据类型，因此可以接受函数当作输入（参数）和输出（返回值）。 函数式编程中，最重要的就是函数。 为什么需要函数问题：输入两个数,再输入一个运算符(+,-)，得到结果. 先使用传统的方式来解决，看看有什么问题没有？ n1 1234567891011121314151617val n2 &#x3D; 20var oper &#x3D; &quot;-&quot;if (oper &#x3D;&#x3D; &quot;+&quot;) &#123;println(&quot;res&#x3D;&quot; + (n1 + n2))&#125; else if (oper &#x3D;&#x3D; &quot;-&quot;) &#123;println(&quot;res&#x3D;&quot; + (n1 - n2))&#125;println(&quot;------做了其他的工作...&quot;)val n3 &#x3D; 10val n4 &#x3D; 20oper &#x3D; &quot;-&quot;if (oper &#x3D;&#x3D; &quot;+&quot;) &#123;println(&quot;res&#x3D;&quot; + (n1 + n2))&#125; else if (oper &#x3D;&#x3D; &quot;-&quot;) &#123;println(&quot;res&#x3D;&quot; + (n1 - n2))&#125; ① 代码冗余 ②不利于代码的维护 将上面的代码抽出一部分，形成代码，就是函数 为完成某一功能的程序指令(语句)的集合,称为函数。1.2 函数的定义基本语法1234def 函数名 ([参数名: 参数类型], ...)[[: 返回值类型] &#x3D;] &#123; 语句... return 返回值&#125; 说明: 函数声明关键字为def (definition) [参数名: 参数类型], …：表示函数的输入(就是参数列表), 可以没有。 如果有，多个参数使用逗号间隔 函数中的语句：表示为了实现某一功能代码块 函数可以有返回值,也可以没有 返回值形式1: : 返回值类型 = 返回值形式2: = 表示返回值类型不确定，使用类型推导完成 返回值形式3: 表示没有返回值，return 不生效 如果没有return ,默认以执行到最后一行的结果作为返回值 函数-调用机制举例:getSum 计算两个数的和,并返回结果。 1234567891011object Test01 &#123; def main(args: Array[String]): Unit &#x3D; &#123; val n1 &#x3D; 1 val n2 &#x3D; 3 val res &#x3D; sum(n1, n2) println(&quot;res&#x3D;&quot; + res) &#125; def sum(n1: Int, n2: Int): Int &#x3D; &#123; return n1 + n2 &#125;&#125; 函数-递归调用 一个函数在函数体内又调用了本身，我们称为递归调用 123456def test (n: Int) &#123; if (n &gt; 2) &#123; test (n - 1) &#125; println(&quot;n&#x3D;&quot; + n) &#x2F;&#x2F; &#125; 1234567def test2 (n: Int) &#123; if (n &gt; 2) &#123; test2 (n - 1) &#125;else &#123; println(&quot;n&#x3D;&quot; + n) &#125; &#125; 函数递归需要遵守的重要原则（总结）: 程序执行一个函数时，就创建一个新的受保护的独立空间(新函数栈) 函数的局部变量是独立的，不会相互影响 递归必须向退出递归的条件逼近，否则就是无限递归，死龟了:) 当一个函数执行完毕，或者遇到return，就会返回，遵守谁调用，就将结果返回给谁。","categories":[{"name":"Scala","slug":"Scala","permalink":"http://www.studyz.club/categories/Scala/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://www.studyz.club/tags/Scala/"},{"name":"Scala学习基础知识","slug":"Scala学习基础知识","permalink":"http://www.studyz.club/tags/Scala%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}]},{"title":"Scala学习之五运算符及程序流程控制","slug":"Scala学习-运算符及程序流程控制","date":"2020-03-23T14:02:49.449Z","updated":"2020-05-13T08:26:13.026Z","comments":true,"path":"posts/b07bb359/","link":"","permalink":"http://www.studyz.club/posts/b07bb359/","excerpt":"","text":"一，运算符介绍 1.1 算术运算符一览 1.2 关系运算符(比较运算符) 1.3 逻辑运算符 1.4 赋值运算符 1.5 位运算符 1.6 运算符的特别说明 1.6.1 1.6.1 运算符优先级 1.7 键盘输入语句 二，程序流程控 2.1 顺序控制 2.2 分支控制 2.3 for循环控制 2.4 while循环控制 一，运算符介绍 运算符是一种特殊的符号，用以表示数据的运算、赋值和比较等。①算术运算符，②赋值运算符，③比较运算符(关系运算符)，④逻辑运算符，⑤位运算符 算术运算符(arithmetic)是对数值类型的变量进行运算的，在Scala程序中使用的非常多。 1.1 算术运算符一览 运算符 运算 范例 结果 | 正号 | +3 | 3 | 负号 | b=4; -b | -4 | 加 | 5+5 | 10 | 减 | 6-4 | 2 | 乘 | 3*4 | 12/ | 除 | 5/5 | 1% | 取模(取余) | 7%5 | 2 | 字符串相加 | “He”+”llo” | “Hello” 123456789101112131415161718192021222324&#x2F;&#x2F; &#x2F; 的使用 var r1 : Int &#x3D; 10 &#x2F; 3 &#x2F;&#x2F; 3 println(&quot;r1&#x3D;&quot; + r1) var r2 : Double &#x3D; 10 &#x2F; 3 &#x2F;&#x2F; 3.0 println(&quot;r2&#x3D;&quot; + r2) var r3 : Double &#x3D; 10.0 &#x2F; 3 &#x2F;&#x2F; 3.333333 println(&quot;r3&#x3D;&quot; + r3 ) println(&quot;r3&#x3D;&quot; + r3.formatted(&quot;%.2f&quot;) )&#x2F;&#x2F; 3.33 &#x2F;&#x2F; % 的使用 &#x2F;&#x2F;1. % 的运算的原则: a % b &#x3D; a - a&#x2F;b * b println(10 % 3) &#x2F;&#x2F; 1 println(-10 % 3) &#x2F;&#x2F; -1 &#x2F;&#x2F; -10 % 3 &#x3D; (-10)- (-3) * 3 &#x3D; -10 + 9 &#x3D; -1 println(-10 % -3 ) &#x2F;&#x2F; -1 &#x2F;&#x2F; -10 % -3 &#x3D; (-10)- (3) * -3 &#x3D; -10 + 9 &#x3D; -1 println(10 % -3 ) &#x2F;&#x2F; 1 &#x2F;&#x2F; ++ 和 -- &#x2F;&#x2F; 说明,在scala中没有 ++ 和 --， 而使用 +&#x3D;1 和 -&#x3D; 1 var num1 &#x3D; 10 &#x2F;&#x2F;num1++ 错误 num1 +&#x3D; 1 &#x2F;&#x2F; 替代 num1++ &#x2F;&#x2F;num-- 错误 num1 -&#x3D; 1 &#x2F;&#x2F; 替代 num1-- println(num1) 对于除号“/”，它的整数除和小数除是有区别的：整数之间做除法时，只保留整数部分而舍弃小数部分。 例如：var x : Int = 10/3 ,结果是 3 当对一个数取模时，可以等价 a%b=a-a/b*b ， 这样我们可以看到取模的一个本质运算(和java 的取模规则一样)。 注意：Scala中没有++、--操作符，需要通过+=、-=来实现同样的效果 实例：1，假如还有97天放假，问：xx个星期零xx天 12345678910111213def main(args: Array[String]): Unit &#x3D; &#123; &#x2F;* 假如还有97天放假，问：xx个星期零xx天 1.搞清楚需求(读题) 2.思路分析 (1) 变量保存97 (2) 使用 &#x2F;7 得到 几个星期 (3) xx天 使用 % 3.代码实现 *&#x2F; val days &#x3D; 97 printf(&quot;统计结果是 %d个星期零%d天&quot;, days &#x2F; 7, days % 7) &#125; 实例2： 定义一个变量保存华氏温度，华氏温度转换摄氏温度的公式为：5/9*(华氏温度-100),请求出华氏温度对应的摄氏温度。[测试：232.5] 12345678&#x2F;* 分析 1. 变量保存华氏温度， 变量保存摄氏温度 2. 公式有，就直接套用 *&#x2F; val huashi &#x3D; 232.5 val sheshi &#x3D; 5.0 &#x2F; 9 * (huashi - 100) println(&quot;对应的摄氏温度&quot; + sheshi.formatted(&quot;%.2f&quot;)) 1.2 关系运算符(比较运算符)关系运算符一览 123456789var a &#x3D; 9 var b &#x3D; 8 println(a&gt;b) &#x2F;&#x2F; true println(a&gt;&#x3D;b) &#x2F;&#x2F; true println(a&lt;&#x3D;b) &#x2F;&#x2F; false println(a&lt;b) &#x2F;&#x2F; false println(a&#x3D;&#x3D;b) &#x2F;&#x2F; false println(a!&#x3D;b) &#x2F;&#x2F; true var flag : Boolean &#x3D; a&gt;b &#x2F;&#x2F; true 细节说明 关系运算符的结果都是Boolean型，也就是要么是true，要么是false。 关系运算符组成的表达式，我们称为关系表达式。 a &gt; b 比较运算符“==”不能误写成“=” 使用陷阱: 如果两个浮点数进行比较，应当保证数据类型一致. 1.3 逻辑运算符 用于连接多个条件（一般来讲就是关系表达式），最终的结果也是一个Boolean值。 逻辑运算符一览 假定变量 A 为 true，B 为 false 12345var a &#x3D; true var b &#x3D; false println(&quot;a &amp;&amp; b &#x3D; &quot; + (a &amp;&amp; b)) &#x2F;&#x2F; false println(&quot;a || b &#x3D; &quot; + (a || b)) &#x2F;&#x2F; true println(&quot;!(a &amp;&amp; b) &#x3D; &quot; + !(a &amp;&amp; b)) &#x2F;&#x2F; true 1.4 赋值运算符 赋值运算符就是将某个运算后的值，赋给指定的变量。 赋值运算符的分类 运算符 描述 实例 = 简单的赋值运算符，将一个表达式的值赋给一个左值 C = A + B 将 A + B 表达式结果赋值给 C += 相加后再赋值 C += A 等于 C = C + A -= 相减后再赋值 C -= A 等于 C = C - A *= 相乘后再赋值 C *= A 等于 C = C * A /= 相除后再赋值 C /= A 等于 C = C / A %= 求余后再赋值 C %= A 等于 C = C % A &lt;&lt;= 左移后赋值 C &lt;&lt;= 2 等于 C = C &lt;&lt; 2 = | 右移后赋值 | C &gt;&gt;= 2 等于 C = C &gt;&gt; 2&amp;= | 按位与后赋值 | C &amp;= 2 等于 C = C &amp; 2^= | 按位异或后赋值 | C ^= 2 等于 C = C ^ 2|= | 按位或后赋值 | C |= 2 等于 C = C | 2 这部分的赋值运算涉及到二进制相关知识，其运行的规则和Java一样。 案例演示:交换两个数的值。 12345678var a &#x3D; 10var b &#x3D; 99println(&quot;a&#x3D;&quot;+a+&quot;\\tb&#x3D;&quot;+b)&#x2F;&#x2F;交换val t &#x3D; aa &#x3D; bb &#x3D; tprintln(&quot;a&#x3D;&quot;+a+&quot;\\tb&#x3D;&quot;+b) 赋值运算符特点 运算顺序从右往左 赋值运算符的左边 只能是变量,右边 可以是变量、表达式、常量值/字面量 复合赋值运算符等价于下面的效果 比如：a+=3 等价于a=a+3 1.5 位运算符 运算符 描述 实例 &amp; 按位与运算符 (a &amp; b) 输出结果 12 ，二进制解释： 0000 1100 按位或运算符 ^ 按位异或运算符 (a ^ b) 输出结果 49 ，二进制解释： 0011 0001 ~ 按位取反运算符 (~a ) 输出结果 -61 ，二进制解释： 1100 0011， 在一个有符号二进制数的补码形式。 &lt;&lt; 左移动运算符 a &lt;&lt; 2 输出结果 240 ，二进制解释： 1111 0000 | 右移动运算符 | a &gt;&gt; 2 输出结果 15 ，二进制解释： 0000 1111 | 无符号右移 | A &gt;&gt;&gt;2 输出结果 15, 二进制解释: 0000 1111 说明: 位运算符的规则和Java一样 1.6 运算符的特别说明Scala不支持三目运算符 , 在Scala 中使用 if – else 的方式实现。12val num &#x3D; 5 &gt; 4 ? 5 : 4 &#x2F;&#x2F;没有val num &#x3D; if (5&gt;4) 5 else 4 案例1：求两个数的最大值案例2：求三个数的最大值 1.6.1 1.6.1 运算符优先级 运算符有不同的优先级，所谓优先级就是表达式运算中的运算顺序。如右表，上一行运算符总优先于下一行。 只有单目运算符、赋值运算符是从右向左运算的。 运算符的优先级和Java一样。 小结运算符的优先级 1.() [] 2.单目运算 3.算术运算符 4.移位运算 5.比较运算符(关系运算符) 6.位运算 7.关系运算符 8.赋值运算 9., (类别数字越小，优先级越高)类别 | 运算符 | 关联性—|—|—1 | () [] | 左到右2 | ! ~ | 右到左3 | * / % | 左到右4 | + - | 左到右5 | &gt;&gt; &gt;&gt;&gt; &lt;&lt; | 左到右6 | &gt; &gt;= &lt; &lt;= | 左到右7 | == != | 左到右8 | &amp; | 左到右9 | ^ | 左到右10 | | | 左到右11 | &amp;&amp; | 左到右12 | || | 左到右13 | = += -= *= /= %= &gt;&gt;= &lt;&lt;= &amp;= ^= |= | 右到左14 | , | 左到右 1.7 键盘输入语句 在编程中，需要接收用户输入的数据，就可以使用键盘输入语句来获取。 步骤 ： 导入该类的所在包 创建该类对象（声明变量） 调用里面的功能123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.jisuanfuimport scala.io.StdInobject Demo02 &#123; def main(args: Array[String]): Unit &#x3D; &#123; &#x2F;* 要求：可以从控制台接收用户信息，【姓名，年龄，薪水】 *&#x2F; println(&quot;请输入姓名&quot;) val name &#x3D; StdIn.readLine() println(&quot;请输入年龄&quot;) val age &#x3D; StdIn.readInt() println(&quot;请输入薪水&quot;) val sal &#x3D; StdIn.readDouble() printf(&quot;用户的信息为 name&#x3D;%s age&#x3D;%d sal&#x3D;%.2f&quot;, name, age, sal) &#x2F;&#x2F;Cat.sayHi() &#x2F;&#x2F;Cat.sayHello() &#x2F;&#x2F;Cat.sayHi() hi() &#125; &#x2F;&#x2F;定义方法 def hi(): Unit &#x3D; &#123; println(&quot;hi&quot;) &#125;&#125;&#x2F;&#x2F;声明了一个对象(伴生对象), 讲解oop时，还要深入系统的讲解object Cat extends AAA &#123; &#x2F;&#x2F;方法 def sayHi(): Unit &#x3D; &#123; println(&quot;小狗汪汪叫....&quot;) &#125;&#125;trait AAA &#123; &#x2F;&#x2F;AAA是特质，等价于java中的interface + abstract class def sayHello(): Unit &#x3D; &#123; println(&quot;AAA sayHello&quot;) &#125;&#125; 二，程序流程控 在程序中，程序运行的流程控制决定程序是如何执行的，是我们必须掌握的，主要有三大流程控制语句。 温馨提示： Scala语言中控制结构和Java语言中的控制结构基本相同，在不考虑特殊应用场景的情况下，代码书写方式以及理解方式都没有太大的区别 2.1 顺序控制 程序从上到下逐行地执行，中间没有任何判断和跳转。 顺序控制举例和注意事项 Scala中定义变量时采用合法的前向引用。如：1234def main(args : Array[String]) : Unit &#x3D; &#123; var num1 &#x3D; 12 var num2 &#x3D; num1 + 2&#125; 错误形式：1234def main(args : Array[String]) : Unit &#x3D; &#123; var num2 &#x3D; num1 + 2 var num1 &#x3D; 12&#125; 2.2 分支控制分支控制if-else介绍 让程序有选择的的执行,分支控制有三种:单分支 双分支 多分支 2.2.1 单分支基本语法 123if (条件表达式) &#123; 执行代码块&#125; 说明：当条件表达式为ture 时，就会执行 &#123; &#125; 的代码。 编写一个程序,可以输入人的年龄,如果该同志的年龄大于18岁,则输出 “age &gt; 18” 1234val age &#x3D; 20if (age &gt; 18) &#123;println(&quot;age &gt; 18&quot;)&#125; 1234567891011121314151617181920&#x2F;&#x2F;import scala.io.StdIn &#x2F;&#x2F;单独的引入一个StdInimport scala.io._ &#x2F;&#x2F; _表示将 scala.io 包的所有内容一起引入object Demo01 &#123; def main(args: Array[String]): Unit &#x3D; &#123; println(&quot;输入年龄&quot;) val age &#x3D; StdIn.readInt() if (age &gt; 18) &#123; println(&quot;age &gt; 18&quot;) &#125; &#x2F;&#x2F;小的技巧，如何查看某个包下包含的内容 &#x2F;&#x2F;1.比如我们想看 scala.io 包有什么内容 &#x2F;&#x2F;2.将光标放在 io上即可，输入ctrl +b &#x2F;&#x2F;3.将光标放在 StdIn上即可，输入ctrl +b,看的是StdIn源码 scala.io.StdIn &#125;&#125; 2.2.2 分支控制if-else基本语法 12345if (条件表达式) &#123;执行代码块1 &#125; else &#123;执行代码块2 &#125; 说明：当条件表达式成立，即执行代码块1，否则执行代码块2. 123456789101112object Demo02 &#123; def main(args: Array[String]): Unit &#x3D; &#123; val age &#x3D; 6 if (age &gt; 18) &#123; println(&quot;age &gt; 18&quot;) &#125; else &#123; println(&quot;age &lt;&#x3D; 18&quot;) &#125; &#125;&#125; 练习 1234567891011121314151617181920212223242526272829object Exercise01 &#123; def main(args: Array[String]): Unit &#x3D; &#123; &#x2F;* 【选作】定义两个变量Int，判断二者的和，是否既能被3又能被5整除，打印提示信息 *&#x2F; val num1 &#x3D; 10 val num2 &#x3D; 5 val sum &#x3D; num1 + num2 if (sum % 3 &#x3D;&#x3D; 0 &amp;&amp; sum % 5 &#x3D;&#x3D; 0) &#123; println(&quot;能被3又能被5整除&quot;) &#125; else &#123; println(&quot;能被3又能被5整除 不成立~&quot;) &#125; &#x2F;* 判断一个年份是否是闰年，闰年的条件是符合下面二者之一：(1)年份能被4整除，但不能被100整除；(2)能被400整除 *&#x2F; &#x2F;&#x2F;定义一个变量保存年份 val year &#x3D; 2018 if ((year % 4 &#x3D;&#x3D; 0 &amp;&amp; year % 100 !&#x3D; 0) || (year % 400 &#x3D;&#x3D; 0)) &#123; println(s&quot;$&#123;year&#125; 是闰年...&quot;) &#125; else &#123; println(s&quot;$&#123;year&#125; 不是闰年&quot;) &#125; &#125;&#125; 多分支1234567891011 if (条件表达式1) &#123;执行代码块1 &#125; else if (条件表达式2) &#123;执行代码块2 &#125; …… else &#123;执行代码块n &#125; 练习 12345678910111213141516171819202122232425262728293031323334353637import scala.math._ &#x2F;&#x2F; _ 表示将scala.math 的所有内容导入object Exercise02 &#123; def main(args: Array[String]): Unit &#x3D; &#123; &#x2F;* 求ax2+bx+c&#x3D;0方程的根。a,b,c分别为函数的参数，如果：b2-4ac&gt;0，则有两个解；b2-4ac&#x3D;0，则有一个解；b2-4ac&lt;0，则无解； [a&#x3D;3 b&#x3D;100 c&#x3D;6]提示1：x1&#x3D;(-b+sqrt(b2-4ac))&#x2F;2a X2&#x3D;(-b-sqrt(b2-4ac))&#x2F;2a提示2：sqrt(num) 在 scala 包中(默认引入的) 的math 的包对象有很多方法直接可用. 思路的分析 1. 定义三个变量a,b,c 2. 使用多分支完成 3. 因为 b2-4ac会多次使用，因此我们可以先计算，并保持到变量中 4. 判断，写逻辑 *&#x2F; val a &#x3D; 3 val b &#x3D; 100 val c &#x3D; 6 val m &#x3D; b * b - 4 * a * c var x1 &#x3D; 0.0 var x2 &#x3D; 0.0 if (m &gt; 0) &#123; x1 &#x3D; (-b + sqrt(m)) &#x2F; 2 * a x2 &#x3D; (-b - sqrt(m)) &#x2F; 2 * a println(&quot;有两个解 x1&#x3D;&quot; + x1.formatted(&quot;%.2f&quot;) + &quot;x2&#x3D;&quot; + x2.formatted(&quot;%.2f&quot;)) &#125; else if (m &#x3D;&#x3D; 0) &#123; x1 &#x3D; (-b + sqrt(m)) &#x2F; 2 * a println(&quot;有一个解 x1&#x3D;&quot; + x1) &#125; else &#123; println(&quot;无解..&quot;) &#125; &#125;&#125; 说明：当条件表达式1成立时，即执行代码块1，如果表达式1不成立，才去判断表达式2是否成立，如果表达式2成立，就执行代码块2，以此类推，如果所有的表达式都不成立，则执行else 的代码块，注意，只能有一个执行入口。 分支控制if-else 注意事项 嵌套分支 在一个分支结构中又完整的嵌套了另一个完整的分支结构，里面的分支的结构称为内层分支外面的分支结构称为外层分支。嵌套分支不要超过3层 基本语法 12345if()&#123; if()&#123; &#125;else&#123; &#125;&#125; 练习 1234567891011121314151617181920212223import scala.io.StdInobject Exercise04 &#123; def main(args: Array[String]): Unit &#x3D; &#123; &#x2F;* 参加百米运动会，如果用时8秒以内进入决赛，否则提示淘汰。并且根据性别提示进入男子组或女子组。【可以让学员先练习下5min】, 输入成绩和性别，进行判断。 *&#x2F; println(&quot;请输入运动员的成绩&quot;) val speed &#x3D; StdIn.readDouble() if (speed &lt;&#x3D; 8) &#123; println(&quot;请输入性别&quot;) val gender &#x3D; StdIn.readChar() if (gender &#x3D;&#x3D; &#39;男&#39;) &#123; println(&quot;进入男子组&quot;) &#125; else &#123; println(&quot;进入女子组&quot;) &#125; &#125; else &#123; println(&quot;你被淘汰...&quot;) &#125; &#125;&#125; switch分支结构在scala中没有switch,而是使用模式匹配来处理。模式匹配涉及到的知识点较为综合，因此我们放在后面讲解。match-case 2.3 for循环控制基本介绍 Scala 也为for 循环这一常见的控制结构提供了非常多的特性，这些for 循环的特性被称为for 推导式（for comprehension）或for 表达式（for expression） 范围数据循环方式1基本案例 1234for(i &lt;- 1 to 3)&#123; print(i + &quot; &quot;)&#125;println() 说明 i 表示循环的变量， &lt;- 规定好 to 规定 i 将会从 1-3 循环， 前后闭合12345678910111213141516171819202122object ForDemo01 &#123; def main(args: Array[String]): Unit &#x3D; &#123; &#x2F;&#x2F;输出10句 &quot;hello,world!&quot; val start &#x3D; 1 val end &#x3D; 10 &#x2F;&#x2F;说明 &#x2F;&#x2F;1. start 从哪个数开始循环 &#x2F;&#x2F;2. to 是关键字 &#x2F;&#x2F;3. end 循环结束的值 &#x2F;&#x2F;4. start to end 表示前后闭合 for (i &lt;- start to end) &#123; println(&quot;你好，世界&quot; + i) &#125; &#x2F;&#x2F;说明for 这种推导时，也可以直接对集合进行遍历 var list &#x3D; List(&quot;hello&quot;, 10, 30, &quot;tom&quot;) for (item &lt;- list) &#123; println(&quot;item&#x3D;&quot; + item) &#125; &#125;&#125; 范围数据循环方式2基本案例 1234for(i &lt;- 1 until 3) &#123; print(i + &quot; &quot;)&#125;println() 说明: 这种方式和前面的区别在于 i 是从1 到 3-1 前闭合后开的范围,和java的arr.length() 类似for (int i = 0; i &lt; arr.lenght; i++){} 循环守卫基本案例 1234for(i &lt;- 1 to 3 if i !&#x3D; 2) &#123; print(i + &quot; &quot;)&#125;println() 基本案例说明 循环守卫，即循环保护式（也称条件判断式，守卫）。保护式为true则进入循环体内部，为false则跳过，类似于continue 上面的代码等价12345for (i &lt;- 1 to 3) &#123; if (i !&#x3D; 2) &#123; println(i+&quot;&quot;)&#125;&#125; 引入变量基本案例 123for(i &lt;- 1 to 3; j &#x3D; 4 - i) &#123; print(j + &quot; &quot;)&#125; 对基本案例说明 没有关键字，所以范围后一定要加；来隔断逻辑 上面的代码等价1234for ( i &lt;- 1 to 3) &#123; val j &#x3D; 4 –i print(j+&quot;&quot;) &#125; 嵌套循环基本案例 123for(i &lt;- 1 to 3; j &lt;- 1 to 3) &#123; println(&quot; i &#x3D;&quot; + i + &quot; j &#x3D; &quot; + j)&#125; 输出结果 123456789i &#x3D;1 j &#x3D; 1i &#x3D;1 j &#x3D; 2i &#x3D;1 j &#x3D; 3i &#x3D;2 j &#x3D; 1i &#x3D;2 j &#x3D; 2i &#x3D;2 j &#x3D; 3i &#x3D;3 j &#x3D; 1i &#x3D;3 j &#x3D; 2i &#x3D;3 j &#x3D; 3 对基本案例说明没有关键字，所以范围后一定要加；来隔断逻辑上面的代码等价 12345for ( i &lt;- 1 to 3) &#123; for ( j &lt;- 1to 3)&#123; println(i + &quot; &quot; + j + &quot; &quot;) &#125;&#125; 循环返回值基本案例 12val res &#x3D; for(i &lt;- 1 to 10) yield iprintln(res) 对基本案例说明 将遍历过程中处理的结果返回到一个新Vector集合中，使用yield关键字 1234567891011121314151617181920212223object yieldFor &#123; def main(args: Array[String]): Unit &#x3D; &#123; &#x2F;&#x2F;說明 val res &#x3D; for (i &lt;-1 to 10) yield i含义 &#x2F;** * 1，对 1 to 10进行遍历， * 2, yield i： 将每次循环得到i 放入到集合Verctor中，并返回给res * 3, yield i： i可以是一个代码块，这就意味着可以对i进行处理 * 4，下面的这个方式，体现出了Scala的一个重要的语法特点， * 就是将一个集合中各个数据进行处理，并返回给新的集合 *&#x2F; val res &#x3D; for (i &lt;-1 to 10) yield i println(res) val re &#x3D; for (i &lt;-1 to 10) yield &#123; if (i%2&#x3D;&#x3D;0)&#123; i &#125;else&#123; &quot;不是偶数&quot; &#125; &#125; println(re) &#125;&#125; 使用花括号{}代替小括号()基本案例 123for(i &lt;- 1 to 3; j &#x3D; i * 2) &#123; println(&quot; i&#x3D; &quot; + i + &quot; j&#x3D; &quot; + j)&#125; 可以写成 12345for&#123; i &lt;- 1 to 3 j &#x3D; i * 2&#125; &#123; println(&quot; i&#x3D; &quot; + i + &quot; j&#x3D; &quot; + j) &#125; 对基本案例说明 {}和()对于for表达式来说都可以 for 推导式有一个不成文的约定：当for 推导式仅包含单一表达式时使用圆括号，当其包含多个表达式时使用大括号 当使用{} 来换行写表达式时，分号就不用写了 注意事项和细节说明 scala 的for循环形式和java是较大差异，这点请注意，但是基本的原理还是一样的。 scala 的for循环的步长如何控制! [for(i &lt;- Range(1,3,2)] 2.4 while循环控制基本语法 12345循环变量初始化while (循环条件) &#123; 循环体(语句) 循环变量迭代&#125; 注意事项和细节说明 循环条件是返回一个布尔值的表达式 while循环是先判断再执行语句 与If语句不同，While语句本身没有值，即整个While语句的结果是Unit类型的() 因为while中没有返回值,所以当要用该语句来计算并返回结果时,就不可避免的使用变量 ，而变量需要声明在while循环的外部，那么就等同于循环的内部对外部的变量造成了影响，所以不推荐使用，而是推荐使用for循环。 do..while循环控制基本语法 12345循环变量初始化; do&#123; 循环体(语句) 循环变量迭代 &#125; while(循环条件) 123456789object doWhile &#123; def main(args: Array[String]): Unit &#x3D; &#123; var i &#x3D; 0 do &#123; printf(i+&quot;hello\\n&quot;) i+&#x3D;1 &#125;while(i&lt;10) &#125;&#125; 运算结果 1234567891011120hello1hello2hello3hello4hello5hello6hello7hello8hello9hello进程已结束，退出代码 0 注意事项和细节说明 循环条件是返回一个布尔值的表达式 do..while循环是先执行，再判断 和while 一样，因为do…while中没有返回值,所以当要用该语句来计算并返回结果时,就不可避免的使用变量 ，而变量需要声明在do…while循环的外部，那么就等同于循环的内部对外部的变量造成了影响，所以不推荐使用，而是推荐使用for循环 多重循环控制 将一个循环放在另一个循环体内，就形成了嵌套循环。其中，for ,while ,do…while均可以作为外层循环和内层循环。【建议一般使用两层，最多不要超过3层】 实质上，嵌套循环就是把内层循环当成外层循环的循环体。当只有内层循环的循环条件为false时，才会完全跳出内层循环，才可结束外层的当次循环，开始下一次的循环。 设外层循环次数为m次，内层为n次， 则内层循环体实际上需要执行m*n=mn次。 练习12345678910111213141516171819202122232425262728import scala.io.StdIn&#x2F;** * 统计三个班成绩情况，每个班有5名同学， * 求出各个班的平均分和所有班级的平均分[学生的成绩从键盘输入] *&#x2F;object Exercise &#123;def main (args: Array[String] ): Unit &#x3D; &#123; val classNum &#x3D; 3 val stuNum &#x3D; 5 var score &#x3D; 0.0 &#x2F;&#x2F;分数 var classScore &#x3D; 0.0 &#x2F;&#x2F;班级总分 var totalScore &#x3D; 0.0 &#x2F;&#x2F;所有班级平均分 for (i&lt;- 1 to classNum)&#123; &#x2F;&#x2F;先将classNum清零 classScore &#x3D; 0.0 for (j&lt;- 1 to stuNum)&#123; printf(&quot;请输入第%d班级的第%d个学生的成绩\\n&quot;,i,j) score &#x3D; StdIn.readDouble() classScore +&#x3D; score &#125; &#x2F;&#x2F;累计totalScore totalScore +&#x3D; classScore printf(&quot;第%d班级的平均分为%.2f&quot;,i,classScore&#x2F;stuNum) &#125; printf(&quot;所有班级的平均分为%.2f&quot;,totalScore&#x2F;(stuNum*classNum)) &#125;&#125; while循环的中断基本说明 Scala内置控制结构特地去掉了break和continue，是为了更好的适应函数化编程，推荐使用函数式的风格解决break和contine的功能，而不是一个关键字。 案例演示 break的这种方式也可以应用在do..while和for循环中break的应用实例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import util.control.Breaks._object myBreak &#123; def main(args: Array[String]): Unit &#x3D; &#123; var n &#x3D; 1 &#x2F;&#x2F;breakable() &#x2F;** * 1，breakable()是一个高阶函数，(可以接受函数的函数就是高阶函数) * 2，源码 * def breakable(op: &#x3D;&gt; Unit): Unit &#x3D; &#123; * try &#123; * op * &#125; catch &#123; * case ex: BreakControl &#x3D;&gt; * if (ex ne breakException) throw ex * &#125; * &#125; * (1)op: &#x3D;&gt; Unit表示接收的参数是一个没有输入，也没有返回值的函数 * (2)即可以简单的理解为接收的是一段代码块 * 3，breakable对break()抛出的异常做了处理，代码就可以继续执行 * 4，当我们传入的是代码块时，一般会将breakable()的小括号改成大括号breakable&#123;&#125; *&#x2F; breakable( while (n &lt;&#x3D; 10)&#123; n +&#x3D; 1 println(&quot;n&#x3D;&quot;+n) if (n &#x3D;&#x3D; 8)&#123; &#x2F;&#x2F;中断while &#x2F;** * 1,在Scala中使用函数式的break来中断循环 * 2,使用break()要导包 import util.control.Breaks._ * 3, def break(): Nothing &#x3D; &#123; throw breakException &#125; * 可以看到，在源码中break是抛出了一个异常 *&#x2F; break() &#125; &#125; ) println(&quot;ok&quot;) &#x2F;&#x2F;应用在for循环中 breakable&#123; for (i&lt;-1 to 100)&#123; println(&quot;i&#x3D;&quot;+i) if (i &#x3D;&#x3D; 20)&#123; break() &#125; &#125; &#125; println(&quot;ok2&quot;)&#125;&#125; 运行结果 1234567891011121314151617181920212223242526272829n&#x3D;2n&#x3D;3n&#x3D;4n&#x3D;5n&#x3D;6n&#x3D;7n&#x3D;8oki&#x3D;1i&#x3D;2i&#x3D;3i&#x3D;4i&#x3D;5i&#x3D;6i&#x3D;7i&#x3D;8i&#x3D;9i&#x3D;10i&#x3D;11i&#x3D;12i&#x3D;13i&#x3D;14i&#x3D;15i&#x3D;16i&#x3D;17i&#x3D;18i&#x3D;19i&#x3D;20ok2 如何实现continue的效果Scala内置控制结构特地也去掉了continue，是为了更好的适应函数化编程，可以使用if – else 或是 循环守卫实现continue的效果 案例 123for (i &lt;- 1 to 10 if (i !&#x3D; 2 &amp;&amp; i !&#x3D; 3)) &#123;println(&quot;i&#x3D;&quot; + i)&#125; 12345678910111213object myContinue &#123; def main(args: Array[String]): Unit &#x3D; &#123; &#x2F;&#x2F;说明 &#x2F;** * 1, 1 to 10 * 2,循环守卫if(i !&#x3D;2 &amp;&amp; i !&#x3D;3)这个条件为true,才执行循环体 * 即当i&#x3D;&#x3D;2或者i&#x3D;&#x3D;3时。跳过 *&#x2F; for (i &lt;- 1 to 10 if(i !&#x3D;2 &amp;&amp; i !&#x3D;3))&#123; println(&quot;i&#x3D;&quot; + i) &#125; &#125;&#125; 运行结果 12345678i&#x3D;1i&#x3D;4i&#x3D;5i&#x3D;6i&#x3D;7i&#x3D;8i&#x3D;9i&#x3D;10","categories":[{"name":"Scala","slug":"Scala","permalink":"http://www.studyz.club/categories/Scala/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://www.studyz.club/tags/Scala/"},{"name":"Scala学习基础知识","slug":"Scala学习基础知识","permalink":"http://www.studyz.club/tags/Scala%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}]},{"title":"Scala学习之三变量","slug":"Scala学习-变量","date":"2020-03-21T07:32:46.257Z","updated":"2020-05-13T08:26:13.024Z","comments":true,"path":"posts/4b8b8568/","link":"","permalink":"http://www.studyz.club/posts/4b8b8568/","excerpt":"","text":"一,为什么需要变量 1.1 变量是程序的基本组成单位 1.2 变量基本使用 二，数据类型 2.1 scala数据类型介绍 2.2 scala数据类型列表 2.3 值类型转换 2.3.1 值类型隐式转换 2.3.2 强制类型转换 2.3.3 值类型和String类型的转换 2.4 标识符的命名规范 一,为什么需要变量 一个程序就是一个世界，在scala中一切都是对象 变量相当于内存中一个数据存储空间的表示，你可以把变量看做是一个房间的门牌号，通过门牌号我们可以找到房间，而通过变量名可以访问到变量(值)。 变量使用的基本步骤 声明/定义变量 (scala要求变量声明时初始化) 使用 1.1 变量是程序的基本组成单位不论是使用哪种高级程序语言编写程序,变量都是其程序的基本组成单位，比如: 12345678910111213ackage com.chapter02object ScalaFunDemo01 &#123; def main(args: Array[String]): Unit &#x3D; &#123; var a : Int &#x3D; 1 &#x2F;&#x2F;定义一个整型变量,取名a,并赋初值1 var b : Int &#x3D; 3 &#x2F;&#x2F;定义一个整型变量,取名b,并赋初值3 b &#x3D; 89 &#x2F;&#x2F;给变量b 赋 89 println(&quot;a&#x3D;&quot; + a) &#x2F;&#x2F;输出语句,把变量a的值输出 println(&quot;b&#x3D;&quot; + b) &#x2F;&#x2F;把变量b的值输出 &#125;&#125; 1.2 变量基本使用Scala变量使用案例入门 Scala变量使用说明 变量声明基本语法var | val 变量名 [: 变量类型] = 变量值 注意事项123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.chapter01.varsobject VarDemo02 &#123; def main(args: Array[String]): Unit &#x3D; &#123; &#x2F;&#x2F;类型推导 var num &#x3D; 10 &#x2F;&#x2F; 这时num就是Int &#x2F;&#x2F;方式1 ， 可以利用idea的提示来证明,给出提示 &#x2F;&#x2F;方式2， 使用isInstanceOf[Int]判断 println(num.isInstanceOf[Int]) &#x2F;&#x2F;类型确定后，就不能修改，说明Scala 是强数据类型语言() &#x2F;&#x2F; num &#x3D; 2.3, 错误 &#x2F;&#x2F;3.在声明&#x2F;定义一个变量时，可以使用var 或者 val 来修饰， var 修饰的变量可改变，val 修饰的变量不可改 var age &#x3D; 10 &#x2F;&#x2F;即age 是可以改变的. age &#x3D; 30 &#x2F;&#x2F; ok val num2 &#x3D; 30 &#x2F;&#x2F;num2 &#x3D; 40 &#x2F;&#x2F; val 修饰的变量是不可以改变. &#x2F;&#x2F;scala设计者为什么设计 var 和 val &#x2F;&#x2F;(1) 在实际编程，我们更多的需求是获取&#x2F;创建一个对象后，读取该对象的属性， &#x2F;&#x2F; 或者是修改对象的属性值, 但是我们很少去改变这个对象本身 &#x2F;&#x2F; dog &#x3D; new Dog() dog.age &#x3D; 10 dog &#x3D; new Dog() &#x2F;&#x2F; 这时，我们就可以使用val &#x2F;&#x2F;(2) 因为val 没有线程安全问题，因此效率高，scala的设计者推荐我们val &#x2F;&#x2F;(3) 如果对象需要改变，则使用var val dog &#x3D; new Dog &#x2F;&#x2F;dog &#x3D; new Dog &#x2F;&#x2F;Reassignment to val dog.age &#x3D; 90 &#x2F;&#x2F;ok dog.name &#x3D; &quot;小花&quot; &#x2F;&#x2F;ok &#125;&#125;class Dog &#123; &#x2F;&#x2F;声明一个age属性，给了一个默认值 _ var age :Int &#x3D; 0 &#x2F;&#x2F; &#x2F;&#x2F;声明名字 var name:String &#x3D; &quot;&quot; &#x2F;&#x2F;&#125; 声明变量时，类型可以省略（编译器自动推导,即类型推导） 类型确定后，就不能修改，说明Scala 是强数据类型语言. 在声明/定义一个变量时，可以使用var 或者 val 来修饰， var 修饰的变量可改变，val 修饰的变量不可改 [案例]. val修饰的变量在编译后，等同于加上final， 通过反编译看下底层代码。 var 修饰的对象引用可以改变，val 修饰的则不可改变，但对象的状态(值)却是可以改变的。(比如: 自定义对象、数组、集合等等) [分析val好处] 1234 class Dog &#123;var age &#x3D; 100 &#125; 变量声明时，需要初始值。 二，数据类型2.1 scala数据类型介绍 Scala 与 Java有着相同的数据类型，在Scala中数据类型都是对象，也就是说scala没有java中的原生类型 12345678910package com.lizhi.scala2object dataType &#123; def main(args: Array[String]): Unit &#x3D; &#123; var num1: Int &#x3D; 10 &#x2F;&#x2F;因为Int是一个类，因此他的一个实例，就是可以使用多种方法 &#x2F;&#x2F;在Scala中，如果一个方法，没有形参，则可以省略()即，toDouble后面没有() println(num1.toDouble + &quot;\\t&quot; + num1.toString()) &#125;&#125; Scala数据类型分为两大类 AnyVal(值类型) 和 AnyRef(引用类型)， 注意：不管是AnyVal还是AnyRef 都是对象。 相对于java的类型系统，scala要复杂些！也正是这复杂多变的类型系统才让面向对象编程和函数式编程完美的融合在了一起 12345678910111213141516object TypeDemo02 &#123; def main(args: Array[String]): Unit &#x3D; &#123; println(sayHello) var num &#x3D; 1.2 &#x2F;&#x2F;默认为double var num2 &#x3D; 1.7f &#x2F;&#x2F;这是float &#x2F;&#x2F;num2 &#x3D; num ,error ,修改num2 &#x3D; num.toFloat &#125; &#x2F;&#x2F;比如开发中，我们有一个方法，就会异常中断，这时就可以返回Nothing &#x2F;&#x2F;即当我们Nothing做返回值，就是明确说明该方法没有没有正常返回值 def sayHello: Nothing &#x3D; &#123; throw new Exception(&quot;抛出异常&quot;) &#125;&#125; 2.2 scala数据类型列表 数据类型 描述 Byte 8位有符号补码整数。数值区间为 -128 到 127 Short 16位有符号补码整数。数值区间为 -32768 到 32767 Int 32位有符号补码整数。数值区间为 -2147483648 到 2147483647 Long 64位有符号补码整数。数值区间为 -9223372036854775808 到 9223372036854775807 Float 32 位, IEEE 754标准的单精度浮点数 Double 64 位 IEEE 754标准的双精度浮点数 Char 16位无符号Unicode字符, 区间值为 U+0000 到 U+FFFF String 字符序列 Boolean true或false Unit 表示无值，和其他语言中void等同。用作不返回任何结果的方法的结果类型。Unit只有一个实例值，写成()。 Null null Nothing Nothing类型在Scala的类层级的最低端；它是任何其他类型的子类型。 Any Any是所有其他类的超类 AnyRef AnyRef类是Scala里所有引用类(reference class)的基类 2.2.1 整数类型 Scala的整数类型就是用于存放整数值的，比如 12 , 30, 3456等等 整型的类型 数据类型 描述 Byte [1] 8位有符号补码整数。数值区间为 -128 到 127 Short [2] 16位有符号补码整数。数值区间为 -32768 到 32767 Int [4] 32位有符号补码整数。数值区间为 -2147483648 到 2147483647 Long [8] 64位有符号补码整数。数值区间为 -9223372036854775808 到 9223372036854775807 = 2的(64-1)次方-1 求最大值最小值12345678910111213object TyepDemo03 &#123; def main(args: Array[String]): Unit &#x3D; &#123; println(&quot;long的最大值&quot; + Long.MaxValue + &quot;~&quot; + Long.MinValue) &#x2F;&#x2F;2.2345678912f , 2.2345678912 var num1:Float &#x3D; 2.2345678912f var num2:Double &#x3D; 2.2345678912 println(&quot;num1&#x3D;&quot; + num1 + &quot;num2&#x3D;&quot; + num2) &#125;&#125; 整型的使用细节 Scala各整数类型有固定的表数范围和字段长度，不受具体OS的影响，以保证Scala程序的可移植性。 Scala的整型 常量/字面量 默认为 Int 型，声明Long型 常量/字面量 须后加‘l’’或‘L’ Scala程序中变量常声明为Int型，除非不足以表示大数，才使用Long 1234567var c &#x3D; 11 &#x2F;&#x2F; c 就是Int类型println(&quot;c&#x3D;&quot; + c)var d &#x3D; 12l &#x2F;&#x2F; d 就是 Long 类型 或者 var d &#x3D; 12Lprintln(&quot;d&#x3D;&quot; + d) var e &#x3D; 9223372036854775807 &#x2F;&#x2F; 正确吗? 如何解决 解决 123var i &#x3D; 10 &#x2F;&#x2F;i Int var j &#x3D; 10l &#x2F;&#x2F;j Long var e &#x3D; 9223372036854775807l &#x2F;&#x2F;说 9223372036854775807 超过int 2.2.2 浮点类型 Scala的浮点类型可以表示一个小数，比如 123.4f，7.8 ，0.12等等 Float [4] 32 位, IEEE 754标准的单精度浮点数 Double [8] 64 位 IEEE 754标准的双精度浮点数 浮点型使用细节 与整数类型类似，Scala 浮点类型也有固定的表数范围和字段长度，不受具体OS的影响。 Scala的浮点型常量默认为Double型，声明Float型常量，须后加‘f’或‘F’。12345var f1 : Float &#x3D; 1.1 &#x2F;&#x2F; double-&gt;float 错误var f2 &#x3D; 1.2 &#x2F;&#x2F; ok 类型推断var f3 : Double &#x3D; 1.3 &#x2F;&#x2F; okvar f4 : Float &#x3D; 1.4f &#x2F;&#x2F; okvar f5 : Double &#x3D; 1.5f &#x2F;&#x2F; float-&gt;double , ok 浮点型常量有两种表示形式 十进制数形式：如：5.12 512.0f .512 (必须有小数点） 科学计数法形式:如：5.12e2 = 5.12乘以10的2次方 5.12E-2 = 5.12除以10的2次方 通常情况下，应该使用Double型，因为它比Float型更精确(小数点后大致7位) //测试数据 ：2.2345678912f , 2.2345678912 2.2.3 字符类型(Char) 字符类型可以表示单个字符,字符类型是Char， 16位无符号Unicode字符(2个字节), 区间值为 U+0000 到 U+FFFF 字符类型使用细节 1234问题：var c2 : Char &#x3D; ‘a’ + 1 正确吗?修改: var c2 : Int &#x3D; &#39;a&#39; + 1 [ok] 1234567891011121314151617181920212223package com.lizhi.scala2object CharDemo &#123; def main(args: Array[String]): Unit &#x3D; &#123; var char1: Char &#x3D; 97 &#x2F;&#x2F;当我们输出一个char类型是，他会输出该数字对应的字符(码值表 unicode)&#x2F;&#x2F;unicode 码值表包括ascii println(&quot;char1&#x3D;&quot; + char1) &#x2F;&#x2F; a &#x2F;&#x2F;char 可以当做数字进行运行 var char2: Char &#x3D; &#39;a&#39; var num &#x3D; 10 + char2 println(&quot;num&#x3D;&quot; + num) &#x2F;&#x2F; 107 &#x2F;&#x2F;原因和分析 &#x2F;&#x2F;1. 当把一个计算的结果赋值一个变量，则编译器会进行类型转换及判断（即会看范围+类型） &#x2F;&#x2F;2. 当把一个字面量赋值一个变量，则编译器会进行范围的判定&#x2F;&#x2F; var c2: Char &#x3D; &#39;a&#39; + 1&#x2F;&#x2F; var c3: Char &#x3D; 97 + 1&#x2F;&#x2F; var c4: Char &#x3D; 98 &#125;&#125; 运行结果 123&quot;D:\\Program Files\\java\\bin\\java.exe&quot; char1&#x3D;anum&#x3D;107 字符类型本质探讨 2.2.4 布尔类型：Boolean 布尔类型也叫Boolean类型，Booolean类型数据只允许取值true和false boolean类型占1个字节。 boolean 类型适于逻辑运算，一般用于程序流程控制[后面详解]：if条件控制语句；while循环控制语句；do-while循环控制语句；for循环控制语句 2.2.5 Unit类型、Null类型和Nothing类型 Unit 表示无值，和其他语言中void等同。用作不返回任何结果的方法的结果类型。Unit只有一个实例值，写成()。 Null null , Null 类型只有一个实例值 null Nothing Nothing类型在Scala的类层级的最低端；它是任何其他类型的子类型。 当一个函数，我们确定没有正常的返回值，可以用Nothing 来指定返回类型，这样有一个好处，就是我们可以把返回的值（异常）赋给其它的函数或者变量（兼容性） 使用细节和注意事项 123456789101112131415161718192021222324252627package com.lizhi.scala2object UnitNullNothingDemo &#123; def main(args: Array[String]): Unit &#x3D; &#123; val res &#x3D; sayHello() println(&quot;res&#x3D;&quot; + res) &#x2F;&#x2F;Null类只有一个实例对象，null，类似于Java中的null引用。null可以赋值给任意引用类型(AnyRef)，但是不能赋值给值类型(AnyVal: 比如 Int, Float, Char, Boolean, Long, Double, Byte, Short) val dog: Dog &#x3D; null &#x2F;&#x2F;错误&#x2F;&#x2F; val char1: Char &#x3D; null&#x2F;&#x2F; println(&quot;ok~~~&quot;) &#125; &#x2F;&#x2F;Unit等价于java的void,只有一个实例值() def sayHello(): Unit &#x3D; &#123; &#125;&#125;class Dog &#123;&#125; 2.3 值类型转换2.3.1 值类型隐式转换 当Scala程序在进行赋值或者运算时，精度小的类型自动转换为精度大的数据类型，这个就是自动类型转换(隐式转换)。 数据类型按精度(容量)大小排序为 自动类型转换细节说明示例： 有多种类型的数据混合运算时，系统首先自动将所有数据转换成容量最大的那种数据类型，然后再进行计算。 5.6 + 10 = 》double123var n1 &#x3D; 10var n2 &#x3D; 5.6var n3 &#x3D; n1 + n2 当我们把精度(容量)大 的数据类型赋值给精度(容量)小 的数据类型时，就会报错，反之就会进行自动类型转换。 (byte, short) 和 char之间不会相互自动转换。 12var n4: Byte &#x3D; 10 &#x2F;&#x2F;var char1 : Char &#x3D; n4 &#x2F;&#x2F; 错误，因为byte 不能自动转换char byte，short，char 他们三者可以计算，在计算时首先转换为int类型 自动提升原则： 表达式结果的类型自动提升为 操作数中最大的类型 2.3.2 强制类型转换 自动类型转换的逆过程，将容量大的数据类型转换为容量小的数据类型。使用时要加上强制转函数，但可能造成精度降低或溢出,格外要注意。 强制类型转换细节说明 当进行数据的 从 大——&gt;小，就需要使用到强制转换 强转符号只针对于最近的操作数有效，往往会使用小括号提升优先级 1234val num1: Int &#x3D; 10 * 3.5.toInt + 6 * 1.5.toInt &#x2F;&#x2F; 36(toInt去留小数部分，不四舍五入)val num2: Int &#x3D; (10 * 3.5 + 6 * 1.5).toInt &#x2F;&#x2F; 44println(num1 + &quot; &quot; + num2) Char类型可以保存 Int的常量值，但不能保存Int的变量值，需要强转 Byte和Short类型在进行运算时，当做Int类型处理。 判断是否能够通过编译,并说明原因123456789101112131）var s : Short &#x3D; 5 &#x2F;&#x2F; ok s &#x3D; s-2 &#x2F;&#x2F; error Int -&gt; Short 2） var b : Byte &#x3D; 3 &#x2F;&#x2F; ok b &#x3D; b + 4 &#x2F;&#x2F; error Int -&gt;Byte b &#x3D; (b+4).toByte &#x2F;&#x2F; ok ，使用强制转换3）var c : Char &#x3D; &#39;a&#39; &#x2F;&#x2F;ok var i : Int &#x3D; 5 &#x2F;&#x2F;ok var d : Float &#x3D; .314F &#x2F;&#x2F;ok var result : Double &#x3D; c+i+d &#x2F;&#x2F;ok Float-&gt;Double4） var b : Byte &#x3D; 5 &#x2F;&#x2F; ok var s : Short &#x3D; 3 &#x2F;&#x2F;ok var t : Short &#x3D; s + b &#x2F;&#x2F; error Int-&gt;Short var t2 &#x3D; s + b &#x2F;&#x2F; ok, 使用类型推导 2.3.3 值类型和String类型的转换 在程序开发中，我们经常需要将基本数据类型转成String 类型。或者将String类型转成基本数据类型。 基本类型转String类型 语法： 将基本类型的值+”” 即可 案例演示： 123val d1 &#x3D; 1.2&#x2F;&#x2F;基本数据类型转 stringval s1 &#x3D; d1 + &quot;&quot; &#x2F;&#x2F;以后看到有下划线，就表示编译器做了转换 String类型转基本数据类型 语法：通过基本类型的String的 toXxx方法即可 案例演示： 12345val s2 &#x3D; &quot;12&quot;val num1 &#x3D; s2.toIntval num2 &#x3D; s2.toByteval num3 &#x3D; s2.toDoubleval num4 &#x3D; s2.toLong 1234567&quot;12&quot;s1.toInts1.toFloats1.toDoubles1.toBytes1.toLongs1.toShort 注意事项 在将String 类型转成 基本数据类型时，要确保String类型能够转成有效的数据，比如 我们可以把 “123” , 转成一个整数，但是不能把 “hello” 转成一个整数 思考就是要把 “12.5” 转成 Int //? 1234&#x2F;&#x2F;在scala中，不是讲小数点后的数据进行截取，而是会抛出异常 val s4 &#x3D; &quot;12.5&quot; &#x2F;&#x2F;println(s4.toInt) &#x2F;&#x2F; error println(s4.toDouble) &#x2F;&#x2F; ok 2.4 标识符的命名规范标识符概念 1,Scala 对各种变量、方法、函数等命名时使用的字符序列称为标识符2.凡是自己可以起名字的地方都叫标识符 标识符的命名规则(记住) Scala中的标识符声明，基本和Java是一致的，但是细节上会有所变化。 首字符为字母，后续字符任意字母和数字，美元符号，可后接下划线_ 数字不可以开头。 首字符为操作符(比如+ - * / )，后续字符也需跟操作符 ,至少一个(反编译) 操作符(比如+-*/)不能在标识符中间和最后. 用反引号(``),包括的任意字符串，即使是关键字(39个)也可以 [true] 标识符举例说明 1234567891011121314hello &#x2F;&#x2F; okhello12 &#x2F;&#x2F; ok1hello &#x2F;&#x2F; errorh-b &#x2F;&#x2F; errorx h &#x2F;&#x2F; errorh_4 &#x2F;&#x2F; ok_ab &#x2F;&#x2F; okInt &#x2F;&#x2F; ok, 在scala中，Int 不是关键字，而是预定义标识符,&#96;可以用，但是不推荐&#96;Float &#x2F;&#x2F; ok_ &#x2F;&#x2F; 不可以，因为在scala中，_ 有很多其他的作用，因此不能使用Abc &#x2F;&#x2F; ok+*- &#x2F;&#x2F; ok+a &#x2F;&#x2F; error 标识符命名注意事项 1,包名：尽量采取有意义的包名，简短，有意义 2,变量名、函数名 、方法名 采用驼峰法。 关键字介绍 Scala有39个关键字： 12345678* package, import, class, object, trait, extends, with, type, forSome* private, protected, abstract, sealed, final, implicit, lazy, override* try, catch, finally, throw * if, else, match, case, do, while, for, return, yield* def, val, var * this, super* new* true, false, null","categories":[{"name":"Scala","slug":"Scala","permalink":"http://www.studyz.club/categories/Scala/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://www.studyz.club/tags/Scala/"},{"name":"Scala学习基础知识","slug":"Scala学习基础知识","permalink":"http://www.studyz.club/tags/Scala%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}]},{"title":"IDEA常用快捷键","slug":"IDEA常用快捷键","date":"2020-03-19T03:39:25.283Z","updated":"2020-05-13T08:26:13.015Z","comments":true,"path":"posts/566321e7/","link":"","permalink":"http://www.studyz.club/posts/566321e7/","excerpt":"","text":"IntelliJ Idea 常用快捷键列表 Ctrl+Shift + Enter 语句完成“！” ， 否定完成 ， 输入表达式时按 “！”键 Ctrl+E 最近的文件 Ctrl+Shift+E 最近更改的文件 Shift+Click 可以关闭文件 Ctrl+[ OR ] 可以跑到大括号的开头与结尾 Ctrl+F12 可以显示当前文件的结构 Ctrl+F7 可以查询当前元素在当前文件中的引用 ， 然后按 F3 可以选择 Ctrl+N 可以快速打开类 Ctrl+Shift+N 可以快速打开文件 Alt+Q 可以看到当前方法的声明 Ctrl+P 可以显示参数信息 Ctrl+Shift+Insert 可以选择剪贴板内容并插入 Alt+Insert 可以生成构造器/Getter/Setter等 Ctrl+Alt+V 可以引入变量。例如：new String(); 自动导入变量定义 Ctrl+Alt+T 可以把代码包在一个块内 ，例如：try/catch Ctrl+Enter 导入包 ，自动修正 Ctrl+Alt+L 格式化代码 Ctrl+Alt+I 将选中的代码进行自动缩进编排 ， 这个功能在编辑 JSP 文件时也可以工作 Ctrl+Alt+O 优化导入的类和包 Ctrl+R 替换文本 Ctrl+F 查找文本 Ctrl+Shift+Space 自动补全代码 Ctrl+空格 代码提示（与系统输入法快捷键冲突） Ctrl+Shift+Alt+N 查找类中的方法或变量 Alt+Shift+C 最近的更改 Alt+Shift+Up/Down 上/下移一行 Shift+F6 重构 – 重命名 Ctrl+X 删除行 Ctrl+D 复制行 Ctrl+/或Ctrl+Shift+/ 注释（//或者/**/） Ctrl+J 自动代码（例如：serr） Ctrl+Alt+J 用动态模板环绕 Ctrl+H 显示类结构图（类的继承层次） Ctrl+Q 显示注释文档 Alt+F1 查找代码所在位置 Alt+1 快速打开或隐藏工程面板 Ctrl+Alt+left/right 返回至上次浏览的位置 Alt+left/right 切换代码视图 Alt+Up/Down 在方法间快速移动定位 Ctrl+Shift+Up/Down 向上/下移动语句 F2 或 Shift+F2 高亮错误或警告快速定位 Tab 代码标签输入完成后 ， 按 Tab ， 生成代码 Ctrl+Shift+F7 高亮显示所有该文本 ， 按 Esc 高亮消失 Alt+F3 逐个往下查找相同文本 ， 并高亮显示 Ctrl+Up/Down 光标中转到第一行或最后一行下 Ctrl+B/Ctrl+Click 快速打开光标处的类或方法（跳转到定义处） Ctrl+Alt+B 跳转到方法实现处 Ctrl+Shift+Backspace 跳转到上次编辑的地方 Ctrl+O 重写方法 Ctrl+Alt+Space 类名自动完成 Ctrl+Alt+Up/Down 快速跳转搜索结果 Ctrl+Shift+J 整合两行 Alt+F8 计算变量值 Ctrl+Shift+V 可以将最近使用的剪贴板内容选择插入到文本 Ctrl+Alt+Shift+V 简单粘贴 Shift+Esc 不仅可以把焦点移到编辑器上 ，而且还可以隐藏当前（或最后活动的）工具窗口 F12 把焦点从编辑器移到最近使用的工具窗口 Shift+F1 要打开编辑器光标字符处使用的类或者方法 Java 文档的浏览器 Ctrl+W 可以选择单词继而语句继而行继而函数 Ctrl+Shift+W 取消选择光标所在词 Alt+F7 查找整个工程中使用地某一个类、方法或者变量的位置 Ctrl+I 实现方法 Ctrl+Shift+U 大小写转化 Ctrl+Y 删除当前行 Shift+Enter 向下插入新行 psvm/sout main/System.out.println(); Ctrl+J ， 查看更多 Ctrl+Shift+F 全局查找 Ctrl+F 查找/Shift+F3 ， 向上查找/F3 ， 向下查找 Ctrl+Shift+S 高级搜索 Ctrl+U 转到父类 Ctrl+Alt+S 打开设置对话框 Alt+Shift+Inert 开启/关闭列选择模式 Ctrl+Alt+Shift+S 打开当前项目/模块属性 Ctrl+G 定位行 Alt+Home 跳转到导航栏 Ctrl+Enter 上插一行 Ctrl+Backspace 按单词删除 Ctrl+”+/-” 当前方法展开、折叠 Ctrl+Shift+”+/-” 全部展开、折叠 【调试部分、编译】 Ctrl+F2 停止 Alt+Shift+F9 选择 Debug Alt+Shift+F10 选择 Run Ctrl+Shift+F9 编译 Ctrl+Shift+F10 运行 Ctrl+Shift+F8 查看断点 F8 步过 F7 步入 Shift+F7 智能步入 Shift+F8 步出 Alt+Shift+F8 强制步过 Alt+Shift+F7 强制步入 Alt+F9 运行至光标处 Ctrl+Alt+F9 强制运行至光标处 F9 恢复程序 Alt+F10 定位到断点 Ctrl+F8 切换行断点 Ctrl+F9 生成项目 Alt+1 项目 Alt+2 收藏 Alt+6 TODO Alt+7 结构 Ctrl+Shift+C 复制路径 Ctrl+Alt+Shift+C 复制引用 ， 必须选择类名 Ctrl+Alt+Y 同步 Ctrl+~ 快速切换方案（界面外观、代码风格、快捷键映射等菜单） Shift+F12 还原默认布局 Ctrl+Shift+F12 隐藏/恢复所有窗口 Ctrl+F4 关闭 Ctrl+Shift+F4 关闭活动选项卡 Ctrl+Tab 转到下一个拆分器 Ctrl+Shift+Tab 转到上一个拆分器 【重构】 Ctrl+Alt+Shift+T 弹出重构菜单 Shift+F6 重命名 F6 移动 F5 复制 Alt+Delete 安全删除 Ctrl+Alt+N 内联 【查找】 Ctrl+F 查找 Ctrl+R 替换 F3 查找下一个 Shift+F3 查找上一个 Ctrl+Shift+F 在路径中查找 Ctrl+Shift+R 在路径中替换 Ctrl+Shift+S 搜索结构 Ctrl+Shift+M 替换结构 Alt+F7 查找用法 Ctrl+Alt+F7 显示用法 Ctrl+F7 在文件中查找用法 Ctrl+Shift+F7 在文件中高亮显示用法","categories":[{"name":"IDEA","slug":"IDEA","permalink":"http://www.studyz.club/categories/IDEA/"}],"tags":[{"name":"IDEA常用快捷键","slug":"IDEA常用快捷键","permalink":"http://www.studyz.club/tags/IDEA%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/"}]},{"title":"Scala学习之二基础知识","slug":"Scala学习-基础知识","date":"2020-03-02T03:41:37.833Z","updated":"2020-05-13T08:26:13.020Z","comments":true,"path":"posts/da6f95b8/","link":"","permalink":"http://www.studyz.club/posts/da6f95b8/","excerpt":"","text":"一，什么是Scala 1.2 Scala和Java以及jvm的关系分析图 1.3 Scala语言的特点 1.4，Scala执行流程分析 1.5，Scala程序开发注意事项 转义字符，输出，注释，缩进 1.6，绑定源码包 一，什么是Scala Spark—新一代内存级大数据计算框架，是大数据的重要内容。 Spark就是使用Scala编写的。因此为了更好的学习Spark, 需要掌握Scala这门语言。 Scala 是 Scalable Language 的简写，是一门多范式(范式/编程方式[面向对象/函数式编程])的编程语言 联邦理工学院洛桑（EPFL）的Martin Odersky于2001年开始设计Scala Spark的兴起，带动Scala语言的发展！ 1.2 Scala和Java以及jvm的关系分析图 一般来说，学Scala的人，都会Java，而Scala是基于Java的，因此我们需要将Scala和Java以及JVM 之间的关系搞清楚，否则学习Scala你会蒙圈。 1.3 Scala语言的特点 静态语言在运行前会生成。class静态文件，Python是动态语言，一句一句执行不会生产静态文件 Scala是一门以java虚拟机（JVM）为运行环境并将面向对象和函数式编程的最佳特性结合在一起的静态类型编程语言。 (1),Scala 是一门多范式 (multi-paradigm) 的编程语言，Scala支持面向对象和函数式编程 (2)，Scala源代码(.scala)会被编译成Java字节码(.class)，然后运行于JVM之上，并可以调用现有的Java类库，实现两种语言的无缝对接。 (3),scala 单作为一门语言来看， 非常的简洁高效 （三元运算， ++ ， --） (4), Scala 在设计时，马丁·奥德斯基 是参考了Java的设计思想，可以说Scala是源于java，同时马丁·奥德斯基 也加入了自己的思想，将函数式编程语言的特点融合到JAVA中, 因此，对于学习过Java的同学，只要在学习Scala的过程中，搞清楚Scala 和 java相同点和不同点，就可以快速的掌握Scala这门语言 (5),快速有效掌握Scala的建议 [1. 学习scala 特有的语法 2. 搞清楚 scala 和java 区别 3. 如何规范的使用scala] 1.4，Scala执行流程分析 1.5，Scala程序开发注意事项Scala程序开发注意事项(重点) Scala源文件以 “.scala” 为扩展名。 Scala程序的执行入口是main()函数。 Scala语言严格区分大小写。 Scala方法由一条条语句构成，每个语句后不需要分号(Scala语言会在每行后自动加分号)，这也体现出Scala的简洁性。 如果在同一行有多条语句，除了最后一条语句不需要分号，其它语句需要分号。 Scala常用的转义字符(escape char) \\t 一个制表位，实现对齐的功能 \\n 换行符 \\ 一个\\ &quot; 一个” \\r 一个回车 println(“hello\\rk”); Scala语言输出的三种方式 字符串通过+号连接（类似java）。 printf用法 （类似C语言）字符串通过 % 传值。 字符串通过$引用(类似PHP）。1234567891011121314151617181920212223package com.lizhi.scala1object printDemo &#123; def main(args: Array[String]): Unit &#x3D; &#123; var str1:String &#x3D; &quot;Hello&quot; var str2:String &#x3D; &quot; World&quot; println(str1 + str2) var name:String &#x3D; &quot;tom&quot; var age:Int &#x3D; 18 var sal:Float &#x3D; 18.64f var hight:Double &#x3D; 190.05 printf(&quot;名字&#x3D;%s 年龄是%d 薪水%.2f 高%.3f&quot;,name,age,sal,hight) println() println(&quot;-------------&quot;) &#x2F;&#x2F;Scala支持使用$输出内容,编译器会去解析$对应变量 println(s&quot;个人信息如下: \\n 名字$name \\n年龄$age \\n薪水$sal&quot;) &#x2F;&#x2F;如果在字符串中出现这种写法，则大括号中为表达式 println(s&quot;个人信息如下: \\n 名字$&#123;name&#125; \\n年龄$&#123;age + 10&#125; \\n薪水$&#123;sal * 5&#125;&quot;) &#125;&#125; 运行结果1234567891011121314&quot;D:\\Program Files\\java\\bin\\java.exe&quot; Hello World名字&#x3D;tom 年龄是18 薪水18.64 高190.050-------------个人信息如下: 名字tom 年龄18 薪水18.64个人信息如下: 名字tom 年龄28 薪水93.2进程已结束，退出代码 0 Scala注释(comment) 用于注解说明解释程序的文字就是注释，注释提高了代码的阅读性；注释是一个程序员必须要具有的良好编程习惯。将自己的思想通过注释先整理出来，再用代码去体现。 单行注释12基本格式格式： &#x2F;&#x2F;注释文字 多行注释12基本格式格式： &#x2F;* 注释文字 *&#x2F; 文档注释 注释内容可以被工具 scaladoc 所解析，生成一套以网页文件形式体现的该程序的说明文档 1234567891011121314package com.lizhi.chapter01.Demo01object Hello &#123; &#x2F;** * @deprecated xxx * @example testing coding * @param args *&#x2F; def main(args: Array[String]): Unit &#x3D; &#123; println(&quot;helllo&quot;) &#125;&#125;scaladoc -d d:&#x2F; Hello.scala 正确的缩进和空白 使用一次tab操作(选中代码)，实现缩进,默认整体向右边移动，时候用shift+tab整体向左移 或者使用 ctrl + alt + L 来进行格式化 [演示] 运算符两边习惯性各加一个空格。比如：2 + 4 * 5。 一行最长不超过80个字符，超过的请使用换行展示，尽量保持格式优雅 1.6，绑定源码包 在选定要看的函数名Ctrl+b选择绑定的源码包 将下载好的scala-2.13.1.tar.gz源码包放在E:\\Program\\Scala\\lib\\目录下(源码包要解压) 再按Ctrl+b就可以查看源码了","categories":[{"name":"Scala","slug":"Scala","permalink":"http://www.studyz.club/categories/Scala/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://www.studyz.club/tags/Scala/"},{"name":"Scala学习基础知识","slug":"Scala学习基础知识","permalink":"http://www.studyz.club/tags/Scala%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}]},{"title":"Scala学习之一环境的搭建和入门","slug":"Scala学习-环境的搭建和入门","date":"2020-03-01T08:23:50.994Z","updated":"2020-05-13T08:26:13.016Z","comments":true,"path":"posts/e0b09ebe/","link":"","permalink":"http://www.studyz.club/posts/e0b09ebe/","excerpt":"","text":"一，Scala开发环境的搭建 首先到Scala官网下载Scala网址为 https://www.scala-lang.org/download/ 找到下图所示位置：选择相对应的版本的Scala进行下载，这里以window为例： 下载后，双击 msi 文件，一步步安装即可，安装过程你可以使用默认的安装目录。 安装好scala后，系统会自动提示，单击 finish，完成安装。 二，配置环境变量 右击我的电脑，单击”属性”，进入如图所示页面。下面开始配置环境变量，右击【我的电脑】–【属性】–【高级系统设置】–【环境变量】，如图： 设置 SCALA_HOME 变量：单击新建，在变量名栏输入：SCALA_HOME: 变量值一栏输入：E:\\Program\\Scala 也就是scala的安装目录， 设置 Path 变量：找到系统变量下的”Path”如图，单击编辑。在”变量值”一栏的最前面添加如下的路径： E:\\Program\\Scala\\bin 在这里需要注意win10不能识别相对路径，如果Path = %SCALA_HOME%\\bin，则会报错scala不是内部或外部的命令。我们只需把路径改为绝对路径即可。 设置 Classpath 变量：找到找到系统变量下的”Classpath”如图，单击编辑，如没有，则单击”新建”:”变量名”：1ClassPath &#x3D; .;%SCALA_HOME%\\bin;%SCALA_HOME%\\lib\\dt.jar;%SCALA_HOME%\\lib\\tools.jar.; 三，检验 四，整合idea 首先使用idea的plugins下载Scala，若没有Scala插件这里输入进行Scala下载然后重启Idea即可 五，Scala的REPL 介绍上面打开的scala命令行窗口，我们称之为REPL，是指：Read-&gt;Evaluation-&gt;Print-&gt;Loop，也称之为交互式解释器。 说明在命令行窗口中输入scala指令代码时，解释器会读取指令代码(R)并计算对应的值(E)，然后将结果打印出来(P)，接着循环等待用户输入指令(L)。 从技术上讲，这里其实并不是一个解释器，而是指令代码被快速的编译成Java字节码并被JVM加载执行。最终将执行结果输出到命令行中 六，Scala快速开发入门需求说明 要求开发一个Hello.scala 程序，可以输出 “hello,世界!” [对scala程序基本结构说明] windows下开发步骤[先使用ed] 可以直接使用文本开发工具[editplus] 将 Scala 代码编写到扩展名为 Hello.scala 的文件中。[ 说明: 比如将源码在目录 C:\\Users\\Administrator&gt;下 ] 12345object Hello&#123; def main(arg:Array[String]):Unit &#x3D; &#123;println(&quot;hello,scala~~&quot;) &#125;&#125; 通过 scalac 命令对该 scala 文件进行编译，生成 .class 文件。[和javac类似] 命令行下 执行 scala Hello 就可以看到运行效果。 注意：scala Hello.scala 命令可以直接运行 Hello.scala 程序 [内部也会有编译和运行过程] 七，Scala程序反编译 看反编译代码(工具jd-gui.exe) 模拟代码 八，IDEA运行","categories":[{"name":"Scala","slug":"Scala","permalink":"http://www.studyz.club/categories/Scala/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"http://www.studyz.club/tags/Scala/"},{"name":"Scala学习-环境的搭建和入门","slug":"Scala学习-环境的搭建和入门","permalink":"http://www.studyz.club/tags/Scala%E5%AD%A6%E4%B9%A0-%E7%8E%AF%E5%A2%83%E7%9A%84%E6%90%AD%E5%BB%BA%E5%92%8C%E5%85%A5%E9%97%A8/"}]},{"title":"win10+php环境搭建","slug":"win10+Apache安装","date":"2020-01-01T05:18:16.998Z","updated":"2020-05-13T08:26:13.021Z","comments":true,"path":"posts/8ea26689/","link":"","permalink":"http://www.studyz.club/posts/8ea26689/","excerpt":"","text":"一，Apache安装① 下载网址https://www.apachelounge.com/download/ ② 解压后到conf文件夹，打开httpd.conf安装目录 1Define SRVROOT &quot;E:\\Program\\wamp\\Apache24&quot; 由于我端口占用，将端口改为81 1Listen 81 因为是本地运行所以添加如下内容 1ServerName localhost:81 ③ 以管理员身份运行cmd进到E:\\Program\\wamp\\Apache24\\bin目录执行命令启动服务-n apache是命名服务名为apache，后面启动服务就用这个名字(默认的是apache2.4) 1httpd.exe -k start -n apache 1httpd.exe -k restart -n apache 然后win+r输入services.msc可以看到在浏览器输入http://localhost:81/查看也可以在bin目录打开ApacheMonitor.exe ④ 如果启动服务报错，可以输入下面命令暂停服务，从新执行1sc delete apache2.4 二、php安装① 官网下载安装包https://windows.php.net/download/② 解压到E:\\Program\\wamp③ 配置环境变量(非必须)将安装目录E:\\Program\\wamp\\php-7.2.26-Win32-VC15-x64添加到系统环境变量中，在cmd中用php -v命令检验一下 ④ 配置php.ini文件 在PHP根目录中有php.ini-production和php.ini-development这两个配置文件（使用哪个取决于你自己，PHP官方建议将前者用于生产环境，后者多用于开发环境），这里我选择使用php.ini-development，现在将php.ini-development复制一份备份，然后将php.ini-development改为php.ini即可。 ⑤ 现在来配置 Apache 来试用 PHP，打开 Apache 的配置文件httpd.conf，在所有LoadModule行后添加以下代码：12345# php7 supportLoadModule php7_module E:&#x2F;Program&#x2F;wamp&#x2F;php-7.2.26-Win32-VC15-x64&#x2F;php7apache2_4.dllAddType application&#x2F;x-httpd-php .php .html .htm#configure the path to php.iniPHPIniDir E:&#x2F;Program&#x2F;wamp&#x2F;php-7.2.26-Win32-VC15-x64 ⑥ 保存文件后，现在我们重启一下 Apache 服务器，如果没有看到错误提示，则说明安装是成功的，我们可以在站点根目录创建一个phpinfo文件查看是否安装成功。即在E:\\Program\\wamp\\Apache24\\htdocs目录创建一个.php的文件并写入 1&lt;?php phpinfo();?&gt; 在浏览器中输入http://localhost:81/test.php访问, ⑦ php.ini常用配置修改 1)设置PHP扩展包的具体目录，找到12; On windows:; extension_dir &#x3D; “ext” 改为(要去掉前面的注释)12; On windows:extension_dir &#x3D; &quot;E:&#x2F;Program&#x2F;wamp&#x2F;php-7.2.26-Win32-VC15-x64&#x2F;ext&quot; 2）开启相应的库功能，找到需要开启的库的所在行(去掉注释)123456extension&#x3D;curlextension&#x3D;gd2extension&#x3D;mbstringextension&#x3D;mysqliextension&#x3D;pdo_mysqlextension&#x3D;xmlrpc ⑧ 更改虚拟目录 因为PHP的运行需要Apache支持，所以，PHP目录需要告知Apache。在Apache的配置文件中配置PHP虚拟目录的位置去掉这个配置项的注释 1LoadModule access_compat_module modules&#x2F;mod_access_compat.so 然后 123456789DocumentRoot &quot;F:\\php_htddocs&quot;&lt;Directory &quot;F:\\php_htddocs&quot;&gt; Options Indexes FollowSymLinks AllowOverride all Require all granted Order Deny,Allow Deny from all Allow from all&lt;&#x2F;Directory&gt; 已经在指定目录下了 三、配置mysql四、配置虚拟主机本地的DNS解析器是C:\\Windows\\System32\\drivers\\etc\\hosts文件我们现在在里面随便指定一个域名到本机 1127.0.0.1 www.baba.com 在配置文件里E:\\Program\\wamp\\Apache24\\conf\\httpd.conf中打开配置项 1Include conf&#x2F;extra&#x2F;httpd-vhosts.conf 在E:\\Program\\wamp\\Apache24\\conf\\extra\\httpd-vhosts.conf修改 12345678910&lt;VirtualHost *:81&gt; #ServerAdmin webmaster@dummy-host2.example.com DocumentRoot &quot;F:\\php_htddocs\\web1&quot; ServerName www.baba.com #ErrorLog &quot;logs&#x2F;dummy-host2.example.com-error.log&quot; #CustomLog &quot;logs&#x2F;dummy-host2.example.com-access.log&quot; common &lt;Directory &quot;F:\\php_htddocs\\web1&quot;&gt; Allow from all &lt;&#x2F;Directory&gt;&lt;&#x2F;VirtualHost&gt; (记得创建web1文件夹)","categories":[{"name":"win10+php环境搭建","slug":"win10-php环境搭建","permalink":"http://www.studyz.club/categories/win10-php%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"win10+php环境搭建","slug":"win10-php环境搭建","permalink":"http://www.studyz.club/tags/win10-php%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"}]},{"title":"hadoop-Hive的DDL数据定义以及DML数据操作","slug":"hadoop-Hive的DDL数据定义以及DML数据操作","date":"2019-12-25T02:40:14.958Z","updated":"2019-12-26T02:58:14.809Z","comments":true,"path":"posts/ded6657a/","link":"","permalink":"http://www.studyz.club/posts/ded6657a/","excerpt":"","text":"一、DDL数据定义 1.1、创建数据库 1.2、查询数据库 1.3、修改数据库 1.4、创建表 1.4.1、管理表(内部表) 1.4.2、外部表 1.4.3、管理表与外部表的互相转换 1.5、分区表 一、DDL数据定义1.1、创建数据库① 创建一个数据库，数据库在 HDFS 上的默认存储路径是/user/hive/warehouse/*.db。123456789hive (default)&gt; create database db_hive;OKTime taken: 13.401 secondshive (default)&gt; show databases;OKdatabase_namedb_hivedefaultTime taken: 0.264 seconds, Fetched: 2 row(s) 使用这个数据库 1234hive (default)&gt; use db_hive;OKTime taken: 0.079 secondshive (db_hive)&gt; ② 避免要创建的数据库已经存在错误，增加 if not exists 判断。（标准写法）1hive (db_hive)&gt; create database if not exists db_hive2; ③ 创建一个数据库，指定数据库在 HDFS 上存放的位置123hive (db_hive)&gt; create database db_hive3 location &#39;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;user&#x2F;db_hive3.db&#39;;OKTime taken: 0.146 seconds 如果创建的时候location后面没跟库名。创建的就是类似于default库，之间在文件目录下，没有库名。create database db_hive3 location ‘/user/hive/warehouse/user’;这样创建的表就会储存在user目录下，没有*.db目录。 注意:在default库中创建的表会在warehouse目录下，并没有与库名对应的*.db目录 1.2、查询数据库① 显示数据库1234567hive (db_hive)&gt; show databases;OKdatabase_namedb_hivedb_hive3defaultTime taken: 0.017 seconds, Fetched: 3 row(s) ② 过滤显示查询的数据库123456hive (db_hive)&gt; show databases like &#39;db_hive*&#39;;OKdatabase_namedb_hivedb_hive3Time taken: 0.053 seconds, Fetched: 2 row(s) 查看数据库详情③ 显示数据库信息12345hive (db_hive)&gt; desc database extended db_hive;OKdb_name comment location owner_name owner_type parametersdb_hive hdfs:&#x2F;&#x2F;mycluster&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;db_hive.db root USERTime taken: 0.101 seconds, Fetched: 1 row(s) ④ 显示数据库详细信息，extended(扩展的)12345hive (db_hive)&gt; desc database extended db_hive;OKdb_name comment location owner_name owner_type parametersdb_hive hdfs:&#x2F;&#x2F;mycluster&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;db_hive.db root USERTime taken: 0.101 seconds, Fetched: 1 row(s) ⑤ 切换当前数据库1hive (db_hive)&gt; use db_hive3; 1.3、修改数据库 用户可以使用 ALTER DATABASE 命令为某个数据库的 DBPROPERTIES 设置键-值对属性值，来描述这个数据库的属性信息。数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。 无法改数据库的位置信息。 ① 删除空数据库123456789hive (db_hive)&gt; drop database db_hive3;OKTime taken: 0.653 secondshive (db_hive)&gt; show databases;OKdatabase_namedb_hivedefaultTime taken: 0.023 seconds, Fetched: 2 row(s) ② 如果删除的数据库不存在，最好采用 if exists 判断数据库是否存在1hive (db_hive)&gt; drop database if exists db_hive2; ③ 如果数据库不为空，可以采用 cascade 命令，强制删除12345678910hive (db_hive)&gt; drop database db_hive;FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database db_hive is not empty. One or more tables exist.)hive (db_hive)&gt; drop database db_hive cascade;OKTime taken: 0.715 secondshive (db_hive)&gt; show databases;OKdatabase_namedefaultTime taken: 0.023 seconds, Fetched: 1 row(s) 1.4、创建表查看建表信息 12345678910111213141516171819202122232425hive (db_hive)&gt; show create table test;OKcreatetab_stmtCREATE TABLE &#96;test&#96;( &#96;name&#96; string, &#96;friends&#96; array&lt;string&gt;, &#96;children&#96; map&lt;string,int&gt;, &#96;address&#96; struct&lt;street:string,city:string&gt;)ROW FORMAT SERDE &#39;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&#39;WITH SERDEPROPERTIES ( &#39;colelction.delim&#39;&#x3D;&#39;_&#39;, &#39;field.delim&#39;&#x3D;&#39;,&#39;, &#39;line.delim&#39;&#x3D;&#39;\\n&#39;, &#39;mapkey.delim&#39;&#x3D;&#39;:&#39;, &#39;serialization.format&#39;&#x3D;&#39;,&#39;)STORED AS INPUTFORMAT &#39;org.apache.hadoop.mapred.TextInputFormat&#39;OUTPUTFORMAT &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39;LOCATION &#39;hdfs:&#x2F;&#x2F;mycluster&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;db_hive.db&#x2F;test&#39;TBLPROPERTIES ( &#39;transient_lastDdlTime&#39;&#x3D;&#39;1577274894&#39;)Time taken: 0.32 seconds, Fetched: 21 row(s) ① 建表语法CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [COMMENT col_comment], ...)][COMMENT table_comment][PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)][CLUSTERED BY (col_name, col_name, …)[SORTED BY (col_name [ASC|DESC], …)] INTO num_buckets BUCKETS][ROW FORMAT row_format][STORED AS file_format][LOCATION hdfs_path] ② 字段解释 1）CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。 2）EXTERNAL 关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。 3）COMMENT：为表和列添加注释。 4）PARTITIONED BY 创建分区表 5）CLUSTERED BY 创建分桶表 6）SORTED BY 不常用 7）ROW FORMAT 1234DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] | SERDE serde_name [WITH SERDEPROPERTIES (property_name&#x3D;property_value,property_name&#x3D;property_value, ...)] 用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive 通过 SerDe确定表的具体的列的数据。 SerDe 是 Serialize/Deserilize 的简称，目的是用于序列化和反序列化。 8）STORED AS 指定存储文件类型 常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件） 如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。 9）LOCATION ：指定表在 HDFS 上的存储位置。 10）LIKE 允许用户复制现有的表结构，但是不复制数据。 1.4.1、管理表(内部表)① 理论 默认创建的表都是所谓的管理表，有时也被称为内部表。因为这种表，Hive 会（或多或少地）控制着数据的生命周期。Hive 默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。当我们删除一个管理表时，Hive 也会删除这个表中数据。管理表不适合和其他工具共享数据。 ② 实例操作 1）普通创建表123456create table if not exists student2(id int, name string)row format delimited fields terminated by &#39;\\t&#39;stored as textfilelocation &#39;&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;student2&#39;; 2）根据查询结果创建表（查询的结果会添加到新创建的表中）1create table if not exists student3 as select id, name from student; 3）根据已经存在的表结构创建表1create table if not exists student4 like student; 4）查询表的类型12hive (default)&gt; desc formatted student2;Table Type: MANAGED_TABLE 内部表认为数据是自己管理的，删除的时候会连通元数据，hdfs上的原始数据一同删除。外部表是建一个数据表去管理hdfs上的数据，在删除的时候无法删除原始数据。 例如:我们删除内部表(default数据库) 12345678910111213141516171819hive (db_hive)&gt; use default;OKTime taken: 0.047 secondshive (default)&gt; show tables;;OKtab_namestustudenttestTime taken: 0.061 seconds, Fetched: 3 row(s)hive (default)&gt; drop table test;OKTime taken: 0.373 secondshive (default)&gt; show tables;OKtab_namestustudentTime taken: 0.038 seconds, Fetched: 2 row(s) 我们发现hdfs里的数据也一并删除了 删除外部表的示例在下面1.4.2、外部表① 理论 因为表是外部表，所以 Hive 并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。 ② 管理表和外部表的使用场景 每天将收集到的网站日志定期流入 HDFS 文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过 SELECT+INSERT进入内部表。 ③ 案例实操 分别创建部门和员工外部表，并向表中导入数据。 （1）原始数据dept.txt 123410 ACCOUNTING 170020 RESEARCH 180030 SALES 190040 OPERATIONS 1700 emp.txt 12345678910111213147369 SMITH CLERK 7902 1980-12-17 800.00 207499 ALLEN SALESMAN 7698 1981-2-20 1600.00 300.00 307521 WARD SALESMAN 7698 1981-2-22 1250.00 500.00 307566 JONES MANAGER 7839 1981-4-2 2975.00 207654 MARTIN SALESMAN 7698 1981-9-28 1250.00 1400.00 307698 BLAKE MANAGER 7839 1981-5-1 2850.00 307782 CLARK MANAGER 7839 1981-6-9 2450.00 107788 SCOTT ANALYST 7566 1987-4-19 3000.00 207839 KING PRESIDENT 1981-11-17 5000.00 107844 TURNER SALESMAN 7698 1981-9-8 1500.00 0.00 307876 ADAMS CLERK 7788 1987-5-23 1100.00 207900 JAMES CLERK 7698 1981-12-3 950.00 307902 FORD ANALYST 7566 1981-12-3 3000.00 207934 MILLER CLERK 7782 1982-1-23 1300.00 10 （2）建表语句创建部门表 123hive (default)&gt; create external table if not exists default.dept(deptno int,dname string,loc int)row format delimited fields terminated by &#39;\\t&#39;;OKTime taken: 0.228 seconds 创建员工表 12345678910create external table if not exists default.emp(empno int,ename string,job string,mgr int,hiredate string, sal double, comm double,deptno int)row format delimited fields terminated by &#39;\\t&#39;; （3）查看创建的表 12345678hive (default)&gt; show tables;OKtab_namedeptempstustudentTime taken: 0.052 seconds, Fetched: 4 row(s) （4）向外部表中导入数据导入数据 12345678hive (default)&gt; load data local inpath &#39;&#x2F;root&#x2F;test&#x2F;dept.txt&#39; into table default.dept;Loading data to table default.deptOKTime taken: 1.357 secondshive (default)&gt; load data local inpath &#39;&#x2F;root&#x2F;test&#x2F;emp.txt&#39; into table default.emp;Loading data to table default.empOKTime taken: 1.108 seconds 查询结果(原始数据的时候第一行空着了，所以是NULL) 12345678910111213141516171819202122232425262728hive (default)&gt; select * from dept;OKdept.deptno dept.dname dept.locNULL NULL NULL10 ACCOUNTING 170020 RESEARCH 180030 SALES 190040 OPERATIONS 1700Time taken: 4.448 seconds, Fetched: 5 row(s)hive (default)&gt; select * from emp;OKemp.empno emp.ename emp.job emp.mgr emp.hiredate emp.sal emp.comm emp.deptnoNULL NULL NULL NULL NULL NULL NULL NULL7369 SMITH CLERK 7902 1980-12-17 800.0 NULL 207499 ALLEN SALESMAN 7698 1981-2-20 1600.0 300.0 307521 WARD SALESMAN 7698 1981-2-22 1250.0 500.0 307566 JONES MANAGER 7839 1981-4-2 2975.0 NULL 207654 MARTIN SALESMAN 7698 NULL 1250.00 1400.0 30.0 NULL7698 BLAKE MANAGER 7839 1981-5-1 2850.0 NULL 307782 CLARK MANAGER 7839 1981-6-9 2450.0 NULL 107788 SCOTT ANALYST 7566 1987-4-19 3000.0 NULL 207839 KING PRESIDENT NULL 5000.00 NULL 10.0 NULL7844 TURNER SALESMAN 7698 1981-9-8 1500.0 0.0 307876 ADAMS CLERK 7788 1987-5-23 1100.0 NULL 207900 JAMES CLERK 7698 1981-12-3 950.0 NULL 307902 FORD ANALYST 7566 1981-12-3 3000.0 NULL 207934 MILLER CLERK 7782 1982-1-23 1300.0 NULL 10Time taken: 0.478 seconds, Fetched: 15 row(s) （5）查看表格式化数据 12345678910111213141516171819202122232425262728293031323334353637hive (default)&gt; desc formatted dept;OKcol_name data_type comment# col_name data_type commentdeptno intdname stringloc int# Detailed Table InformationDatabase: defaultOwner: rootCreateTime: Wed Dec 25 20:17:10 CST 2019LastAccessTime: UNKNOWNRetention: 0Location: hdfs:&#x2F;&#x2F;mycluster&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;deptTable Type: EXTERNAL_TABLETable Parameters: EXTERNAL TRUE numFiles 1 numRows 0 rawDataSize 0 totalSize 70 transient_lastDdlTime 1577276682# Storage InformationSerDe Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDeInputFormat: org.apache.hadoop.mapred.TextInputFormatOutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormatCompressed: NoNum Buckets: -1Bucket Columns: []Sort Columns: []Storage Desc Params: field.delim \\t serialization.format \\tTime taken: 0.152 seconds, Fetched: 33 row(s) (6)删除外部表示例 123456789101112131415161718hive (default)&gt; show tables;OKtab_namedeptempstustudentTime taken: 0.053 seconds, Fetched: 4 row(s)hive (default)&gt; drop table dept;OKTime taken: 0.391 secondshive (default)&gt; show tables;OKtab_nameempstustudentTime taken: 0.042 seconds, Fetched: 3 row(s) 我们发现虽然表的元数据删除了但是hdfs上的数据并没有删除如果我们再创建表会发现，表依然可以创建，并且可以查询 1234567891011121314151617181920hive (default)&gt; create external table if not exists default.dept(deptno int,dname string,loc int)row format delimited fields terminated by &#39;\\t&#39;;OKTime taken: 0.163 secondshive (default)&gt; show tables;OKtab_namedeptempstustudentTime taken: 0.038 seconds, Fetched: 4 row(s)hive (default)&gt; select * from dept;OKdept.deptno dept.dname dept.locNULL NULL NULL10 ACCOUNTING 170020 RESEARCH 180030 SALES 190040 OPERATIONS 1700Time taken: 0.308 seconds, Fetched: 5 row(s) 删除内部表的示例在上面 因此一般我们建表的时候一般建外部表比较安全，包括在生产环境中。只有当我们要建临时表或中间表的时候，用完就删的情况下建内部表。如果数据被多个部门共同管理的时候一定要建外部表。1.4.3、管理表与外部表的互相转换 1）查询表的类型 12345678910111213141516171819202122232425262728293031323334hive (default)&gt; desc formatted student;OKcol_name data_type comment# col_name data_type commentid intname string# Detailed Table InformationDatabase: defaultOwner: rootCreateTime: Mon Dec 23 09:18:50 CST 2019LastAccessTime: UNKNOWNRetention: 0Location: hdfs:&#x2F;&#x2F;mycluster&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;studentTable Type: MANAGED_TABLETable Parameters: numFiles 3 numRows 2 rawDataSize 12 totalSize 41 transient_lastDdlTime 1577108583# Storage InformationSerDe Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDeInputFormat: org.apache.hadoop.mapred.TextInputFormatOutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormatCompressed: NoNum Buckets: -1Bucket Columns: []Sort Columns: []Storage Desc Params: serialization.format 1Time taken: 0.103 seconds, Fetched: 30 row(s) 从上面我们可以看到表的类型 1Table Type: MANAGED_TABLE 2）修改内部表 student 为外部表(内部表=&gt;外部表TRUE,外部表=&gt;内部表FALSE) 123hive (default)&gt; alter table student set tblproperties(&#39;EXTERNAL&#39;&#x3D;&#39;TRUE&#39;);OKTime taken: 0.173 seconds 3）查询表的类型 12345678910111213141516171819202122232425262728293031323334353637hive (default)&gt; desc formatted student;OKcol_name data_type comment# col_name data_type commentid intname string# Detailed Table InformationDatabase: defaultOwner: rootCreateTime: Mon Dec 23 09:18:50 CST 2019LastAccessTime: UNKNOWNRetention: 0Location: hdfs:&#x2F;&#x2F;mycluster&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;studentTable Type: EXTERNAL_TABLETable Parameters: EXTERNAL TRUE last_modified_by root last_modified_time 1577279835 numFiles 3 numRows 2 rawDataSize 12 totalSize 41 transient_lastDdlTime 1577279835# Storage InformationSerDe Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDeInputFormat: org.apache.hadoop.mapred.TextInputFormatOutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormatCompressed: NoNum Buckets: -1Bucket Columns: []Sort Columns: []Storage Desc Params: serialization.format 1Time taken: 0.112 seconds, Fetched: 33 row(s) 发现已经转换成了外部表 1Table Type: EXTERNAL_TABLE 注意：(&#39;EXTERNAL&#39;=&#39;TRUE&#39;)和(&#39;EXTERNAL&#39;=&#39;FALSE&#39;)为固定写法，区分大小写！ 如果是小写会增加一个属性在Table Parameters:下面external true，而本身的Table Type: 并不会改变。 1.5、分区表 分区表实际上就是对应一个 HDFS 文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive 中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过 WHERE 子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。 也就是说,本来hive建立的数据库对应的是hdfs上的一个文件夹，里面存放着各种数据，分区就是再建立一个或多个子文件夹用来对不同的表进行分类。这样在进行全表扫描的时候比较方便。可以按天，或者小时建立文件夹存储表。 1.5.1、分区表基本操作 引入分区表（需要根据日期对日志进行管理）123&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_partition&#x2F;20191225&#x2F;20191225.log&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_partition&#x2F;20191226&#x2F;20191226.log&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;log_partition&#x2F;20191227&#x2F;20191227.log 创建分区表语法12345hive (default)&gt; create table dept_partition(deptno int, dname string, loc string)&#96;partitioned by (month string)&#96;row format delimited fields terminated by &#39;\\t&#39;; 运行完之后会多个表 dept_partition123456789hive (default)&gt; show tables;OKtab_namedeptdept_partitionempstustudentTime taken: 0.035 seconds, Fetched: 5 row(s) 加载数据到分区表中123456789hive (default)&gt; load data local inpath &#39;&#x2F;root&#x2F;test&#x2F;dept.txt&#39; into table default.dept_partition partition(month&#x3D;&#39;201912&#39;);Loading data to table default.dept_partition partition (month&#x3D;201912)OKTime taken: 1.57 secondshive (default)&gt; load data local inpath &#39;&#x2F;root&#x2F;test&#x2F;dept.txt&#39; into table default.dept_partition partition(month&#x3D;&#39;201911&#39;);Loading data to table default.dept_partition partition (month&#x3D;201911)OKTime taken: 1.644 seconds 查询分区表中数据单分区查询123456789hive (default)&gt; select * from dept_partition where month&#x3D;&#39;201912&#39;;OKdept_partition.deptno dept_partition.dname dept_partition.loc dept_partition.monthNULL NULL NULL 20191210 ACCOUNTING 1700 20191220 RESEARCH 1800 20191230 SALES 1900 20191240 OPERATIONS 1700 201912Time taken: 1.687 seconds, Fetched: 5 row(s) 多分区联合查询``联合查询之前最好先开启本地模式。要不然跑mr会卡住hive (default)&gt; set hive.exec.mode.local.auto=true;1234567891011121314151617181920212223242526272829303132333435&#96;&#96;&#96;hive (default)&gt; select * from dept_partition where month&#x3D;&#39;201912&#39; union select * from dept_partition where month&#x3D;&#39;201911&#39;;Automatically selecting local only mode for queryWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.Query ID &#x3D; root_20191225231822_978aec2d-26d3-4be8-b909-b924536049f6Total jobs &#x3D; 1Launching Job 1 out of 1Number of reduce tasks not specified. Estimated from input data size: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer&#x3D;&lt;number&gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max&#x3D;&lt;number&gt;In order to set a constant number of reducers: set mapreduce.job.reduces&#x3D;&lt;number&gt;Job running in-process (local Hadoop)2019-12-25 23:18:25,000 Stage-1 map &#x3D; 0%, reduce &#x3D; 0%2019-12-25 23:18:26,047 Stage-1 map &#x3D; 100%, reduce &#x3D; 100%Ended Job &#x3D; job_local999609340_0001MapReduce Jobs Launched:Stage-Stage-1: HDFS Read: 350 HDFS Write: 103861531 SUCCESSTotal MapReduce CPU Time Spent: 0 msecOK_u1.deptno _u1.dname _u1.loc _u1.monthNULL NULL NULL 201911NULL NULL NULL 20191210 ACCOUNTING 1700 20191110 ACCOUNTING 1700 20191220 RESEARCH 1800 20191120 RESEARCH 1800 20191230 SALES 1900 20191130 SALES 1900 20191240 OPERATIONS 1700 20191140 OPERATIONS 1700 201912Time taken: 3.864 seconds, Fetched: 10 row(s)","categories":[{"name":"hadoop-hive-DDL-DML","slug":"hadoop-hive-DDL-DML","permalink":"http://www.studyz.club/categories/hadoop-hive-DDL-DML/"}],"tags":[{"name":"-hive","slug":"hive","permalink":"http://www.studyz.club/tags/hive/"}]},{"title":"Hive报错处理","slug":"Hive报错处理","date":"2019-12-22T14:59:32.869Z","updated":"2019-12-26T02:58:14.798Z","comments":true,"path":"posts/c651ad21/","link":"","permalink":"http://www.studyz.club/posts/c651ad21/","excerpt":"","text":"问题一:FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient在对hive进行操作时报错1FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient 在hive-site.xml中添加 1234&lt;property&gt; &lt;name&gt;datanucleus.schema.autoCreateAll&lt;&#x2F;name&gt; &lt;value&gt;true&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; 如果是hive连接过的mysql则需要删除hive_metastore之后再初始话，第一次连接则不需要 123456drop database hive_metastore;Query OK, 57 rows affected (0.42 sec) mysql &gt; create database hive_metastore;Query OK, 1 row affected (0.18 sec) 初始化元数据库 12345678[root@master conf]# schematool -dbType mysql -initSchemaMetastore connection URL: jdbc:mysql:&#x2F;&#x2F;master:3306&#x2F;metastore?createDatabaseIfNotExist&#x3D;trueMetastore Connection Driver : com.mysql.cj.jdbc.DriverMetastore connection User: rootStarting metastore schema initialization to 2.3.0Initialization script hive-schema-2.3.0.mysql.sqlInitialization script completedschemaTool completed 如果没有删除原来映射到MySQL中的表的话会报如下错误：org.apache.hadoop.hive.metastore.HiveMetaException 12345678910jdbc:mysql:&#x2F;&#x2F;master:3306&#x2F;metastore?createDatabaseIfNotExist&#x3D;trueMetastore Connection Driver : com.mysql.cj.jdbc.DriverMetastore connection User: rootStarting metastore schema initialization to 2.3.0Error: Duplicate key name &#39;PCS_STATS_IDX&#39; (state&#x3D;42000,code&#x3D;1061)org.apache.hadoop.hive.metastore.HiveMetaException: Schema initialization FAILED! Metastore state would be inconsistent !!Underlying cause: java.io.IOException : Schema script failed, errorcode 2Use --verbose for detailed stacktrace.*** schemaTool failed *** 问题二: Number of reduce tasks is set to 0 since there’s no reduce operator hive 向表中插入数据时卡在这个地方(insert) 卡了n久之后报错信息终于出来了，是因为没有连接上resourcemanager，于是我将resourcemanager部署到和hive一个节点，注意：如果是移动resourcemanager要清空和zookeeper里的zkData/version-2等信息。重启,清除version-2就相当于把zookeeper格式化了。别忘了把同目录的pid文件删除。问题三[①(这里之所以是①是因为这个办法之后虽然有效，但还是会卡住，下面有报错信息在②里)]: Kill Command = /usr/local/hadoop-2.7.3/bin/hadoop job -kill job_1577097476269_0001同样等了n久，报错信息如下12345678910111213141516WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.Query ID &#x3D; root_20191223184504_1d439037-be80-4f3b-a8f4-cfc19a80e806Total jobs &#x3D; 3Launching Job 1 out of 3Number of reduce tasks is set to 0 since there&#39;s no reduce operatorStarting Job &#x3D; job_1577097476269_0001, Tracking URL &#x3D; http:&#x2F;&#x2F;master:8088&#x2F;proxy&#x2F;application_1577097476269_0001&#x2F;Kill Command &#x3D; &#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;bin&#x2F;hadoop job -kill job_1577097476269_0001Hadoop job information for Stage-1: number of ma ppers: 0; number of reducers: 02019-12-23 19:11:17,425 Stage-1 map &#x3D; 0%, reduc e &#x3D; 0%Ended Job &#x3D; job_1577097476269_0001 with errorsError during job, obtaining debugging informatio n...FAILED: Execution Error, return code 2 from org. apache.hadoop.hive.ql.exec.mr.MapRedTaskMapReduce Jobs Launched:Stage-Stage-1: HDFS Read: 0 HDFS Write: 0 FAILTotal MapReduce CPU Time Spent: 0 msec 原因是namenode内存空间不够，jvm不够新job启动导致。大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。(有可能是这个原因导致大但我的不是，具体看卡住的第二回分解②)不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务时消耗可能会比实际job的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化。 12345678&#x2F;&#x2F;开启本地mrhive&gt; set hive.exec.mode.local.auto&#x3D;true; &#x2F;&#x2F;设置local mr的最大输入数据量，当输入数据量小于这个值时采用local mr的方式，默认为134217728，即128Mhive&gt; set hive.exec.mode.local.auto.inputbytes.max&#x3D;50000000;&#x2F;&#x2F;设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4hive&gt; set hive.exec.mode.local.auto.input.files.max&#x3D;10; 再次运行 12345678910111213141516171819202122232425hive&gt; insert into table student values(2,&#39;erza&#39;);Automatically selecting local only mode for queryWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.Query ID &#x3D; root_20191223194935_56369692-19e6-450a-8b1c-aaa841d932bfTotal jobs &#x3D; 3Launching Job 1 out of 3Number of reduce tasks is set to 0 since there&#39;s no reduce operatorJob running in-process (local Hadoop)2019-12-23 19:49:38,033 Stage-1 map &#x3D; 100%, reduce &#x3D; 0%Ended Job &#x3D; job_local1312049602_0002Stage-4 is selected by condition resolver.Stage-3 is filtered out by condition resolver.Stage-5 is filtered out by condition resolver.Moving data to directory hdfs:&#x2F;&#x2F;mycluster&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;student&#x2F;.hive-staging_hive_2019-12-23_19-49-35_791_2882996143253230519-1&#x2F;-ext-10000Loading data to table default.studentMapReduce Jobs Launched:Stage-Stage-1: HDFS Read: 92 HDFS Write: 170 SUCCESSTotal MapReduce CPU Time Spent: 0 msecOKTime taken: 3.185 secondshive&gt; select * from student;OK1 baba2 erzaTime taken: 0.295 seconds, Fetched: 2 row(s) 问题三[②]: Kill Command = /usr/local/hadoop-2.7.3/bin/hadoop job -kill job_1577097476269_0001这条命令在hive客户端卡了n久之后出来了报错信息(我的虚拟机内存已经由2G提升到了4G) 12345Error during job, obtaining debugging information...FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTaskMapReduce Jobs Launched:Stage-Stage-1: HDFS Read: 0 HDFS Write: 0 FAILTotal MapReduce CPU Time Spent: 0 msec 感觉没啥用,然后去看yarn的mr日志http://master.:8042/node/containerlogs/container_1577324512799_0001_01_000001/root/syslog/?start=0 然后发现了两条报错信息 123456789102019-12-25 22:55:04,034 WARN [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Could not parse the old history file. Will not have old AMinfos java.io.FileNotFoundException: File does not exist: &#x2F;tmp&#x2F;hadoop-yarn&#x2F;staging&#x2F;root&#x2F;.staging&#x2F;job_1577284899317_0001&#x2F;job_1577284899317_0001_1.jhist at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71) at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: &#x2F;tmp&#x2F;hadoop-yarn&#x2F;staging&#x2F;root&#x2F;.staging&#x2F;job_1577284899317_0001&#x2F;job_1577284899317_0001_1.jhist at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71) at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828) 这个报错由啥引起的呢，不解然后看下面的报错 1232019-12-25 22:55:08,140 INFO [main] org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at &#x2F;0.0.0.0:80302019-12-25 22:55:09,358 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0&#x2F;0.0.0.0:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries&#x3D;10, sleepTime&#x3D;1000 MILLISECONDS)2019-12-25 22:55:10,359 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0&#x2F;0.0.0.0:8030. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries&#x3D;10, sleepTime&#x3D;1000 MILLISECONDS) 这个报错倒是在网上找到了答案在yarn-site.xml文件里添加如下配置(具体有没有用我也不知道，因为配置完之后再运行还是卡住了) 123456789101112&lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;&#x2F;name&gt; &lt;value&gt;master:8032&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;&#x2F;name&gt; &lt;value&gt;master:8030&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;&#x2F;name&gt; &lt;value&gt;master:8031&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; 然后百度找不到就去咕噜咕噜找答案，幸运的是在stackoverflow看到了一个类似的问题 1.staging&#x2F;job_1541144755485_0002&#x2F;job.splitmetainfo does not exist 下面有说运行任务的这台机器没有启动DataNode，然后我在slaves文件里填上master之后再去运行，发现通过了。这如便秘般的堵塞终于被我顺利的拉出去了。(但是速度，没有在本地模式拉的快，小文件) 123456789101112131415161718192021222324252627282930313233343536Hive (default)&gt; select * from dept_partition where month&#x3D;&#39;201912&#39; union select * from dept_partition where month&#x3D;&#39;201911&#39;;WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.Query ID &#x3D; root_20191226095547_b1549a4c-c0a4-4823-a536-4881e0575974Total jobs &#x3D; 1Launching Job 1 out of 1Number of reduce tasks not specified. Estimated from input data size: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer&#x3D;&lt;number&gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max&#x3D;&lt;number&gt;In order to set a constant number of reducers: set mapreduce.job.reduces&#x3D;&lt;number&gt;Starting Job &#x3D; job_1577324512799_0001, Tracking URL &#x3D; http:&#x2F;&#x2F;slave1:8088&#x2F;proxy&#x2F;application_1577324512799_0001&#x2F;Kill Command &#x3D; &#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;bin&#x2F;hadoop job -kill job_1577324512799_0001Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 12019-12-26 09:56:37,712 Stage-1 map &#x3D; 0%, reduce &#x3D; 0%2019-12-26 09:56:53,717 Stage-1 map &#x3D; 100%, reduce &#x3D; 0%, Cumulative CPU 4.68 sec2019-12-26 09:57:06,786 Stage-1 map &#x3D; 100%, reduce &#x3D; 100%, Cumulative CPU 7.31 secMapReduce Total cumulative CPU time: 7 seconds 310 msecEnded Job &#x3D; job_1577324512799_0001MapReduce Jobs Launched:Stage-Stage-1: Map: 2 Reduce: 1 Cumulative CPU: 7.31 sec HDFS Read: 17113 HDFS Write: 433 SUCCESSTotal MapReduce CPU Time Spent: 7 seconds 310 msecOK_u1.deptno _u1.dname _u1.loc _u1.monthNULL NULL NULL 201911NULL NULL NULL 20191210 ACCOUNTING 1700 20191110 ACCOUNTING 1700 20191220 RESEARCH 1800 20191120 RESEARCH 1800 20191230 SALES 1900 20191130 SALES 1900 20191240 OPERATIONS 1700 20191140 OPERATIONS 1700 201912Time taken: 81.493 seconds, Fetched: 10 row(s) 这期间我经历了调大内存，未知有木有用，先记录一下吧 不过在执行查询的时候，虚拟机占用的内存确实超过了2Gyarn-site.xml 12345678910111213141516171819202122&lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;&#x2F;name&gt; &lt;value&gt;4096&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;&#x2F;name&gt; &lt;value&gt;2048&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;&#x2F;name&gt; &lt;value&gt;2.1&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.memory.mb&lt;&#x2F;name&gt; &lt;value&gt;1024&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.memory.mb&lt;&#x2F;name&gt; &lt;value&gt;1024&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; 问题四: User: root is not allowed to impersonate root (state=08S01,code=0) 在用beeline连接hiveserver2时报错 123456beeline&gt; !connect jdbc:hive2:&#x2F;&#x2F;master:10000&#x2F;defaultConnecting to jdbc:hive2:&#x2F;&#x2F;master:10000&#x2F;defaultEnter username for jdbc:hive2:&#x2F;&#x2F;master:10000&#x2F;default: rootEnter password for jdbc:hive2:&#x2F;&#x2F;master:10000&#x2F;default:19&#x2F;12&#x2F;24 09:24:06 [main]: WARN jdbc.HiveConnection: Failed to connect to master:10000Error: Could not open client transport with JDBC Uri: jdbc:hive2:&#x2F;&#x2F;master:10000&#x2F;default: Failed to open new session: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User: root is not allowed to impersonate root (state&#x3D;08S01,code&#x3D;0) 问题解决我配置了以下两个内容在hadoop 的core-site.xml添加如下内容 然后重启 1234567891011&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;&#x2F;name&gt; &lt;value&gt;root&lt;&#x2F;value&gt; &lt;description&gt;Allow the superuser oozie to impersonate any members of the group group1 and group2&lt;&#x2F;description&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;&#x2F;name&gt; &lt;value&gt;*&lt;&#x2F;value&gt; &lt;description&gt;The superuser can connect only from host1 and host2 to impersonate a user&lt;&#x2F;description&gt;&lt;&#x2F;property&gt; 由于我在配置完这个后没有重启虚拟机因此不知道单配置这个能否成功 然后在hive-site.xml中添加如下内容 12345678910&lt;property&gt; &lt;name&gt;hive.server2.thrift.client.user&lt;&#x2F;name&gt; &lt;value&gt;root&lt;&#x2F;value&gt; &lt;description&gt;Username to use against thrift client&lt;&#x2F;description&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;hive.server2.thrift.client.password&lt;&#x2F;name&gt; &lt;value&gt;******&lt;&#x2F;value&gt; &lt;description&gt;Password to use against thrift client&lt;&#x2F;description&gt; &lt;&#x2F;property&gt; 然后重启，问题解决 12345678beeline&gt; !connect jdbc:hive2:&#x2F;&#x2F;master:10000&#x2F;defaultConnecting to jdbc:hive2:&#x2F;&#x2F;master:10000&#x2F;defaultEnter username for jdbc:hive2:&#x2F;&#x2F;master:10000&#x2F;default: rootEnter password for jdbc:hive2:&#x2F;&#x2F;master:10000&#x2F;default: ******Connected to: Apache Hive (version 2.3.6)Driver: Hive JDBC (version 2.3.6)Transaction isolation: TRANSACTION_REPEATABLE_READ0: jdbc:hive2:&#x2F;&#x2F;master:10000&#x2F;default&gt; 在后续使用中由于我没有输入密码直接回车也能连接，因此我猜想是，core-site.xml里的配置文件起了作用。","categories":[{"name":"Hive","slug":"Hive","permalink":"http://www.studyz.club/categories/Hive/"},{"name":"Hadoop-Hive","slug":"Hive/Hadoop-Hive","permalink":"http://www.studyz.club/categories/Hive/Hadoop-Hive/"}],"tags":[{"name":"Hadoop-Hive","slug":"Hadoop-Hive","permalink":"http://www.studyz.club/tags/Hadoop-Hive/"}]},{"title":"Centos7安装MySql8","slug":"Centos7安装MySql8","date":"2019-12-22T05:53:03.053Z","updated":"2019-12-26T02:58:14.797Z","comments":true,"path":"posts/c8a9c56e/","link":"","permalink":"http://www.studyz.club/posts/c8a9c56e/","excerpt":"","text":"前期准备1、先检查有没有装过mysql1rpm -qa | grep -i mysql 2、如果安装了则需要删除mysql1yum -y remove *-MySQL-* 一般用rpm -e 的命令删除mysql,这样表面上删除了mysql,可是mysql的一些残余程序仍然存在,并且通过第一步的方式也查找不到残余,而yum命令比较强大,可以完全删除mysql.(ps:用rpm删除后再次安装的时候会提示已经安装了,这就是rpm没删除干净的原因) 3、找到并删除mysql的目录1find &#x2F; -name mysql 查找mysql的一些目录，把所有出现的目录删除，可以使用rm -rf 路径，删除时请注意，一旦删除无法恢复。 4、删除配置文件1rm -rf &#x2F;etc&#x2F;my.cnf 5、删除mysql的默认密码1rm -rf &#x2F;root&#x2F;.mysql_sercret 删除mysql的默认密码,如果不删除,以后安装mysql这个sercret中的默认密码不会变,使用其中的默认密码就可能会报类似Access denied for user ‘root@localhost’ (using password:yes)的错误. 准备安装1、配置Mysql 8.0安装源1rpm -Uvh https:&#x2F;&#x2F;dev.mysql.com&#x2F;get&#x2F;mysql80-community-release-el7-3.noarch.rpm 2、安装mysql8.01yum --enablerepo&#x3D;mysql80-community install mysql-community-server 看到complet(完毕)就是安装完啦 如果是用安装包安装，service文件要安装在root用户 3、启动mysql服务12[root@master opt]# service mysqld startRedirecting to &#x2F;bin&#x2F;systemctl start mysqld.service 4、查看mysql服务运行状态12345678910111213141516[root@master opt]# service mysqld statusRedirecting to &#x2F;bin&#x2F;systemctl status mysqld.service● mysqld.service - MySQL Server Loaded: loaded (&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;mysqld.service; enabled; vendor preset: disabled) Active: active (running) since 日 2019-12-22 14:04:32 CST; 1min 23s ago Docs: man:mysqld(8) http:&#x2F;&#x2F;dev.mysql.com&#x2F;doc&#x2F;refman&#x2F;en&#x2F;using-systemd.html Process: 4356 ExecStartPre&#x3D;&#x2F;usr&#x2F;bin&#x2F;mysqld_pre_systemd (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS) Main PID: 4444 (mysqld) Status: &quot;Server is operational&quot; CGroup: &#x2F;system.slice&#x2F;mysqld.service └─4444 &#x2F;usr&#x2F;sbin&#x2F;mysqld12月 22 14:04:22 master systemd[1]: Starting MySQL Server...12月 22 14:04:32 master systemd[1]: Started MySQL Server. 4、查看root临时密码12[root@master opt]# grep &quot;A temporary password&quot; &#x2F;var&#x2F;log&#x2F;mysqld.log2019-12-22T06:04:26.807797Z 5 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: &#x2F;!ECy_whH4D- 5、更改临时密码1234567输入：mysql -u root -p在Enter password：后面输入临时密码登录成功输入： ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;new password&#39;;会提示：ERROR 1819 (HY000): Your password does not satisfy the current policy requirements(密码不符合当前策略)方案1: 设置符合策略的密码(大小写字母+数据+符号)方案2:密码策略改简单一点 ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ‘******‘ at line 1这个提示是单引号的问题 6、解决密码问题除了设置更复杂的密码还可以更改密码策略6.1、查看 mysql 初始的密码策略1234567891011121314mysql&gt; SHOW VARIABLES LIKE &#39;validate_password%&#39;;+--------------------------------------+--------+| Variable_name | Value |+--------------------------------------+--------+| validate_password.check_user_name | ON || validate_password.dictionary_file | || validate_password.length | 8 || validate_password.mixed_case_count | 1 || validate_password.number_count | 1 || validate_password.policy | MEDIUM || validate_password.special_char_count | 1 |+--------------------------------------+--------+7 rows in set (0.02 sec) 6.2、首先需要设置密码的验证强度等级，设置 validate_password.policy 的全局参数为 LOW 即可，12mysql&gt; set global validate_password.policy&#x3D;LOW;Query OK, 0 rows affected (0.00 sec) 6.3、当前密码长度为 8 ，按照通用的来讲，设置为 6 位的密码，设置validate_password.length 的全局参数为 6 即可，12mysql&gt; set global validate_password.length&#x3D;6;Query OK, 0 rows affected (0.00 sec) 6.4、现在可以为 mysql 设置简单密码了，只要满足六位的长度即可，123mysql&gt; ALTER USER &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;******&#39;;Query OK, 0 rows affected (0.01 sec) 6.5、注：在默认密码的长度最小值为 4 ，由 大/小写字母各一个 + 阿拉伯数字一个 + 特殊字符一个，只要设置密码的长度小于 3 ，都将自动设值为 46.7、关于 mysql 密码策略相关参数； 1）、validate_password.length 固定密码的总长度； 2）、validate_password.dictionary_file 指定密码验证的文件路径； 3）、validate_password.mixed_case_count 整个密码中至少要包含大/小写字母的总个数； 4）、validate_password.number_count 整个密码中至少要包含阿拉伯数字的个数； 5）、validate_password.policy 指定密码的强度验证等级，默认为 MEDIUM； 关于 validate_password.policy 的取值： LOW：只验证长度； 1/MEDIUM：验证长度、数字、大小写、特殊字符； 2/STRONG：验证长度、数字、大小写、特殊字符、字典文件； validate_password.special_char_count 整个密码中至少要包含特殊字符的个数； 7、退出当前状态123mysql&gt; quit;Bye 或者这样修改密码，待验证 123．修改密码mysql&gt;SET PASSWORD&#x3D;PASSWORD(&#39;000000&#39;); 8、设置允许远程连接8.1、连接服务器12345678910111213[root@master opt]# mysql -u root -pEnter password:Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 10Server version: 8.0.18 MySQL Community Server - GPLCopyright (c) 2000, 2019, Oracle and&#x2F;or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and&#x2F;or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#39;help;&#39; or &#39;\\h&#39; for help. Type &#39;\\c&#39; to clear the current input statement. 8.2、看当前所有数据库12345678910mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys |+--------------------+4 rows in set (0.01 sec) 8.3、进入mysql数据库12mysql&gt; use mysql;Database changed 8.4、查看mysql数据库中所有的表123456789101112131415161718192021222324252627282930313233343536373839mysql&gt; show tables;+---------------------------+| Tables_in_mysql |+---------------------------+| columns_priv || component || db || default_roles || engine_cost || func || general_log || global_grants || gtid_executed || help_category || help_keyword || help_relation || help_topic || innodb_index_stats || innodb_table_stats || password_history || plugin || procs_priv || proxies_priv || role_edges || server_cost || servers || slave_master_info || slave_relay_log_info || slave_worker_info || slow_log || tables_priv || time_zone || time_zone_leap_second || time_zone_name || time_zone_transition || time_zone_transition_type || user |+---------------------------+33 rows in set (0.00 sec) 8.5、查看user表中的数据1234567891011mysql&gt; mysql&gt; select host, user from user;+-----------+------------------+| host | user |+-----------+------------------+| localhost | mysql.infoschema || localhost | mysql.session || localhost | mysql.sys || localhost | root |+-----------+------------------+4 rows in set (0.00 sec) 8.6、修改user表中的Host:1234567891011121314mysql&gt; update user set host &#x3D; &#39;%&#39; where user &#x3D; &#39;root&#39;;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; select host, user from user;+-----------+------------------+| host | user |+-----------+------------------+| % | root || localhost | mysql.infoschema || localhost | mysql.session || localhost | mysql.sys |+-----------+------------------+4 rows in set (0.00 sec) % 代表任意的客户端,可替换成具体IP地址。 8.7、刷新12mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 9、其他配置9.1、设置安全选项：(重置)1mysql_secure_installation 9.2、关闭MySQL1systemctl stop mysqld 9.3、重启MySQL1systemctl restart mysqld 9.4、查看MySQL运行状态1systemctl status mysqld 9.5、设置开机启动1systemctl enable mysqld 9.6、关闭开机启动1systemctl disable mysqld 9.7、配置默认编码为utf8：1vi &#x2F;etc&#x2F;my.cnf 添加 1[mysqld] character_set_server&#x3D;utf8 init_connect&#x3D;&#39;SET NAMES utf8&#39; 其他默认配置文件路径：/etc/my.cnf日志文件：/var/log/var/log/mysqld.log服务启动脚本：/usr/lib/systemd/system/mysqld.servicesocket文件：/var/run/mysqld/mysqld.pid 9.8、查看版本1234567mysql&gt; select version();+-----------+| version() |+-----------+| 8.0.18 |+-----------+1 row in set (0.00 sec) 9.10 命令行导入数据库1mysql -h localhost -u root -p 数据库名&lt; &#x2F;home&#x2F;fps001.sql 9.11 命令行导出数据库1mysqldump -h localhost -u root -p 数据库名&gt; &#x2F;home&#x2F;fps001.sql","categories":[{"name":"mysql","slug":"mysql","permalink":"http://www.studyz.club/categories/mysql/"},{"name":"centos7-mysql8","slug":"mysql/centos7-mysql8","permalink":"http://www.studyz.club/categories/mysql/centos7-mysql8/"}],"tags":[{"name":"centos7-mysql8","slug":"centos7-mysql8","permalink":"http://www.studyz.club/tags/centos7-mysql8/"}]},{"title":"Hadoop-Hive","slug":"Hadoop-Hive","date":"2019-12-21T14:04:45.363Z","updated":"2019-12-26T02:58:14.812Z","comments":true,"path":"posts/23fb5515/","link":"","permalink":"http://www.studyz.club/posts/23fb5515/","excerpt":"","text":"一、 Hive 基本概念 1.1、 什么是 Hive 1.2、Hive 的优缺点 1.3、Hive 架构原理 1.4、 Hive和数据库比较 二、Hive安装及配置 2.1、Hive下载 2.2、解压配置 2.3、Hadoop 集群配置 2.4、配置mysql 2.5、配置 Metastore 到 MySql 三、Hive 实际操作 3.1、常用命令 3.2、将本地文件导入 Hive 案例 3.3、HiveJDBC访问 3.4、Hive常用交互命令 3.5、Hive 其他命令操作 3.6、Hive 常见属性配置 3.6.1、Hive 数据仓库位置配置 3.6.2、查询后信息显示配置 3.6.3、Hive 运行日志信息配置 3.6.4、参数配置方式 四、Hive 数据类型 4.1、基本数据类型 4.2、集合数据类型 4.3、类型转化 一、 Hive 基本概念1.1、 什么是 Hive Hive：由 Facebook 开源用于解决·海量结构化日志·的数据统计。 Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类 SQL 查询功能。(也就是说基于hadoop,本身不存储数据) 本质是：将 HQL 转化成 MapReduce 程序 HQL转换MR流程 1）Hive 处理的数据存储在 HDFS 2）Hive 分析数据底层的默认实现是 MapReduce 3）执行程序运行在Yarn上 1.2、Hive 的优缺点1.2.1 优点 操作接口采用类 SQL 语法，提供快速开发的能力（简单、容易上手）。 避免了去写 MapReduce，减少开发人员的学习成本。 Hive 的执行延迟比较高，因此 Hive 常用于数据分析，对实时性要求不高的场合。 Hive 优势在于处理大数据，对于处理小数据没有优势，因为 Hive 的执行延迟比较高。 Hive 支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。 1.2.2 缺点1．Hive 的 HQL 表达能力有限 （1）迭代式算法无法表达 （2）数据挖掘方面不擅长2．Hive 的效率比较低 （1）Hive 自动生成的 MapReduce 作业，通常情况下不够智能化 （2）Hive 调优比较困难，粒度较粗 1.3、Hive 架构原理 1．用户接口：ClientCLI（hive shell）、JDBC/ODBC(java 访问 hive)、WEBUI（浏览器访问 hive） 2．元数据：Metastore元数据包括：表名、表所属的数据库（默认是 default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等； 默认存储在自带的 derby 数据库中，推荐使用 MySQL 存储 Metastore3．Hadoop使用 HDFS 进行存储，使用 MapReduce 进行计算。 4．驱动器：Driver*（1）解析器（SQL Parser）：将 SQL 字符串转换成抽象语法树 AST，这一步一般都用第三方工具库完成，比如 antlr；对 AST 进行语法分析，比如表是否存在、字段是否存在、SQL 语义是否有误。 （2）编译器（Physical Plan）：将 AST 编译生成逻辑执行计划。 （3）优化器（Query Optimizer）：对逻辑执行计划进行优化。 （4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于 Hive 来说，就是 MR/Spark Hive 通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的 Driver，结合元数据(MetaStore)，将这些指令翻译成 MapReduce，提交到 Hadoop 中执行，最后，将执行返回的结果输出到用户交互接口。 1.4、 Hive和数据库比较 由于 Hive 采用了类似 SQL 的查询语言 HQL(Hive Query Language)，因此很容易将 Hive 理解为数据库。其实从结构上来看，Hive 和数据库除了拥有类似的查询语言，再无类似之处。本节将从多个方面来阐述 Hive 和数据库的差异。数据库可以用在 Online 的应用中，但是 Hive 是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性。 1.4.1、查询语言 由于 SQL 被广泛的应用在数据仓库中，因此，专门针对 Hive 的特性设计了类 SQL 的查询语言 HQL。熟悉 SQL 开发的开发者可以很方便的使用 Hive 进行开发。 1.4.2、数据存储位置Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中。 1.4.3、数据更新由于 Hive 是针对数据仓库应用设计的，而`数据仓库的内容是读多写少的`。因此，`Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的`。而数据库中的数据通常是需 要 经 常 进 行 修 改 的 ， 因 此 可 以 使 用 INSERT INTO … VALUES 添 加 数 据 ， 使用 UPDATE … SET 修改数据。 1.4.4、索引Hive 在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些 Key 建立索引。Hive 要访问数据中满足条件的特定值时，需要`暴力扫描整个数据`，因此访问延迟较高。由于 MapReduce 的引入， Hive 可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive 仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了 Hive 不适合在线数据查询。 1.4.5、执行Hive 中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的。而数据库通常有自己的执行引擎。 1.4.6、执行延迟Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce 框架。由于 MapReduce 本身具有较高的延迟，因此在利用 MapReduce 执行 Hive 查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive 的并行计算显然能体现出优势。 1.4.7、可扩展性由于 Hive 是建立在 Hadoop 之上的，因此 Hive 的可扩展性是和 Hadoop 的可扩展性是一致的（世界上最大的 Hadoop 集群在 Yahoo!，2009 年的规模在 4000 台节点左右）。而数据库由于 ACID 语义的严格限制，扩展行非常有限。目前最先进的并行数据库 Oracle 在理论上的扩展能力也只有 100 台左右。 1.4.8 数据规模由于 Hive 建立在集群上并可以利用 MapReduce 进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。 二、Hive安装及配置2.1、Hive下载 hive相当于一个客户端，安装在主节点master上(hive最好安装在resourcemanager上)是独立的，只用装在一台机器里就可以，如果装在三台机器，就是三台不同的客户端。 ① 由官网得到与Hadoop的兼容信息https://hive.apache.org/downloads.html由兼容信息去镜像网站下载对应的版本http://mirrors.tuna.tsinghua.edu.cn/apache/hive/文档查看地址https://cwiki.apache.org/confluence/display/Hive/GettingStarted 2.2、解压配置① 将apache-hive-2.3.6-bin.tar.gz上传到/usr/local/目录下② 解压1tar -zxvf &#x2F;usr&#x2F;local&#x2F;apache-hive-2.3.6-bin.tar.gz -C &#x2F;usr&#x2F;local&#x2F; ③ 更改目录名为hive1mv apache-hive-2.3.6-bin hive ④ 修改hive/conf/hive-env.sh.template名称为 hive-env.sh,并编辑1mv hive-env.sh.template hive-env.sh ⑤ 配置 hive-env.sh 文件（a）配置 HADOOP_HOME 路径 1export HADOOP_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3 （b）配置 HIVE_CONF_DIR 路径 1export HIVE_CONF_DIR&#x3D;&#x2F;usr&#x2F;local&#x2F;hive&#x2F;conf 2.3、Hadoop 集群配置（1）必须启动 hdfs 和 yarn在master机上 1start-all.sh 在slave2机上 1yarn-daemon.sh start resourcemanager （2）在 HDFS 上创建/tmp 和/user/hive/warehouse 两个目录并修改他们的同组权限可写(可不操作，系统会自动创建) 12hadoop fs -mkdir -p&#x2F;user&#x2F;hive&#x2F;warehouse 123hadoop fs -chmod g+w &#x2F;tmphadoop fs -chmod g+w &#x2F;user&#x2F;hive&#x2F;warehouse 2.4、配置mysql① 查看我们mysql安装的版本12[root@master bin]# mysql -Vmysql Ver 8.0.18 for Linux on x86_64 (MySQL Community Server - GPL) ② 根据我们的mysql版本去官网下载相对应的连接包12http:&#x2F;&#x2F;central.maven.org&#x2F;maven2&#x2F;mysql&#x2F;mysql-connector-java&#x2F; ③ 将连接包上传到hive/lib/下2.5、配置 Metastore 到 MySql① 1．在hive/conf 目录下创建一个 hive-site.xml12[root@master conf]# touch hive-site.xml[root@master conf]# vi hive-site.xml ② 根据官方文档配置参数，拷贝数据到 hive-site.xml 文件中https://cwiki.apache.org/confluence/display/Hive/AdminManual https://cwiki.apache.org/confluence/display/Hive/AdminManual+Configuration 123456789101112131415161718192021222324252627282930313233&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; standalone&#x3D;&quot;no&quot;?&gt;&lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt; &lt;value&gt;jdbc:mysql:&#x2F;&#x2F;master:3306&#x2F;metastore?createDatabaseIfNotExist&#x3D;true&lt;&#x2F;value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;&#x2F;description&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;&#x2F;name&gt; &lt;value&gt;com.mysql.cj.jdbc.Driver&lt;&#x2F;value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;&#x2F;description&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;&#x2F;name&gt; &lt;value&gt;root&lt;&#x2F;value&gt; &lt;description&gt;username to use against metastore database&lt;&#x2F;description&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;&#x2F;name&gt; &lt;value&gt;******&lt;&#x2F;value&gt; &lt;description&gt;password to use against metastore database&lt;&#x2F;description&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;datanucleus.schema.autoCreateAll&lt;&#x2F;name&gt; &lt;value&gt;true&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;&#x2F;configuration&gt; 三、Hive 实际操作3.1、常用命令① 启动 hive123[root@master conf]# hiveLogging initialized using configuration in jar:file:&#x2F;usr&#x2F;local&#x2F;hive&#x2F;lib&#x2F;hive-common-2.3.6.jar!&#x2F;hive-log4j2.properties Async: trueHive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases. ② 显示数据库1234hive&gt; show databases;OKdefaultTime taken: 17.07 seconds, Fetched: 1 row(s) ③ 使用 default 数据库123hive&gt; use default;OKTime taken: 0.208 seconds ④ 显示 default 数据库中的表123hive&gt; show tables;OKTime taken: 0.22 seconds ⑤ 创建表1234567hive&gt; create table student(id int,name string);OKTime taken: 15.157 secondshive&gt; show tables;OKstudentTime taken: 0.224 seconds, Fetched: 1 row(s) ⑥ 查询表123hive&gt; select * from student;OKTime taken: 2.764 seconds ⑦ 向表中插入数据(插不进去去看报错集合，是未连接上resourcemanager还是没开启本地模式 set hive.exec.mode.local.auto=true; )1234567891011121314151617181920hive&gt; insert into table student values(1,&#39;baba&#39;);Automatically selecting local only mode for queryWARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.Query ID &#x3D; root_20191223194425_4b748a12-c2e0-40f9-b7b7-11e454c1473fTotal jobs &#x3D; 3Launching Job 1 out of 3Number of reduce tasks is set to 0 since there&#39;s no reduce operatorJob running in-process (local Hadoop)2019-12-23 19:44:50,135 Stage-1 map &#x3D; 100%, reduce &#x3D; 0%Ended Job &#x3D; job_local724226221_0001Stage-4 is selected by condition resolver.Stage-3 is filtered out by condition resolver.Stage-5 is filtered out by condition resolver.Moving data to directory hdfs:&#x2F;&#x2F;mycluster&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;student&#x2F;.hive-staging_hive_2019-12-23_19-44-25_730_7752990075577229926-1&#x2F;-ext-10000Loading data to table default.studentMapReduce Jobs Launched:Stage-Stage-1: HDFS Read: 7 HDFS Write: 85 SUCCESSTotal MapReduce CPU Time Spent: 0 msecOKTime taken: 26.335 seconds ⑧ 查询表中的数据12345hive&gt; select * from student;OK1 baba2 erzaTime taken: 0.352 seconds, Fetched: 2 row(s) 1234hive&gt; select count(*) from student;OK2Time taken: 0.713 seconds, Fetched: 1 row(s) ⑨ 查看表结构12345hive&gt; desc student;OKid intname stringTime taken: 0.179 seconds, Fetched: 2 row(s) ⑩ 退出hive1hive&gt; quit; (11)删除已创建的 students 表1、仅删除表中数据，保留表结构123456hive&gt; truncate table students;OKTime taken: 8.735 secondshive&gt; select * from students;OKTime taken: 2.872 seconds truncate操作用于删除指定表中的所有行，相当于delete from table where 1=1.表达的是一个意思。 注意：truncate 不能删除外部表！因为外部表里的数据并不是存放在Hive Meta store中。创建表的时候指定了EXTERNAL，外部表在删除分区后，hdfs中的数据还存在，不会被删除。因此要想删除外部表数据，可以把外部表转成内部表或者删除hdfs文件。 2、删除表1234567hive&gt; drop table if exists students;OKTime taken: 0.7 secondshive&gt; show tables;OKstudentTime taken: 0.068 seconds, Fetched: 1 row(s) 3.2、将本地文件导入 Hive 案例① 创建数据1[root@master test]# vi stu.txt 写入如下内容:(注意这里使用tab键分割的) 12341 baba2 erza3 nver4 dog ② 上传数据1234hive&gt; load data local inpath &#39;&#x2F;root&#x2F;test&#x2F;stu.txt&#39; into table student;Loading data to table default.studentOKTime taken: 3.768 seconds 文件上传到了hdfs的/user/hive/warehouse/student目录下如果此时我们查询数据会发现: 123456789hive&gt; select * from student;OK1 baba2 erzaNULL NULLNULL NULLNULL NULLNULL NULLTime taken: 2.801 seconds, Fetched: 6 row(s) 这是由于分隔符造成的分隔符 我们新建一个指定分隔符的表 123hive&gt; create table stu(id int,name string) row format delimited fields terminated by &#39;\\t&#39;;OKTime taken: 0.546 seconds 然后再将数据写入stu表 1234567891011121314hive&gt; create table stu(id int,name string) row format delimited fields terminated by &#39;\\t&#39;;OKTime taken: 0.546 secondshive&gt; load data local inpath &#39;&#x2F;root&#x2F;test&#x2F;stu.txt&#39; into table stu;Loading data to table default.stuOKTime taken: 1.824 secondshive&gt; select * from stu;OK1 baba2 erza3 nver4 dogTime taken: 0.466 seconds, Fetched: 4 row(s) ③ 新建数据,将其追加到hdfs目录里的stu.txt我们在hdfs的webUI里可以看到刚才传数据的stu.txt的完整目录为/user/hive/warehouse/stu/stu.txt。我们将新建的stu1.txt追加到这个文件里。 新建数据 1vi &#x2F;root&#x2F;test&#x2F;stu1.txt 1235 cat6 zy7 ll 将stu1.txt上传到同一目录 1[root@master test]# hadoop fs -put stu1.txt &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;stu&#x2F; 再查询stu表 12345678910hive&gt; select * from stu;OK1 baba2 erza3 nver4 dog5 cat6 zy7 llTime taken: 0.457 seconds, Fetched: 7 row(s) 因此我们不仅可以用load将数据加载上去，也可以直接用hadoop fs -put将数据上传，格式相同会自动合并④ hdfs端加载数据到表1vi stu2.txt 128 zo9 ha 1[root@master test]# hadoop fs -put stu2.txt &#x2F; 12345678910111213141516hive&gt; load data inpath &#39;&#x2F;stu2.txt&#39; into table stu;Loading data to table default.stuOKTime taken: 1.217 secondshive&gt; select * from stu;OK1 baba2 erza3 nver4 dog5 cat6 zy7 ll8 zo9 haTime taken: 0.401 seconds, Fetched: 9 row(s) 不过需要注意的是,但我们从本地上传文件使相当于copy,原来的stu.txt还在本地,当从hdfs端上传的时候相当于移动,stu2.txt已经不再/目录下了,而是被移动到/user/hive/warehouse/stu/,其实底层也并不是移动,而是修改namenode中的元数据,将文件的位置信息改到了/user/hive/warehouse/stu/目录下,实际存储位置还是根目录下3.3、HiveJDBC访问在hadoop 的core-site.xml添加如下内容 然后重启 123456789101112&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;&#x2F;name&gt; &lt;value&gt;root&lt;&#x2F;value&gt; &lt;description&gt;Allow the superuser oozie to impersonate any members of the group group1 and group2&lt;&#x2F;description&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;&#x2F;name&gt; &lt;value&gt;*&lt;&#x2F;value&gt; &lt;description&gt;The superuser can connect only from host1 and host2 to impersonate a user&lt;&#x2F;description&gt;&lt;&#x2F;property&gt; (下面这个选配，可以先不配置试试)现在配置文件hive-site.xml里添加如下内容,用于登录的用户名和密码 12345678910&lt;property&gt; &lt;name&gt;hive.server2.thrift.client.user&lt;&#x2F;name&gt; &lt;value&gt;root&lt;&#x2F;value&gt; &lt;description&gt;Username to use against thrift client&lt;&#x2F;description&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;hive.server2.thrift.client.password&lt;&#x2F;name&gt; &lt;value&gt;******&lt;&#x2F;value&gt; &lt;description&gt;Password to use against thrift client&lt;&#x2F;description&gt; &lt;&#x2F;property&gt; 启动 hiveserver2 服务注:启动会很长时间，在这挂着就行了，这是一个阻塞进程 123456789[root@master test]# hiveserver2&amp;[2] 79231[root@master test]# 2019-12-23 22:38:54: Starting HiveServer2SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:&#x2F;usr&#x2F;local&#x2F;hive&#x2F;lib&#x2F;log4j-slf4j-impl-2.6.2.jar!&#x2F;org&#x2F;slf4j&#x2F;impl&#x2F;StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:&#x2F;usr&#x2F;local&#x2F;hbase&#x2F;lib&#x2F;slf4j-log4j12-1.7.25.jar!&#x2F;org&#x2F;slf4j&#x2F;impl&#x2F;StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:&#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;lib&#x2F;slf4j-log4j12-1.7.10.jar!&#x2F;org&#x2F;slf4j&#x2F;impl&#x2F;StaticLoggerBinder.class]SLF4J: See http:&#x2F;&#x2F;www.slf4j.org&#x2F;codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] 可用命令看端口是否正常打开，服务是否正常运行1234[root@master ~]# netstat -anp | grep 10000tcp 0 0 0.0.0.0:10000 0.0.0.0:* LISTEN 3905&#x2F;javatcp 0 0 192.168.43.159:10000 192.168.43.159:49672 ESTABLISHED 3905&#x2F;javatcp 0 0 192.168.43.159:49672 192.168.43.159:10000 ESTABLISHED 4236&#x2F;java 新开一个窗口启动 beeline123[root@master ~]# beelineBeeline version 2.3.6 by Apache Hivebeeline&gt; ① 连接1234567beeline&gt; !connect jdbc:hive2:&#x2F;&#x2F;master:10000Connecting to jdbc:hive2:&#x2F;&#x2F;master:10000Enter username for jdbc:hive2:&#x2F;&#x2F;master:10000: rootEnter password for jdbc:hive2:&#x2F;&#x2F;master:10000: (未设置密码时，可以直接回车)Connected to: Apache Hive (version 2.3.6)Driver: Hive JDBC (version 2.3.6)Transaction isolation: TRANSACTION_REPEATABLE_READ ② 查看数据库12345670: jdbc:hive2:&#x2F;&#x2F;master:10000&gt; show databases;+----------------+| database_name |+----------------+| default |+----------------+1 row selected (0.554 seconds) 当beeline输入语句，查询什么的会在刚才打开hiveserver2的窗口打印日志，包括错误信息。3.4、Hive常用交互命令① hive常用命令123456789101112131415[root@master ~]# hive -helpusage: hive -d,--define &lt;key&#x3D;value&gt; Variable substitution to apply to Hive commands. e.g. -d A&#x3D;B or --define A&#x3D;B --database &lt;databasename&gt; Specify the database to use -e &lt;quoted-query-string&gt; SQL from command line -f &lt;filename&gt; SQL from files -H,--help Print help information --hiveconf &lt;property&#x3D;value&gt; Use value for given property --hivevar &lt;key&#x3D;value&gt; Variable substitution to apply to Hive commands. e.g. --hivevar A&#x3D;B -i &lt;filename&gt; Initialization SQL file -S,--silent Silent mode in interactive shell -v,--verbose Verbose mode (echo executed SQL to the console) ② “-e”不进入 hive 的交互窗口执行 sql 语句123456[root@master ~]# hive -e &quot;show databases;&quot;Logging initialized using configuration in jar:file:&#x2F;usr&#x2F;local&#x2F;hive&#x2F;lib&#x2F;hive-common-2.3.6.jar!&#x2F;hive-log4j2.properties Async: trueOKdefaultTime taken: 31.602 seconds, Fetched: 1 row(s) ③ “-f”执行脚本中 sql 语句 在/root/test目录下创建hive.hql1show databases; 执行12345[root@master test]# hive -f &#x2F;root&#x2F;test&#x2F;hive.hqlLogging initialized using configuration in jar:file:&#x2F;usr&#x2F;local&#x2F;hive&#x2F;lib&#x2F;hive-common-2.3.6.jar!&#x2F;hive-log4j2.properties Async: trueOKdefaultTime taken: 32.177 seconds, Fetched: 1 row(s) ④ 执行文件中的 sql 语句并将结果写入文件中1hive -f &#x2F;root&#x2F;test&#x2F;hive.hql &#x2F;root&#x2F;test&#x2F;stu.txt 3.5、Hive 其他命令操作① 在 hive cli 命令窗口中如何查看 hdfs 文件系统123456789101112hive&gt; dfs -ls &#x2F;;Found 10 itemsdrwxrwxrwx - root supergroup 0 2019-12-16 12:37 &#x2F;benchmarksdrwxrwxrwx - root supergroup 0 2019-12-23 15:57 &#x2F;hbasedrwxrwxrwx - root supergroup 0 2019-12-15 23:30 &#x2F;input_fruitdrwxrwxrwx - root supergroup 0 2019-06-11 23:40 &#x2F;kcdrwxrwxrwx - root supergroup 0 2019-06-12 19:46 &#x2F;kechengshejidrwxrwxrwx - root supergroup 0 2019-06-12 14:06 &#x2F;shiyandrwxr-xr-x - root supergroup 0 2019-12-16 17:48 &#x2F;testdrwxrwxrwx - root supergroup 0 2019-12-23 16:26 &#x2F;tmpdrwxrwxrwx - root supergroup 0 2019-12-21 01:08 &#x2F;userdrwxrwxrwx - root supergroup 0 2019-12-23 16:26 &#x2F;usr ② 在 hive cli 命令窗口中如何查看本地文件系统12345hive&gt; ! ls &#x2F;root&#x2F;test;hive.hqlstu1.txtstu2.txtstu.txt ③ 查看在 hive 中输入的所有历史命令 进入到当前用户的根目录/root或者home目录下当前登陆的用户文件夹下 查看. hivehistory 文件1[root@master ~]# cat .hivehistory 3.6、Hive 常见属性配置3.6.1、Hive 数据仓库位置配置① Default 数据仓库的最原始位置是在 hdfs 上的：/user/hive/warehouse 路径下。② 在仓库目录下，没有对默认的数据库 default 创建文件夹。如果某张表属于 default数据库，直接在数据仓库目录下创建一个文件夹。③ 修改 default 数据仓库原始位置（将 hive-default.xml.template 如下配置信息拷贝到hive-site.xml 文件中）。 修不修改位置都行，看需求。 123456&lt;property&gt;&lt;name&gt;hive.metastore.warehouse.dir&lt;&#x2F;name&gt;&lt;value&gt;&#x2F;user&#x2F;hive&#x2F;warehouse&lt;&#x2F;value&gt;&lt;description&gt;location of default database for the warehouse&lt;&#x2F;description&gt;&lt;&#x2F;property&gt; 配置同组用户有执行权限 1hdfs dfs -chmod g+w &#x2F;user&#x2F;hive&#x2F;warehouse 3.6.2、查询后信息显示配置 为配置前查询 123456789101112hive&gt; select * from stu;OK1 baba2 erza3 nver4 dog5 cat6 zy7 ll8 zo9 haTime taken: 16.644 seconds, Fetched: 9 row(s) ① 在 hive-site.xml 文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。123456789&lt;property&gt; &lt;name&gt;hive.cli.print.header&lt;&#x2F;name&gt; &lt;value&gt;true&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;hive.cli.print.current.db&lt;&#x2F;name&gt; &lt;value&gt;true&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; 配置完之后再查询(添加了表的头信息,以及该表所存在与哪个数据库数据库（default）) 12345678910111213hive (default)&gt; select * from stu;OKstu.id stu.name1 baba2 erza3 nver4 dog5 cat6 zy7 ll8 zo9 haTime taken: 0.559 seconds, Fetched: 9 row(s) 3.6.3、Hive 运行日志信息配置① Hive 的 log 默认存放在/tmp/root/hive.log 目录下（当前用户名下）② 修改 hive 的 log 存放日志到/usr/local/hive/logs 修改/usr/local/hive/conf/hive-log4j2.properties.template 文件名称为hive-log4j2.properties 1[root@master conf]# mv hive-log4j2.properties.template hive-log4j2.properties 在 hive-log4j.properties 文件中修改 log 存放位置 1hive.log.dir&#x3D;&#x2F;usr&#x2F;local&#x2F;hive&#x2F;logs 3.6.4、参数配置方式① 查看当前所有的配置信息1hive (default)&gt; set; ② 参数的配置三种方式 (1)、配置文件方式默认配置文件：hive-default.xml用户自定义配置文件：hive-site.xml 注意：用户自定义配置会覆盖默认配置。另外，Hive 也会读入 Hadoop 的配置，因为 Hive 是作为 Hadoop 的客户端启动的，Hive 的配置会覆盖 Hadoop 的配置。配置文件的设定对本机启动的所有 Hive 进程都有效。 （2）、命令行参数方式 启动 Hive 时，可以在命令行添加-hiveconf param=value 来设定参数。 例如: 1234[root@master hive]# hive -hiveconf mapred.reduce.tasks&#x3D;10;Logging initialized using configuration in file:&#x2F;usr&#x2F;local&#x2F;hive&#x2F;conf&#x2F;hive-log4j2.properties Async: trueHive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.hive (default)&gt; 注意：仅对本次 hive 启动有效查看参数设置： 12hive (default)&gt; set mapred.reduce.tasks;mapred.reduce.tasks&#x3D;10 （3）、参数声明方式 可以在 HQL 中使用 SET 关键字设定参数 例如: 123hive (default)&gt; set mapred.reduce.tasks&#x3D;100;hive (default)&gt; set mapred.reduce.tasks;mapred.reduce.tasks&#x3D;100 注意：仅对本次 hive 启动有效。上述三种设定方式的优先级依次递增。即配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如 log4j 相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。四、Hive 数据类型4.1、基本数据类型 Hive 数据类型 Java 数据类型 长度 例子 TINYINT byte 1byte有符号整数 20 SMALINT short 2byte 有符号整数 20 INT int 4byte 有符号整数 20 BIGINT long 8byte 有符号整数 20 BOOLEAN boolean 布尔类型，true 或者 false TRUE FALSE FLOAT float 单精度浮点数 3.14159 DOUBLE double 双精度浮点数 3.14159 STRING string 字符系列。可以指定字符集。可以使用单引号或者双引号。 ‘now is the time’ “for all good men” TIMESTAMP 时间类型 BINARY 字节数组 对于 Hive 的 String 类型相当于数据库的 varchar 类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储 2GB 的字符数。 集合数据类型 数据类型 描述 语法示例 STRUCT 和 c 语言中的 struct 类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是 STRUCT{first STRING, last STRING},那么第 1 个元素可以通过字段.first 来引用。 struct() MAP MAP 是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是 MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素 map() ARRAY 数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第 2 个元素可以通过数组名[1]进行引用。 Array() Hive 有三种复杂数据类型 ARRAY、MAP 和 STRUCT。ARRAY 和 MAP 与 Java 中的Array 和 Map 类似，而 STRUCT 与 C 语言中的 Struct 类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。 在生产环境比较少用集合类型，因为比较复杂。可以用自定义函数将集合里的元素分割成字段。案例实操① 假设某表有如下一行，我们用 JSON 格式来表示其数据结构。在 Hive 下访问的格式为(在hive中不能识别这样格式的数据，大括号必须在一行，为看方便看才写成这样)123456789101112&#123; &quot;name&quot;: &quot;songsong&quot;, &quot;friends&quot;: [&quot;bingbing&quot; , &quot;lili&quot;] , &#x2F;&#x2F;列表 Array, &quot;children&quot;: &#123; &#x2F;&#x2F;键值 Map, &quot;xiao song&quot;: 18 , &quot;xiaoxiao song&quot;: 19 &#125; &quot;address&quot;: &#123; &#x2F;&#x2F;结构 Struct, &quot;street&quot;: &quot;hui long guan&quot; , &quot;city&quot;: &quot;beijing&quot; &#125;&#125; ② 基于上述数据结构，我们在 Hive 里创建对应的表，并导入数据。创建本地测试文件 test.txt12songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijingyangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing 注意：MAP，STRUCT 和 ARRAY 里的元素间关系都可以用同一个字符表示，这里用“_”。④ Hive 上创建测试表 test1234567891011121314hive (default)&gt; create table test( &gt; name string, &gt; friends array&lt;string&gt;, &gt; children map&lt;string, int&gt;, &gt; address struct&lt;street:string, city:string&gt; &gt; ) &gt; row format delimited &gt; fields terminated by &#39;,&#39; &gt; collection items terminated by &#39;_&#39; &gt; map keys terminated by &#39;:&#39; &gt; lines terminated by &#39;\\n&#39;;OKTime taken: 13.817 seconds 字段解释：row format delimited fields terminated by ‘,’ – 列分隔符collection items terminated by ‘_’ –MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)map keys terminated by ‘:’ – MAP 中的 key 与 value 的分隔符lines terminated by ‘\\n’; – 行分隔符 ④ 导入文本数据到测试表创建数据 12[root@master ~]# cd &#x2F;root&#x2F;test&#x2F;[root@master test]# vi ss.txt 内容为 12songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijingyangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing 上传数据 1234hive (default)&gt; load data local inpath &quot;&#x2F;root&#x2F;test&#x2F;ss.txt&quot; into table test;Loading data to table default.testOKTime taken: 2.031 seconds ⑤ 访问三种集合列里的数据，以下分别是 ARRAY，MAP，STRUCT 的访问方式123456hive (default)&gt; select * from test;OKtest.name test.friends test.children test.addresssongsong [&quot;bingbing&quot;,&quot;lili&quot;] &#123;&quot;xiao song&quot;:18,&quot;xiaoxiao song&quot;:19&#125; &#123;&quot;street&quot;:&quot;hui long guan&quot;,&quot;city&quot;:&quot;beijing&quot;&#125;yangyang [&quot;caicai&quot;,&quot;susu&quot;] &#123;&quot;xiao yang&quot;:18,&quot;xiaoxiao yang&quot;:19&#125; &#123;&quot;street&quot;:&quot;chao yang&quot;,&quot;city&quot;:&quot;beijing&quot;&#125;Time taken: 0.297 seconds, Fetched: 2 row(s) 12345hive (default)&gt; select friends[1],children[&#39;xiao song&#39;],address.city from test where name&#x3D;&quot;songsong&quot;;OK_c0 _c1 citylili 18 beijingTime taken: 0.076 seconds, Fetched: 1 row(s) 4.3、类型转化 Hive 的原子数据类型是可以进行隐式转换的，类似于 Java 的类型转换，例如某表达式使用 INT 类型，TINYINT 会自动转换为 INT 类型，但是 Hive 不会进行反向转化，例如，某表达式使用 TINYINT 类型，INT 不会自动转换为 TINYINT 类型，它会返回错误，除非使用 CAST 操作。 ① 隐式类型转换规则如下（1）任何整数类型都可以隐式地转换为一个范围更广的类型，如 TINYINT 可以转换成 INT，INT 可以转换成 BIGINT。（2）所有整数类型、FLOAT 和 STRING 类型都可以隐式地转换成 DOUBLE。（3）TINYINT、SMALLINT、INT 都可以转换为 FLOAT。（4）BOOLEAN 类型不可以转换为任何其它的类型。 ② 可以使用 CAST 操作显示进行数据类型转换 例如 CAST(‘1’ AS INT)将把字符串’1’ 转换成整数 1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。","categories":[{"name":"hadoop-hive","slug":"hadoop-hive","permalink":"http://www.studyz.club/categories/hadoop-hive/"}],"tags":[{"name":"-hive","slug":"hive","permalink":"http://www.studyz.club/tags/hive/"}]},{"title":"Hadoop-Hbase与Hive集成","slug":"Hadoop-Hbase与Hive集成","date":"2019-12-20T11:33:09.558Z","updated":"2019-12-26T02:58:14.809Z","comments":true,"path":"posts/cb00c37b/","link":"","permalink":"http://www.studyz.club/posts/cb00c37b/","excerpt":"","text":"一、 HBase 与 Hive 的对比 二、HBase 与 Hive 集成使用 2.1、Hive安装及配置 一、 HBase 与 Hive 的对比1．Hive (1) 数据仓库 Hive 的本质其实就相当于将 HDFS 中已经存储的文件在 Mysql 中做了一个双射关系，以方便使用 HQL 去管理查询。 (2) 用于数据分析、清洗 Hive 适用于离线的数据分析和清洗，延迟较高。 (3) 基于 HDFS、MapReduce Hive 存储的数据依旧在 DataNode 上，编写的 HQL 语句终将是转换为 MapReduce 代码执行。 2．HBase (1) 数据库 是一种面向列族存储的非关系型数据库。 (2) 用于存储结构化和非结构化的数据 适用于单表非关系型数据的存储，不适合做关联查询（类似 JOIN 等操作）。 (3) 基于 HDFS 数据持久化存储的体现形式是 HFile，存放于 DataNode 中，被 ResionServer 以region 的形式进行管理。 (4) 延迟较低，接入在线业务使用 面对大量的企业数据，HBase 可以直线单表大量数据的存储，同时提供了高效的数据访问速度。 二、HBase 与 Hive 集成使用2.1、Hive安装及配置 hive相当于一个客户端，安装在主节点master上 ① 由官网得到与Hadoop的兼容信息https://hive.apache.org/downloads.html由兼容信息去镜像网站下载对应的版本http://mirrors.tuna.tsinghua.edu.cn/apache/hive/ ② 安装元数据库（使用MySQL）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&gt; 安装mariadb客户端和服务器[root@master ~]# yum -y install mariadb mariadb-server &gt; 启动mysql服务[root@master ~]# systemctl start mariadb &gt; 设置开机启动(有可能要执行两次)[root@master ~]# systemctl enable mariadb&gt; 简单配置[root@master ~]# mysql_secure_installationEnter current password for root (enter for none): &lt;–初次运行直接回车OK, successfully used password, moving on...Set root password? [Y&#x2F;n] &lt;-回车设置密码New password: &lt;-设置root密码Re-enter new password: &lt;-再次确认Password updated successfully!Remove anonymous users? [Y&#x2F;n] &lt;-是否删除匿名用户，回车Disallow root login remotely? [Y&#x2F;n] &lt;-是否禁止root远程登录，回车Remove test database and access to it? [Y&#x2F;n] &lt;-是否删除test数据库，回车Reload privilege tables now? &lt;– 是否重新加载权限表，回车...Thanks for using MariaDB!&gt; 登陆mysql[root@master ~]# mysql -u root -p &gt; 查看数据库MariaDB [(none)]&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema |+--------------------+3 rows in set (0.00 sec)&gt; 创建数据库hive，保存hive元数据：MariaDB [(none)]&gt; create database hive;Query OK, 1 row affected (0.01 sec)&gt; 使操作系统用户root可以通过密码******操作数据库hive的所有表：(注意这里是master.，为什么是master.呢？因为在配置hbase的时候hosts文件不写成192.168.43.159 master. master的形式就会报错找不到主机。如果hosts文件里没有写&quot;.&quot;,则不用，要不然后面初始化数据库会失败)MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON hive.* TO root@&#39;master.&#39; IDENTIFIED BY &#39;******&#39;;flush privileges;Query OK, 0 rows affected (0.00 sec)Query OK, 0 rows affected (0.01 sec) ③ 将下载的Hive上传到集群并解压④ 修改配置文件添加12export HIVE_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;hiveexport PATH&#x3D;$PATH:$HIVE_HOME&#x2F;bin 1source &#x2F;etc&#x2F;profile ⑤ 添加mysql的JDBC驱动 不难理解，hive的元数据库就是mysql,而连接mysql需要对应的驱动jar包mysql-connector-java-5.1.28 1添加至&#x2F;usr&#x2F;local&#x2F;hive&#x2F;lib&#x2F;下 ⑥ 修改hive自身的配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#1.目录在&#x2F;usr&#x2F;local&#x2F;hive&#x2F;conf，进入confcp hive-env.sh.template hive-env.sh#在hive-env.sh中添加：export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_91export HADOOP_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3export HIVE_CONF_DIR&#x3D;&#x2F;usr&#x2F;local&#x2F;hive&#x2F;confexport HIVE_AUX_JARS_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;hive&#x2F;lib#2cp hive-default.xml.template .&#x2F;hive-site.xml(将原来的配置项全删除)#在hive-site.xml中添加：&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.local&lt;&#x2F;name&gt; &lt;value&gt;true&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;&#x2F;name&gt; &lt;value&gt;jdbc:mysql:&#x2F;&#x2F;master:3306&#x2F;hive?createDatabaseIfNotExist&#x3D;true&lt;&#x2F;value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;&#x2F;description&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;&#x2F;name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;&#x2F;value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;&#x2F;description&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;&#x2F;name&gt; &lt;value&gt;root&lt;&#x2F;value&gt; &lt;description&gt;username to use against metastore database&lt;&#x2F;description&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;&#x2F;name&gt; &lt;value&gt;******&lt;&#x2F;value&gt; &lt;description&gt;password to use against metastore database&lt;&#x2F;description&gt; &lt;&#x2F;property&gt;#若不配置以下属性，会出错。#和hive仓库存储有关，会用到这两个属性，如hive.exec.local.scratchdir中 &lt;property&gt; &lt;name&gt;system:java.io.tmpdir&lt;&#x2F;name&gt; &lt;value&gt;&#x2F;usr&#x2F;local&#x2F;hive&#x2F;tmp&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;system:user.name&lt;&#x2F;name&gt; &lt;value&gt;root&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; ⑦ 验证安装先启动hadoop和mysql，然后初始化mysql 1234567891011121314[root@master conf]# schematool -initSchema -dbType mysql SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:&#x2F;usr&#x2F;local&#x2F;hive&#x2F;lib&#x2F;log4j-slf4j-impl-2.6.2.jar!&#x2F;org&#x2F;slf4j&#x2F;impl&#x2F;StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:&#x2F;usr&#x2F;local&#x2F;hbase&#x2F;lib&#x2F;slf4j-log4j12-1.7.25.jar!&#x2F;org&#x2F;slf4j&#x2F;impl&#x2F;StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:&#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;share&#x2F;hadoop&#x2F;common&#x2F;lib&#x2F;slf4j-log4j12-1.7.10.jar!&#x2F;org&#x2F;slf4j&#x2F;impl&#x2F;StaticLoggerBinder.class]SLF4J: See http:&#x2F;&#x2F;www.slf4j.org&#x2F;codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Metastore connection URL: jdbc:mysql:&#x2F;&#x2F;master:3306&#x2F;hive?createDatabaseIfNotExist&#x3D;trueMetastore Connection Driver : com.mysql.jdbc.DriverMetastore connection User: rootStarting metastore schema initialization to 2.3.0Initialization script hive-schema-2.3.0.mysql.sqlInitialization script completedschemaTool completed ⑤ 将hbase软连接链接到hive下12345678[root@master local]# ln -s $HBASE_HOME&#x2F;lib&#x2F;hbase-common-1.4.12.jar $HIVE_HOME&#x2F;lib&#x2F;hbase-common-1.4.12.ja[root@master local]# ln -s $HBASE_HOME&#x2F;lib&#x2F;hbase-server-1.4.12.jar $HIVE_HOME&#x2F;lib&#x2F;hbase-server-1.4.12.ja[root@master local]# ln -s $HBASE_HOME&#x2F;lib&#x2F;hbase-client-1.4.12.jar $HIVE_HOME&#x2F;lib&#x2F;hbase-client-1.4.12.ja[root@master local]# ln -s $HBASE_HOME&#x2F;lib&#x2F;hbase-protocol-1.4.12.jar $HIVE_HOME&#x2F;lib&#x2F;hbase-protocol-1.4.1[root@master local]# ln -s $HBASE_HOME&#x2F;lib&#x2F;hbase-it-1.4.12.jar $HIVE_HOME&#x2F;lib&#x2F;hbase-it-1.4.12.jar[root@master local]# ln -s $HBASE_HOME&#x2F;lib&#x2F;htrace-core-3.1.0-incubating.jar $HIVE_HOME&#x2F;lib&#x2F;htrace-core-3cubating.jar[root@master local]# ln -s $HBASE_HOME&#x2F;lib&#x2F;hbase-hadoop2-compat-1.4.12.jar $HIVE_HOME&#x2F;lib&#x2F;hbase-hadoop2-1.4.12.jar[root@master local]# ln -s $HBASE_HOME&#x2F;lib&#x2F;hbase-hadoop-compat-1.4.12.jar $HIVE_HOME&#x2F;lib&#x2F;hbase-hadoop-co4.12.jar ⑥ 运行命令发现进入hive1bin&#x2F;hive ⑦ 执行命令发现报错命令: 12CREATE TABLE hive_hbase_emp_table (empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int) STORED BY &#39;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#39; WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; &#x3D; &quot;:key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:eptno&quot;) TBLPROPERTIES (&quot;hbase.table.name&quot; &#x3D; &quot;hbase_emp_table&quot;); 报错信息: 12FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient 报错原因hbase与hive不兼容⑧ 为了解决兼容性的问题需要自己编辑jar包解决兼容性问题重新编译hive-hbase-handler-2.3.6.jar在下载站下载对应版本的src源码包http://mirrors.tuna.tsinghua.edu.cn/apache/hive/ 在windows上解压打开eclipse新建一个java工程,然后再src上右键import选file syste导入刚才解压的源码包 在项目工程上右键创建一个lib文件夹存放hive/lib里的jar包(注意:要把里面非jar包的东西删除,lib包下面只能放jar包)","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/categories/hadoop/"},{"name":"hbase-Hive","slug":"hadoop/hbase-Hive","permalink":"http://www.studyz.club/categories/hadoop/hbase-Hive/"}],"tags":[{"name":"-hbase - hbase-Hive","slug":"hbase-hbase-Hive","permalink":"http://www.studyz.club/tags/hbase-hbase-Hive/"}]},{"title":"集群的时间同步问题","slug":"集群的时间同步问题","date":"2019-12-13T07:09:29.571Z","updated":"2019-12-17T15:25:43.245Z","comments":true,"path":"posts/e854b1d5/","link":"","permalink":"http://www.studyz.club/posts/e854b1d5/","excerpt":"","text":"一、利用ntp服务自动同步网络时间(需要网络) 二、局域网内选一台服务器作为时间服务器 一、自动同步网络时间(需要网络)(1)通过外网同步时间123[root@node01 ~]# ntpdate time.nist.gov# 或者[root@node01 ~]# ntpdate 192.43.244.18 美国标准技术院时间服务器：time.nist.gov（192.43.244.18）上海交通大学网络中心NTP服务器地址：ntp.sjtu.edu.cn（202.120.2.101）中国国家授时中心服务器地址：cn.pool.ntp.org（210.72.145.44） 若以上提供的网络时间服务器不可用，请自行上网寻找可用的网络时间服务器 (2)设置自动执行任务，定时更新时间① 使用命令：crontab -e1[root@node01 ~]# crontab -e ② 然后往里加入一行内容1*&#x2F;10 * * * * ntpdate 202.120.2.101 上面的配置表示，每隔十分钟从 202.120.2.101该时间服务器同步一次时间。 ③ 保存退出(3)说明以上两步操作可以让node01这个服务器每隔10分钟去指定的服务器同步时间，如果需要让集群中的所有服务器(hadoop01-hadoop04)时间同步，那么每台服务器都要做以上两步操作。 二、局域网内选一台服务器作为时间服务器 局域网内选一台服务器作为时间服务器，其他服务器从局域网内的时间服务器更新时间，同时局域网内的时间服务器向外网时间服务器同步时间 master的IP为192.168.43.158，让它作为时间服务器，192.168.43.0局域网内的所有服务器都向它同步时间，而master这台时间服务器本身，向外网时间服务器同步时间(比如中国国家授时中心服务器) (1)选择一台服务器作为NTP服务器master (2) 给局域网所有需要同步时间的服务器安装ntp服务1yum install ntp -y (3) 给所有的需要同步时间的机器(包括master)设置ntp服务开机自启动，但不要启动服务1chkconfig ntpd on (4)修改master配置文件/etc/ntp.conf1我去掉了所有的默认注释，对其中的修改写了自己的注释，没有写注释的是默认配置 (5)启动ntpd服务之前，手动同步一下时间因为ntpd服务开启之后，就不能手动同步时间了，那么为什么要先手动同步时间呢？当server(中国国家授时中心服务器)与client(master)之间的时间误差过大时（可能是1000秒），master去同步时间可能对系统和应用带来不可预知的问题，master将停止时间同步！所以如果发现master启动之后时间并不进行同步时，应该考虑到可能是时间差过大引起的，此时需要先手动进行时间同步！ (6) 启动node01的ntpd服务12[root@master &#x2F;]# &#x2F;bin&#x2F;systemctl start ntpd.service (7) 检查ntp端口是否已经开启12[root@master &#x2F;]# netstat -unlnp | grep ntpd 看到红色框中的内容，表示连接和监听已正确 这里的前4行就是我们配置的4个中国国家授时中心的服务器的信息 12345# 中国国家授时中心服务器地址server 0.cn.pool.ntp.orgserver 1.cn.pool.ntp.orgserver 2.cn.pool.ntp.orgserver 3.cn.pool.ntp.org 最后一行就是本地时间服务的信息 123# 外部时间服务器不可用时，以本地时间作为时间服务server 127.127.1.0fudge 127.127.1.0 stratum 10 下面对每个列的意义进行说明： 每个remote地址前的符号 *：响应的NTP服务器和最精确的服务器 +：响应这个查询请求的NTP服务器 blank(什么都没有)：没有响应的NTP服务器 说明 ntp服务启动后，一般需要5-10分钟左右的时候才能与外部时间服务器开始同步时间，所以需要等待几分钟才能看到正常的现象，否则你看到的是响应的NTP服务器和最精确的服务器是LOCAL(0)，最后一行前面是*符号，其他都是空白 remote 响应这个请求的NTP服务器的名称 refid NTP服务器使用的更高一级服务器的名称 st 正在响应请求的NTP服务器的级别 when 上一次成功请求之后到现在的秒数 poll 本地和远程服务器多少时间进行一次同步，单位秒，在一开始运行NTP的时候这个poll值会比较小，服务器同步的频率大，可以尽快调整到正确的时间范围，之后poll值会逐渐增大，同步的频率也就会相应减小 reach 用来测试能否和服务器连接，是一个八进制值，每成功连接一次它的值就会增加 delay 从本地机发送同步要求到ntp服务器的往返时间 offset 主机通过NTP时钟同步与所同步时间源的时间偏移量，单位为毫秒，offset越接近于0，主机和ntp服务器的时间越接近 jitter 统计了在特定个连续的连接数里offset的分布情况。简单地说这个数值的绝对值越小，主机的时间就越精确 (8) 查看master的ntp服务状态12345[root@master &#x2F;]# ntpstatsynchronised to local net (127.127.1.0) at stratum 11 time correct to within 1948 ms polling server every 64 s 同样，服务启动后需要等待5-10分钟才能看到这个正常的信息到这里，我们局域网内的时间服务器master就已经配置完毕了 (9) 配置客户端(slave1-slave3)向master同步时间① 修改每台客户端的/etc/ntp.conf配置文件文件内容如下：同样，没有写注释的都是默认的配置 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# Hosts on local network are less restricted.#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap# Use public servers from the pool.ntp.org project.# Please consider joining the pool (http:&#x2F;&#x2F;www.pool.ntp.org&#x2F;join.html).#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburst# 从master中同步时间server 192.168.43.159#broadcast 192.168.1.255 autokey # broadcast server#broadcastclient # broadcast client#broadcast 224.0.1.1 autokey # multicast server#multicastclient 224.0.1.1 # multicast client#manycastserver 239.255.254.254 # manycast server#manycastclient 239.255.254.254 autokey # manycast client# Enable public key cryptography.#crypto# 允许node01修改本地时间restrict 192.168.21.101 nomodify notrap noquery# 如果node01不可用，用本地的时间服务server 127.127.1.0fudge 127.127.1.0 stratum 10includefile &#x2F;etc&#x2F;ntp&#x2F;crypto&#x2F;pwkeys &#x2F;etc&#x2F;ntp&#x2F;keys# 同步时间后写到硬件中SYNC_HWCLOCK&#x3D;yes# Specify the key identifiers which are trusted.#trustedkey 4 8 42# Specify the key identifier to use with the ntpdc utility.#requestkey 8# Specify the key identifier to use with the ntpq utility.#controlkey 8# Enable writing of statistics records.#statistics clockstats cryptostats loopstats peerstats# Disable the monitoring facility to prevent amplification attacks using ntpdc# monlist command when default restrict does not include the noquery flag. See# CVE-2013-5211 for more details.# Note: Monitoring will not be disabled with the limited restriction flag.disable monitor~~ ② 每台客户端在启动ntpd服务之前，手动同步一下时间原因同(5) 1ntpdate -u master ③ 启动每台客户端的ntpd服务1&#x2F;bin&#x2F;systemctl start ntpd.service ④ 等待5-10分钟后，查看每个客户端的状态(10) 测试,在4台服务器同时执性date命令1date &#39;+%Y-%m-%d %H:%M:%S&#39; 说明：若以上提供的网络时间服务器不可用，请自行上网寻找可用的网络时间服务器，另外需要关闭各服务器的防火墙，才能进行时间同步","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/categories/hadoop/"}],"tags":[{"name":"集群的时间同步","slug":"集群的时间同步","permalink":"http://www.studyz.club/tags/%E9%9B%86%E7%BE%A4%E7%9A%84%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5/"}]},{"title":"Hbase各种报错解决办法","slug":"Hbase各种报错解决办法","date":"2019-12-08T13:18:49.606Z","updated":"2019-12-18T04:40:27.599Z","comments":true,"path":"posts/159c0783/","link":"","permalink":"http://www.studyz.club/posts/159c0783/","excerpt":"","text":"问题十二:大数据常见端口 问题一hbase/WALs/slave2,16020,1575783816243-splitting is non empty’: Directory is not empty1234567org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.fs.PathIsNotEmptyDirectoryException): &#96;&#x2F;hbase&#x2F;WALs&#x2F;slave2,16020,1575783816243-splitting is non empty&#39;: Directory is not empty at org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.delete(FSDirDeleteOp.java:85) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3714) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:953) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:611) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at 解决办法:进入hadoop文件系统，删除掉报错的目录或整个WALs 问题二WARN [main] fs.FileSystem: Cannot load filesystem java.util.ServiceConfigurationError:12345678910111213141516171819202019-12-08 21:22:09,679 WARN [main] fs.FileSystem: Cannot load filesystemjava.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.fs.s3a.S3AFi leSystem could not be instantiated at java.util.ServiceLoader.fail(ServiceLoader.java:232) at java.util.ServiceLoader.access$100(ServiceLoader.java:185) at java.util.ServiceLoader$LazyIterator.nextServiceCaused by: java.lang.NoClassDefFoundError: com&#x2F;amazonaws&#x2F;event&#x2F;ProgressListener at java.lang.Class.getDeclaredConstructors0(Native Method) at java.lang.Class.privateGetDeclaredConstructors(Class.java:2671) at java.lang.Class.getConstructor0(Class.java:3075) at java.lang.Class.newInstance(Class.java:412) at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380) ... 61 moreCaused by: java.lang.ClassNotFoundException: com.amazonaws.event.ProgressListener at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 66 more 重要的是下面的这两个报错，缺少aws-java-sdk-1.7.4.jar将/hadoop/share/hadoop/tools/lib/下的aws-java-sdk包引入HBASE问题三zookeeper报错123456789102019-12-13 23:26:44,562 [myid:] - ERROR [main:QuorumPeerMain@88] - Invalid config, exiting abnormallyorg.apache.zookeeper.server.quorum.QuorumPeerConfig$ConfigException: Error processing &#x2F;opt&#x2F;zookeeper&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parse(QuorumPeerConfig.java:156) at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:104) at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:81)Caused by: java.lang.IllegalArgumentException: serverid null is not a number at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parseProperties(QuorumPeerConfig.java:422) at org.apache.zookeeper.server.quorum.QuorumPeerConfig.parse(QuorumPeerConfig.java:152) ... 2 moreInvalid config, exiting abnormally myid里没有写进去问题四WARN [main] hdfs.DFSUtil: Namenode for mycluster remains unresolved for ID nn2. Check your hdfs-site.xml file to ensure namenodes are configured properly.解决办法: 配置高可用时将hbase-site.xml里的不要配置dfs.nameservices 1234&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;&#x2F;name&gt; &lt;value&gt;hdfs:&#x2F;&#x2F;mycluster:8020&#x2F;hbase&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; 改成 1234&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;&#x2F;name&gt; &lt;value&gt;hdfs:&#x2F;&#x2F;nn1:8020&#x2F;hbase&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; 问题五123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354Error: A JNI error has occurred, please check your installation and try againException in thread &quot;main&quot; java.lang.NoClassDefFoundError: org&#x2F;apache&#x2F;hadoop&#x2F;hbase&#x2F;regionserver&#x2F;MetricsRegionServerWrapper at java.lang.Class.getDeclaredMethods0(Native Method) at java.lang.Class.privateGetDeclaredMethods(Class.java:2701) at java.lang.Class.privateGetMethodRecursive(Class.java:3048) at java.lang.Class.getMethod0(Class.java:3018) at java.lang.Class.getMethod(Class.java:1784) at sun.launcher.LauncherHelper.validateMainClass(LauncherHelper.java:544) at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:526)Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrappermaster: running regionserver, logging to &#x2F;usr&#x2F;local&#x2F;hbase&#x2F;bin&#x2F;..&#x2F;logs&#x2F;hbase-root-regionserver-master.outslave2: running regionserver, logging to &#x2F;usr&#x2F;local&#x2F;hbase&#x2F;bin&#x2F;..&#x2F;logs&#x2F;hbase-root-regionserver-slave2.outslave1: running regionserver, logging to &#x2F;usr&#x2F;local&#x2F;hbase&#x2F;bin&#x2F;..&#x2F;logs&#x2F;hbase-root-regionserver-slave1.outmaster: Error: A JNI error has occurred, please check your installation and try againmaster: Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org&#x2F;apache&#x2F;hadoop&#x2F;hbase&#x2F;regionserver&#x2F;MetricsRegionServerWrappermaster: at java.lang.Class.getDeclaredMethods0(Native Method)master: at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)master: at java.lang.Class.privateGetMethodRecursive(Class.java:3048)master: at java.lang.Class.getMethod0(Class.java:3018)master: at java.lang.Class.getMethod(Class.java:1784)master: at sun.launcher.LauncherHelper.validateMainClass(LauncherHelper.java:544)master: at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:526)master: Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperslave1: Error: A JNI error has occurred, please check your installation and try againslave1: Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org&#x2F;apache&#x2F;hadoop&#x2F;hbase&#x2F;regionserver&#x2F;MetricsRegionServerWrapperslave1: at java.lang.Class.getDeclaredMethods0(Native Method)slave1: at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)slave1: at java.lang.Class.privateGetMethodRecursive(Class.java:3048)slave1: at java.lang.Class.getMethod0(Class.java:3018)slave1: at java.lang.Class.getMethod(Class.java:1784)slave1: at sun.launcher.LauncherHelper.validateMainClass(LauncherHelper.java:544)slave1: at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:526)slave1: Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperslave2: Error: A JNI error has occurred, please check your installation and try againslave2: Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org&#x2F;apache&#x2F;hadoop&#x2F;hbase&#x2F;regionserver&#x2F;MetricsRegionServerWrapperslave2: at java.lang.Class.getDeclaredMethods0(Native Method)slave2: at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)slave2: at java.lang.Class.privateGetMethodRecursive(Class.java:3048)slave2: at java.lang.Class.getMethod0(Class.java:3018)slave2: at java.lang.Class.getMethod(Class.java:1784)slave2: at sun.launcher.LauncherHelper.validateMainClass(LauncherHelper.java:544)slave2: at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:526)slave2: Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperslave1: running master, logging to &#x2F;usr&#x2F;local&#x2F;hbase&#x2F;bin&#x2F;..&#x2F;logs&#x2F;hbase-root-master-slave1.outslave1: Error: A JNI error has occurred, please check your installation and try againslave1: Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org&#x2F;apache&#x2F;hadoop&#x2F;hbase&#x2F;regionserver&#x2F;MetricsRegionServerWrapperslave1: at java.lang.Class.getDeclaredMethods0(Native Method)slave1: at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)slave1: at java.lang.Class.privateGetMethodRecursive(Class.java:3048)slave1: at java.lang.Class.getMethod0(Class.java:3018)slave1: at java.lang.Class.getMethod(Class.java:1784)slave1: at sun.launcher.LauncherHelper.validateMainClass(LauncherHelper.java:544)slave1: at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:526)slave1: Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper jar版本不对，建议用命令从hadoop里赋值hadoop的jar包到hbase里其他的不要动1find &#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F; -name hadoop-*.jar | xargs -i cp &#123;&#125; &#x2F;usr&#x2F;local&#x2F;hbase-1.3.5&#x2F;lib&#x2F; 1scp -r &#x2F;opt&#x2F;zookeeper&#x2F;zookeeper-3.4.14.jar &#x2F;usr&#x2F;local&#x2F;hbase-1.3.5&#x2F;lib&#x2F; 不要忘了还有 1aws-java-sdk-1.7.4.jar 问题六123456789101112131415161718192021222324252019-12-15 15:12:29,516 ERROR [main] master.HMasterCommandLine: Master exitingjava.lang.RuntimeException: Failed construction of Master: class org.apache.hadoop.hbase.master.HMaster. at org.apache.hadoop.hbase.master.HMaster.constructMaster(HMaster.java:2824) at org.apache.hadoop.hbase.master.HMasterCommandLine.startMaster(HMasterCommandLine.java:234) at org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:138) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:127) at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:2834)Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode &#x3D; NoNode for &#x2F;hbase&#x2F;backup-masters&#x2F;master,16000,1576393934462 at org.apache.zookeeper.KeeperException.create(KeeperException.java:114) at org.apache.zookeeper.KeeperException.create(KeeperException.java:54) at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:792) at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.createNonSequential(RecoverableZooKeeper.java:565) at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.create(RecoverableZooKeeper.java:544) at org.apache.hadoop.hbase.zookeeper.ZKUtil.createEphemeralNodeAndWatch(ZKUtil.java:984) at org.apache.hadoop.hbase.zookeeper.MasterAddressTracker.setMasterAddress(MasterAddressTracker.java:211) at org.apache.hadoop.hbase.master.HMaster.startActiveMasterManager(HMaster.java:2019) at org.apache.hadoop.hbase.master.HMaster.&lt;init&gt;(HMaster.java:508) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.hadoop.hbase.master.HMaster.constructMaster(HMaster.java:2817) ... 5 more zookeeper版本不一致引起的，我有两个节点的zookeeper的jar包没有传到hbase/lib里。问题七123456789102019-12-15 16:29:15,464 INFO [master:16000.activeMasterManager] zookeeper.MetaTableLocator: Failed verification of hbase:meta,,1 at address&#x3D;slave1,16020,1576396609211, exception&#x3D;org.apache.hadoop.hbase.NotServingRegionException: Region hbase:meta,,1 is not online on slave1,16020,1576398535132 at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.java:3072) at org.apache.hadoop.hbase.regionserver.RSRpcServices.getRegion(RSRpcServices.java:1271) at org.apache.hadoop.hbase.regionserver.RSRpcServices.getRegionInfo(RSRpcServices.java:1557) at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$2.callBlockingMethod(AdminProtos.java:22731) at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2399) at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:124) at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:311) at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:291) 将WALs里报错的东西删除然后重启问题八1234567891011121314151617181920212223242526272829303132333435363738394041424344452019-12-15 20:47:16,497 INFO [regionserver&#x2F;slave2&#x2F;192.168.43.161:16020] hfile.CacheConfig: Created cacheConfig: blockCache&#x3D;LruBlockCache &#123;blockCount&#x3D;0, currentSize&#x3D;73480, freeSize&#x3D;98885880, maxSize&#x3D;98959360, heapSize&#x3D;73480, minSize&#x3D;94011392, minFactor&#x3D;0.95, multiSize&#x3D;470056 96, multiFactor&#x3D;0.5, singleSize&#x3D;23502848, singleFactor&#x3D;0.25&#125;, cacheDataOnRead&#x3D;true, cacheDataOnWrite&#x3D;false, cacheIndexesOnWrite&#x3D;false, ca cheBloomsOnWrite&#x3D;false, cacheEvictOnClose&#x3D;false, cacheDataCompressed&#x3D;false, prefetchOnOpen&#x3D;false2019-12-15 20:47:16,606 INFO [regionserver&#x2F;slave2&#x2F;192.168.43.161:16020] regionserver.HRegionServer: STOPPED: Failed initialization2019-12-15 20:47:16,607 ERROR [regionserver&#x2F;slave2&#x2F;192.168.43.161:16020] regionserver.HRegionServer: Failed initjava.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: &quot;master&quot;:8020; java.net.UnknownHostExcep tion; For more details see: http:&#x2F;&#x2F;wiki.apache.org&#x2F;hadoop&#x2F;UnknownHost at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792) at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:744) at org.apache.hadoop.ipc.Client$Connection.&lt;init&gt;(Client.java:409) at org.apache.hadoop.ipc.Client.getConnection(Client.java:1518) at org.apache.hadoop.ipc.Client.call(Client.java:1451) at org.apache.hadoop.ipc.Client.call(Client.java:1412) at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229) at com.sun.proxy.$Proxy16.getFileInfo(Unknown Source) at Caused by: java.net.UnknownHostException ... 31 more2019-12-15 20:47:16,613 FATAL [regionserver&#x2F;slave2&#x2F;192.168.43.161:16020] regionserver.HRegionServer: ABORTING region server slave2,16020, 1576414019032: Unhandled: Invalid host name: local host is: (unknown); destination host is: &quot;master&quot;:8020; java.net.UnknownHostException; For more details see: http:&#x2F;&#x2F;wiki.apache.org&#x2F;hadoop&#x2F;UnknownHostjava.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: &quot;master&quot;:8020; java.net.UnknownHostExcep tion; For more details see: http:&#x2F;&#x2F;wiki.apache.org&#x2F;hadoop&#x2F;UnknownHost at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792) at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:744) at org.apache.hadoop.ipc.Client$Connection.&lt;init&gt;(Client.java:409) at org.apache.hadoop.ipc.Client.getConnection(Client.java:1518) at org.apache.hadoop.ipc.Client.call(Client.java:1451) at org.apache.hadoop.ipc.Client.call(Client.java:1412) at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229) at com.sun.proxy.$Proxy16.getFileInfo(Unknown Source) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102) at com.sun.proxy.$Proxy17.getFileInfo(Unknown Source) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)Caused by: java.net.UnknownHostException ... 31 more 这个问题官方给了几种可能，上面报错信息里有链接 这里说一下我的错误的原因:① 因为之前使用的是桥接模式，后来转成了NAT模式，配置文件/etc/sysconfig/network-scripts/ifcfg-ens33里的DNS要改成和NAT所使用的的网卡一个DNS② 在vi /etc/hosts文件里写上主机名 1234192.168.43.159 master. master192.168.43.160 slave1. slave1192.168.43.161 slave2. slave2192.168.43.162 slave3. slave3 这里是ip地址 主机名(后面要加个点) hadoop设置的域名 问题九12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849502019-11-16 13:55:22,002 WARN org.apache.hadoop.ha.FailoverController: Unable to gracefully make NameNode at master&#x2F;192.168.43.159:8020 standby (unable to connect)java.net.ConnectException: Call From slave1&#x2F;192.168.43.160 to master:8020 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see: http:&#x2F;&#x2F;wiki.apache.org&#x2F;hadoop&#x2F;ConnectionRefused at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792) at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732) at org.apache.hadoop.ipc.Client.call(Client.java:1479) at org.apache.hadoop.ipc.Client.call(Client.java:1412) at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229) at com.sun.proxy.$Proxy9.transitionToStandby(Unknown Source) at org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB.transitionToStandby(HAServiceProtocolClientSideTranslatorPB.java:112) at org.apache.hadoop.ha.FailoverController.tryGracefulFence(FailoverController.java:172) at org.apache.hadoop.ha.ZKFailoverController.doFence(ZKFailoverController.java:514) at org.apache.hadoop.ha.ZKFailoverController.fenceOldActive(ZKFailoverController.java:505) at org.apache.hadoop.ha.ZKFailoverController.access$1100(ZKFailoverController.java:61) at org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks.fenceOldActive(ZKFailoverController.java:892) at org.apache.hadoop.ha.ActiveStandbyElector.fenceOldActive(ActiveStandbyElector.java:910) at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:809) at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:418) at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599) at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)Caused by: java.net.ConnectException: 拒绝连接 at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)2019-12-14 20:49:25,491 FATAL org.apache.hadoop.hdfs.tools.DFSZKFailoverController: Got a fatal error, exiting nowjava.lang.IllegalArgumentException: Missing required configuration &#39;ha.zookeeper.quorum&#39; for ZooKeeper quorum at com.google.common.base.Preconditions.checkArgument(Preconditions.java:115) at org.apache.hadoop.ha.ZKFailoverController.initZK(ZKFailoverController.java:341) at org.apache.hadoop.ha.ZKFailoverController.doRun(ZKFailoverController.java:191) at org.apache.hadoop.ha.ZKFailoverController.access$000(ZKFailoverController.java:61) at org.apache.hadoop.ha.ZKFailoverController$1.run(ZKFailoverController.java:172) at org.apache.hadoop.ha.ZKFailoverController$1.run(ZKFailoverController.java:168) at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415) at org.apache.hadoop.ha.ZKFailoverController.run(ZKFailoverController.java:168) at org.apache.hadoop.hdfs.tools.DFSZKFailoverController.main(DFSZKFailoverController.java:181)2019-12-14 21:51:13,847 INFO org.apache.hadoop.hdfs.tools.DFSZKFailoverController: Failover controller configured for NameNode NameNode at slave1&#x2F;192.168.43.160:80202019-12-14 21:51:14,646 FATAL org.apache.hadoop.hdfs.tools.DFSZKFailoverController: Got a fatal error, exiting nowjava.lang.IllegalArgumentException: Missing required configuration &#39;ha.zookeeper.quorum&#39; for ZooKeeper quorum at com.google.common.base.Preconditions.checkArgument(Preconditions.java:115) at org.apache.hadoop.ha.ZKFailoverController.initZK(ZKFailoverController.java:341) at org.apache.hadoop.ha.ZKFailoverController.doRun(ZKFailoverController.java:191) at org.apache.hadoop.ha.ZKFailoverController.access$000(ZKFailoverController.java:61) at org.apache.hadoop.ha.ZKFailoverController$1.run(ZKFailoverController.java:172) at org.apache.hadoop.ha.ZKFailoverController$1.run(ZKFailoverController.java:168) at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:415) at org.apache.hadoop.ha.ZKFailoverController.run(ZKFailoverController.java:168) at org.apache.hadoop.hdfs.tools.DFSZKFailoverController.main(DFSZKFailoverController.java:181) 确定core-site.xml里写了zookeeper的信息1234&lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;&#x2F;name&gt; &lt;value&gt;master:2181,slave1:2181,slave2:2181&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; 问题十 问题十一123456789101112131415161718192021Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org&#x2F;apache&#x2F;hadoop&#x2F;hbase&#x2F;filter&#x2F;Filter at java.lang.Class.getDeclaredMethods0(Native Method) at java.lang.Class.privateGetDeclaredMethods(Class.java:2701) at java.lang.Class.privateGetMethodRecursive(Class.java:3048) at java.lang.Class.getMethod0(Class.java:3018) at java.lang.Class.getMethod(Class.java:1784) at org.apache.hadoop.util.ProgramDriver$ProgramDescription.&lt;init&gt;(ProgramDriver.java:59) at org.apache.hadoop.util.ProgramDriver.addClass(ProgramDriver.java:103) at org.apache.hadoop.hbase.mapreduce.Driver.main(Driver.java:42) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136)Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.filter.Filter at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 14 more 在hadoop下没有相关HBase的支持，缺少hbase相关的jar包问题十二:大数据常见端口常见端口汇总： Hadoop： 50070：HDFS WEB UI端口 8020 ： 高可用的HDFS RPC端口 9000 ： 非高可用的HDFS RPC端口 8088 ： Yarn 的WEB UI 接口 8485 ： JournalNode 的RPC端口 8019 ： ZKFC端口 19888：jobhistory WEB UI端口 Zookeeper: 2181 ： 客户端连接zookeeper的端口 2888 ： zookeeper集群内通讯使用，Leader监听此端口 3888 ： zookeeper端口 用于选举leader Hbase: 60010：Hbase的master的WEB UI端口 （旧的） 新的是16010 60030：Hbase的regionServer的WEB UI 管理端口 Hive: 9083 : metastore服务默认监听端口 10000：Hive 的JDBC端口 Spark： 7077 ： spark 的master与worker进行通讯的端口 standalone集群提交Application的端口 8080 ： master的WEB UI端口 资源调度 8081 ： worker的WEB UI 端口 资源调度 4040 ： Driver的WEB UI 端口 任务调度 18080：Spark History Server的WEB UI 端口 Kafka： 9092： Kafka集群节点之间通信的RPC端口 Redis： 6379： Redis服务端口 CDH： 7180： Cloudera Manager WebUI端口 7182： Cloudera Manager Server 与 Agent 通讯端口 HUE： 8888： Hue WebUI 端口 问题十三12345678910111213141519&#x2F;12&#x2F;16 16:09:38 INFO retry.RetryInvocationHandler: Exception while invoking getNewApplication of class Application ClientProtocolPBClientImpl over rm2 after 1 fail over attempts. Trying to fail over after sleeping for 16425ms.java.net.ConnectException: Call From master&#x2F;192.168.43.159 to slave2:8032 failed on connection exception: java.net.C onnectException: 拒绝连接; For more details see: http:&#x2F;&#x2F;wiki.apache.org&#x2F;hadoop&#x2F;ConnectionRefused at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792) at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732) at org.apache.hadoop.ipc.Client.call(Client.java:1479) at org.apache.hadoop.ipc.Client.call(Client.java:1412) at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229) at com.sun.proxy.$Proxy13.getNewApplication(Unknown Source) at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getNewApplication(Applica tionClientProtocolPBClientImpl.java:221) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 注意看8032端口，未启动resourcemanager问题十四12345678910111213141516171819Map-Reduce Framework Map input records&#x3D;3 Map output records&#x3D;0 Input split bytes&#x3D;105 Spilled Records&#x3D;0 Failed Shuffles&#x3D;0 Merged Map outputs&#x3D;0 GC time elapsed (ms)&#x3D;100 CPU time spent (ms)&#x3D;1350 Physical memory (bytes) snapshot&#x3D;113160192 Virtual memory (bytes) snapshot&#x3D;2111504384 Total committed heap usage (bytes)&#x3D;30474240 ImportTsv Bad Lines&#x3D;3 File Input Format Counters Bytes Read&#x3D;54 File Output Format Counters Bytes Written&#x3D;0 这个本身是没什么问题的 Bad Lines=3是说有三行坏数据问题十五卡在 1map 100% reduce 0% 调大了虚拟机的内存，并且在yarn-site.xml里指定了虚拟内存以及可以申请的内存等123456789101112&lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;&#x2F;name&gt; &lt;value&gt;2048&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;&#x2F;name&gt; &lt;value&gt;2048&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;&#x2F;name&gt; &lt;value&gt;2.1&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; 指定resourcemanager运行的节点 1234&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;&#x2F;name&gt; &lt;value&gt;master&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; 问题十六用create创建表提示表已存在，但是用list展示表时却不存在造成问题的原因：之前使用的是hbase1.3.5后来换成了1.4.12版本，无法创建的表是hbase1.3.5时创建的，但是表名存在在了zookeeper里。解决办法：删除zookeeper里的表名 ① 启动zkCli.sh服务 ② ls /hbase/table-lock查看已经建立的表。rmr /hbase/table-lock/[tableName] ③ ls /hbase/table查看已经建立的表。rmr /hbase/table/[tableName]","categories":[{"name":"hbase-","slug":"hbase","permalink":"http://www.studyz.club/categories/hbase/"}],"tags":[{"name":"-hbase","slug":"hbase","permalink":"http://www.studyz.club/tags/hbase/"}]},{"title":"Hadoop-Hbase-MapReduce整合","slug":"Hadoop-Hbase-MapReduce整合","date":"2019-12-08T12:58:14.327Z","updated":"2019-12-20T09:22:24.519Z","comments":true,"path":"posts/92f7106e/","link":"","permalink":"http://www.studyz.club/posts/92f7106e/","excerpt":"","text":"一、HBase-MapReduc配置 二、运行官方的 MapReduce 任务 三、自定义MapReduce 3.1、自定义编写MapReduce实现数据写流程 3.2、在本地实现将一个表的数据写到另一个表 一、HBase-MapReduce配置 通过 HBase 的相关 JavaAPI，我们可以实现伴随 HBase 操作的 MapReduce 过程，比如使用MapReduce 将数据从本地文件系统导入到 HBase 的表中，比如我们从 HBase 中读取一些原 始数据后使用 MapReduce 做数据分析。 1、查看 HBase 的 MapReduce 任务的执行1[root@slave1 hbase-1.3.5]# &#x2F;bin&#x2F;hbase mapredcp 2、环境变量的导入（1）执行环境变量的导入（临时生效，在命令行执行下述操作）123$ export HBASE_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hbase$ export HADOOP_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2$ export HADOOP_CLASSPATH&#x3D;&#96;$&#123;HBASE_HOME&#125;&#x2F;bin&#x2F;hbase mapredcp&#96; （2）永久生效：在/etc/profile 配置(选择这种)12345678910export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_91export JRE_HOME&#x3D;$JAVA_HOME&#x2F;jreexport PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin:$JRE_HOME&#x2F;binexport CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar:$JRE_HOME&#x2F;libexport ZOOKEEPER_HOME&#x3D;&#x2F;opt&#x2F;zookeeperexport PATH&#x3D;$PATH:&#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;sbin:$ZOOKEEPER_HOME&#x2F;bin:PATHexport HBASE_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;hbase-1.3.5export PATH&#x3D;$PATH:$HBASE_HOME&#x2F;bin:&#x2F;$HBASE_HOME&#x2F;sbin 并在 hadoop-env.sh 中配置：（注意：在 for 循环之后配，done后面）1export HADOOP_CLASSPATH&#x3D;$HADOOP_CLASSPATH:&#x2F;usr&#x2F;local&#x2F;hbase&#x2F;lib&#x2F;* 123456789# Extra Java CLASSPATH elements. Automatically insert capacity-scheduler.for f in $HADOOP_HOME&#x2F;contrib&#x2F;capacity-scheduler&#x2F;*.jar; do if [ &quot;$HADOOP_CLASSPATH&quot; ]; then export HADOOP_CLASSPATH&#x3D;$HADOOP_CLASSPATH:$f else export HADOOP_CLASSPATH&#x3D;$f fidoneexport HADOOP_CLASSPATH&#x3D;$HADOOP_CLASSPATH:&#x2F;usr&#x2F;local&#x2F;hbase&#x2F;lib&#x2F;* 分发12[root@master hadoop]# scp -r hadoop-env.sh slave1:&#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;etc&#x2F;hadoop&#x2F; 二、运行官方的 MapReduce 任务案例一：统计 pp 表中有多少行数据1yarn jar &#x2F;usr&#x2F;local&#x2F;hbase&#x2F;lib&#x2F;hbase-server-1.4.12.jar rowcounter pp 案例二：使用 MapReduce 将本地数据导入到 HBase①在本地创建一个 tsv 格式的文件：fruit.tsv(用空格隔开)②创建 Hbase 表1Hbase(main):001:0&gt; create &#39;fruit&#39;,&#39;info&#39; ③在 HDFS 中创建 input_fruit 文件夹并上传 fruit.tsv 文件1234vi fruit.tsv1001 Apple Red1002 Pear Yellow1003 Pineapple Yellow 12hadoop fs -mkdir &#x2F;input_fruit&#x2F;hadoop fs -put fruit.tsv &#x2F;input_fruit&#x2F; ④执行 MapReduce 到 HBase 的 fruit 表中12yarn jar &#x2F;usr&#x2F;local&#x2F;hbase&#x2F;lib&#x2F;hbase-server-1.4.12.jar importtsv -Dimporttsv.columns&#x3D;HBASE_ROW_KEY,info:name,info:color -Dimporttsv.separator&#x3D;&quot; &quot; fruit hdfs:&#x2F;&#x2F;master:8020&#x2F;input_fruit fruit是指定输出的表名 HBase中的importtsv类来处理。Importtsv作业接收下面几个参数： -Dimporttsv.skip.bad.lines=false 若遇到无效行则失败 -Dimporttsv.separator=&quot; &quot; 文件中代替tabs的分隔符空格就用” “,逗号就直接用=,就行不写这个可能无法正确读取数据，一定要注意 -Dimporttsv.columns=HBASE_ROW_KEY,info:name,info:color 输入行的格式，第一列是rowKey,第二列是name,第三列是color -Dimporttsv.timestamp=currentTimeAsLong 导入时使用指定的时间戳 -Dimporttsv.mapper.class=my.Mapper 使用用户指定的Mapper类来代替默认的 -Dimporttsv.bulk.output=/user/yarn/output 作业的输出目录 ⑤ 使用 scan 命令查看导入后的结果 三、自定义MapReduce3.1、自定义编写MapReduce实现数据写流程① FruitMapper.java1234567891011121314151617181920212223package com.lizhi.mr1;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.IOException;&#x2F;** * 将数据读进来 *&#x2F;public class FruitMapper extends Mapper&lt;LongWritable, Text,LongWritable, Text&gt; &#123; &#x2F;** *map方法，打出map会自动提示 *&#x2F; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; context.write(key,value); &#125;&#125; FruitDriver.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package com.lizhi.mr1;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;&#x2F;** * 写完implements Tool之后提示 选implements *&#x2F;public class FruitDriver implements Tool &#123; &#x2F;&#x2F;定义一个Configuration private Configuration configuration &#x3D; null; @Override public int run(String[] args) throws Exception &#123; &#x2F;&#x2F;1、获取Jpb对象 Job job &#x3D; Job.getInstance(configuration); &#x2F;&#x2F;2、设置驱动类路径 job.setJarByClass(FruitDriver.class); &#x2F;&#x2F;3、设置Mapper和Mapper输出的KV类型 job.setMapperClass(FruitMapper.class); job.setMapOutputKeyClass(LongWritable.class); job.setMapOutputValueClass(Text.class); &#x2F;&#x2F;4、设置Reduce类 &#x2F;&#x2F;第二个参数表名args[1] TableMapReduceUtil.initTableReducerJob(args[1],FruitReduce.class,job); &#x2F;&#x2F;5、设置输入参数(org.apache.hadoop.mapreduce.lib.input.FileInputFormat;) &#x2F;&#x2F;第一个参数，输入参数表名、args[0] FileInputFormat.setInputPaths(job,new Path(args[0])); &#x2F;&#x2F;6、提交任务 boolean result &#x3D; job.waitForCompletion(true); return result ? 0 : 1; &#125; @Override public void setConf(Configuration conf) &#123; configuration &#x3D; conf; &#125; @Override public Configuration getConf() &#123; return configuration; &#125; public static void main(String[] args) &#123; try &#123; Configuration configuration &#x3D; new Configuration(); int run &#x3D; ToolRunner.run(configuration, new FruitDriver(), args); System.exit(run); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; FruitReduce.java1234567891011121314151617181920212223242526272829303132333435363738package com.lizhi.mr1;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.mapreduce.TableReducer;import org.apache.hadoop.hbase.util.Bytes;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import java.io.IOException;&#x2F;** * 将数据写到HBASE表里 *&#x2F;public class FruitReduce extends TableReducer&lt;LongWritable,Text,NullWritable&gt; &#123; @Override protected void reduce(LongWritable key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; &#x2F;&#x2F;1、遍历values ：1001 Apple Red for (Text value : values)&#123; &#x2F;&#x2F;2、获取每一行数据 元数据中我们使用空格区分数据的所以这里split(&quot; &quot;) String[] fields&#x3D;value.toString().split(&quot; &quot;); &#x2F;&#x2F;3、构建put对象 Put put &#x3D; new Put(Bytes.toBytes(fields[0])); &#x2F;&#x2F;4、给Put对象赋值(对应于我们数据中的1001 Apple Red)) put.addColumn(Bytes.toBytes(&quot;info&quot;),Bytes.toBytes(&quot;name&quot;),Bytes.toBytes(fields[1])); put.addColumn(Bytes.toBytes(&quot;info&quot;),Bytes.toBytes(&quot;color&quot;),Bytes.toBytes(fields[2])); &#x2F;&#x2F;5、写出到表 context.write(NullWritable.get(),put); &#125; &#125;&#125; ④将编写好的类导出成jar包,并上传到集群中导出后的jar包在target ⑤运行12yarn jar hbase-demo-1.0-SNAPSHOT.jar com.lizhi.mr1.FruitDriver &#x2F;input_fruit&#x2F;fruit.tsv fruit1 注意* 在运行前应当先创建fruit1表 1create &#39;fruit1&#39;,&#39;info&#39; * 运行报错 12345678910111213141516171819202122232425262728293031323334353619&#x2F;12&#x2F;18 20:03:17 INFO mapreduce.Job: Running job: job_1576663323001_000119&#x2F;12&#x2F;18 20:04:09 INFO mapreduce.Job: Job job_1576663323001_0001 running in uber mode : false19&#x2F;12&#x2F;18 20:04:09 INFO mapreduce.Job: map 0% reduce 0%19&#x2F;12&#x2F;18 20:04:48 INFO mapreduce.Job: map 100% reduce 0%19&#x2F;12&#x2F;18 20:05:07 INFO mapreduce.Job: map 100% reduce 67%19&#x2F;12&#x2F;18 20:05:21 INFO mapreduce.Job: Task Id : attempt_1576663323001_0001_r_000000_0, Status : FAILEDError: java.lang.ArrayIndexOutOfBoundsException: 1 at com.lizhi.mr1.FruitReduce.reduce(FruitReduce.java:30) at com.lizhi.mr1.FruitReduce.reduce(FruitReduce.java:16) at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171) at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)19&#x2F;12&#x2F;18 20:05:22 INFO mapreduce.Job: map 100% reduce 0%19&#x2F;12&#x2F;18 20:05:56 INFO mapreduce.Job: Task Id : attempt_1576663323001_0001_r_000000_1, Status : FAILEDError: java.lang.ArrayIndexOutOfBoundsException: 1 at com.lizhi.mr1.FruitReduce.reduce(FruitReduce.java:30) at com.lizhi.mr1.FruitReduce.reduce(FruitReduce.java:16) at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171) at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)Container killed by the ApplicationMaster.Container killed on request. Exit code is 143Container exited with a non-zero exit code 143 报错原因是在FruitReduce中读取fruit.tsv数据的切片不对，是以空格分开的3.2、在本地实现将一个表的数据写到另一个表将fruit表的数据写入fruit2表Fruit2Mapper.java1234567891011121314151617181920212223242526272829303132333435package com.lizhi.mr2;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.CellUtil;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableMapper;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;public class Fruit2Mapper extends TableMapper&lt;ImmutableBytesWritable, Put&gt; &#123; @Override protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException &#123; &#x2F;&#x2F;构建Put对象 Put put &#x3D; new Put(key.get()); &#x2F;&#x2F;1、获取数据 for (Cell cell : value.rawCells()) &#123; &#x2F;&#x2F;2、判断当前的cell是否为“name”列 if (&quot;name&quot;.equals(Bytes.toString(CellUtil.cloneQualifier(cell))))&#123; &#x2F;&#x2F;3、给Put对象赋值 put.add(cell); &#125; &#125; &#x2F;&#x2F;4、写出 context.write(key,put); &#125;&#125; Fruit2Driver.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package com.lizhi.mr2;import com.lizhi.mr1.FruitDriver;import com.lizhi.test.testAPI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.client.Scan;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;import static com.lizhi.test.testAPI.configuration;public class Fruit2Driver implements Tool &#123; &#x2F;&#x2F;定义配置信息 private Configuration conf &#x3D; null; @Override public int run(String[] args) throws Exception &#123; &#x2F;&#x2F;1、获取Job对象 Job job &#x3D; Job.getInstance(conf); &#x2F;&#x2F;2、设置主类路径 job.setJarByClass(Fruit2Driver.class); &#x2F;&#x2F;3、设置Mapper和输出KV类型 TableMapReduceUtil.initTableMapperJob(&quot;fruit&quot;, new Scan(), Fruit2Mapper.class , ImmutableBytesWritable.class, Put.class, job); &#x2F;&#x2F;4、设置Reduce和输出的表 TableMapReduceUtil.initTableReducerJob(&quot;fruit2&quot;, Fruit2Reducer.class, job); &#x2F;&#x2F;5、提交任务 boolean result &#x3D; job.waitForCompletion(true); return result ? 0 : 1; &#125; @Override public void setConf(Configuration configuration) &#123; conf &#x3D; configuration; &#125; @Override public Configuration getConf() &#123; return conf; &#125; public static void main(String[] args) &#123; try &#123;&#x2F;&#x2F; Configuration configuration &#x3D; new Configuration(); Configuration configuration &#x3D; HBaseConfiguration.create(); ToolRunner.run(configuration,new Fruit2Driver(),args); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; Fruit2Reducer.java1234567891011121314151617181920package com.lizhi.mr2;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableReducer;import org.apache.hadoop.io.NullWritable;import java.io.IOException;public class Fruit2Reducer extends TableReducer&lt;ImmutableBytesWritable, Put, NullWritable&gt; &#123; @Override protected void reduce(ImmutableBytesWritable key, Iterable&lt;Put&gt; values, Context context) throws IOException, InterruptedException &#123; &#x2F;&#x2F;遍历写出 for (Put value : values) &#123; context.write(NullWritable.get(),value); &#125; &#125;&#125; 注意要将hbase的配置文件hbase-site.xml上传到resources中 集群里用的是mycluster，但是windows的hosts里没有mycluster的映射，因此这里改成namenode的名称(已在hosts文件里映射)。","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/categories/hadoop/"},{"name":"hbase-MapReduce","slug":"hadoop/hbase-MapReduce","permalink":"http://www.studyz.club/categories/hadoop/hbase-MapReduce/"}],"tags":[{"name":"-hbase - hbase-Mapreduce","slug":"hbase-hbase-Mapreduce","permalink":"http://www.studyz.club/tags/hbase-hbase-Mapreduce/"}]},{"title":"Hadoop-HbaseAPI","slug":"Hadoop-HbaseAPI","date":"2019-11-30T06:44:24.180Z","updated":"2019-12-08T12:44:34.612Z","comments":true,"path":"posts/42b49db3/","link":"","permalink":"http://www.studyz.club/posts/42b49db3/","excerpt":"","text":"一、环境搭建 1.2、添加Maven依赖 二、编写HBaseAPI 2.1、判断表是否存在 2.2、创建表 2.3、删除表 2.4、创建命名空间 2.5、向表插入数据 2.6、获取数据（get） 2.7、扫描全表 三、报错信息 四、HbaseAPI测试实例源码 一、环境搭建1.2、添加Maven依赖新建Maven项目后在pom.xml中添加依赖12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;&#x2F;groupId&gt; &lt;artifactId&gt;hbase-server&lt;&#x2F;artifactId&gt; &lt;version&gt;1.3.5&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;&#x2F;groupId&gt; &lt;artifactId&gt;hbase-client&lt;&#x2F;artifactId&gt; &lt;version&gt;1.3.5&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; 二、编写HBaseAPI 注意，这部分的学习内容，我们先学习使用老版本的API，接着再写出新版本的API调用方式。 因为在企业中，有些时候我们需要一些过时的API来提供更好的兼容性。 基本概念 java类 对应数据模型 HBaseConfiguration HBase配置类 HBaseAdmin HBase管理Admin类 Table HBase Table操作类 Put HBase添加操作数据模型 Get HBase单个查询操作数据模型 Scan HBase Scan检索操作数据模型 Result HBase单个查询的结果模型 ResultScanner HBase检索结果模型 2.1、判断表是否存在 将重复的代码写到静态代码块方便调用 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586package com.lizhi.test;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.Admin;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.ConnectionFactory;import java.io.IOException;&#x2F;** * DDL: * 1、判断表是否存在 * 2、创建表 * 3、创建命名空间 * 4、删除表 * * DDM: * 5、插入数据 * 6、查数据(get) * 7、查数据(scan) * 8、删除数据 * *&#x2F;public class testAPI &#123; private static Connection connection &#x3D; null; private static Admin admin &#x3D; null; static &#123; try &#123; &#x2F;&#x2F;① 获取配置文件信息 Configuration configuration &#x3D; HBaseConfiguration.create(); configuration.set(&quot;hbase.zookeeper.quorum&quot;,&quot;master&quot;); configuration.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;); &#x2F;&#x2F;② 创建连接对象 connection &#x3D; ConnectionFactory.createConnection(configuration); &#x2F;&#x2F;③ 创建admin对象 admin &#x3D; connection.getAdmin(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public static void close()&#123; if (admin!&#x3D;null)&#123; try &#123; admin.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (connection!&#x3D;null)&#123; try &#123; connection.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#x2F;** * 1、判断表是否存在 * @param *&#x2F; public static boolean isTableExist(String tableNme) throws IOException &#123; &#x2F;&#x2F;③ 判断表是否存在 boolean exists &#x3D; admin.tableExists(TableName.valueOf(tableNme)); &#x2F;&#x2F;④ 关闭连接 &#x2F;&#x2F;admin.close(); &#x2F;&#x2F;⑤ 返回结果 return exists; &#125; public static void main(String[] args) throws IOException &#123; &#x2F;&#x2F;1、测试表是否存在 System.out.println(isTableExist(&quot;student&quot;)); &#x2F;&#x2F;关闭资源 close(); &#125;&#125; 2.2、创建表12345678910111213141516171819202122232425262728293031323334353637&#x2F;** * 2、创建表 * @param * @throws IOException * 注: String... :是可变形参，由于列族可以有多个所以用可变形参 *&#x2F; public static void creatTable(String tableName,String... cfs) throws IOException &#123; &#x2F;&#x2F;① 判断是否存在列族 if (cfs.length&lt;&#x3D;0)&#123; System.out.println(&quot;请设置列族信息!&quot;); return; &#125; &#x2F;&#x2F;② 判断表是否存在 if (isTableExist(tableName))&#123; System.out.println(&quot;表&quot;+tableName+&quot;已存在!&quot;); return; &#125; &#x2F;&#x2F;③ 创建表描述器 TableName.valueOf(tableName)是获取表名 &#x2F;** * 代码逻辑:创建表需要admin对象，然后又发现admin对象需要创建列族描述器，添加列族信息，勿死记硬背 *&#x2F; HTableDescriptor hTableDescriptor &#x3D; new HTableDescriptor(TableName.valueOf(tableName)); &#x2F;&#x2F;④ 循环添加列族信息 for (String cf:cfs)&#123; &#x2F;&#x2F;⑤ 创建列族描述器 HColumnDescriptor hColumnDescriptor &#x3D; new HColumnDescriptor(cf); &#x2F;&#x2F;⑥ 添加列族信息 hTableDescriptor.addFamily(hColumnDescriptor); &#125; &#x2F;&#x2F;⑦ 创建表 admin.createTable(hTableDescriptor); &#125; 测试123456789101112public static void main(String[] args) throws IOException &#123; &#x2F;&#x2F;1、测试表是否存在 System.out.println(isTableExist(&quot;student&quot;)); &#x2F;&#x2F;2、创建表 &quot;0408:stu&quot;在0408命名空间创建表 creatTable(&quot;stu&quot;,&quot;info1&quot;,&quot;info2&quot;); System.out.println(isTableExist(&quot;stu&quot;)); &#x2F;&#x2F;关闭资源 close(); &#125; 2.3、删除表1234567891011121314151617181920&#x2F;** * 3、删除表 * @param * @throws IOException *&#x2F; public static void dropTable(String tableName) throws IOException &#123; &#x2F;&#x2F;① 判断表是否存在 if (!isTableExist(tableName))&#123; System.out.println(&quot;表&quot;+tableName+&quot;不存在!&quot;); return; &#125; &#x2F;&#x2F;② 使表下线 admin.disableTable(TableName.valueOf(tableName)); &#x2F;&#x2F;③ 删除表 admin.deleteTable(TableName.valueOf(tableName)); &#125; 测试123456789101112131415public static void main(String[] args) throws IOException &#123; &#x2F;&#x2F;1、测试表是否存在 System.out.println(isTableExist(&quot;stu&quot;)); &#x2F;&#x2F;2、创建表 &quot;0408:stu&quot;在0408命名空间创建表 creatTable(&quot;stu&quot;,&quot;info1&quot;,&quot;info2&quot;); System.out.println(isTableExist(&quot;stu&quot;)); &#x2F;&#x2F;3、删除表 dropTable(&quot;stu&quot;); System.out.println(isTableExist(&quot;stu&quot;)); &#x2F;&#x2F;关闭资源 close(); &#125; 2.4、创建命名空间123456789101112131415161718&#x2F;** * 4、创建命名空间 * @param * @throws IOException * NamespaceExistException这个异常是已存在是抛出的，复制就行了 *&#x2F; public static void creatNameSpace(String ns)&#123; &#x2F;&#x2F;① 创建命名空间描述器 NamespaceDescriptor namespaceDescriptor &#x3D; NamespaceDescriptor.create(ns).build(); &#x2F;&#x2F;② 创建命名空间 try &#123; admin.createNamespace(namespaceDescriptor); &#125;catch (NamespaceExistException e)&#123; System.out.println(ns+&quot;命名空间已存在！&quot;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; 测试12345678910111213141516171819public static void main(String[] args) throws IOException &#123; &#x2F;&#x2F;1、测试表是否存在 System.out.println(isTableExist(&quot;stu&quot;)); &#x2F;&#x2F;2、创建表 &quot;0408:stu&quot;在0408命名空间创建表 creatTable(&quot;0408:stu&quot;,&quot;info1&quot;,&quot;info2&quot;); System.out.println(isTableExist(&quot;stu&quot;)); &#x2F;&#x2F;3、删除表 dropTable(&quot;stu&quot;); System.out.println(isTableExist(&quot;stu&quot;)); &#x2F;&#x2F;4、创建命名空间 creatNameSpace(&quot;0408&quot;); &#x2F;&#x2F;关闭资源 close(); &#125; 2.5、向表插入数据1234567891011121314151617&#x2F;** * 5、向表插入数据 * @param * @throws IOException *&#x2F; public static void putData(String tableName,String rowKey,String columnFamily,String column,String value) throws IOException &#123; &#x2F;&#x2F;① 获取表对象 Table table &#x3D; connection.getTable(TableName.valueOf(tableName)); &#x2F;&#x2F;② 创建put对象 Bytes是Hbase下的一个util Put put &#x3D; new Put(Bytes.toBytes(rowKey)); &#x2F;&#x2F;③ 向put数组中装入对象 put.addColumn(Bytes.toBytes(columnFamily),Bytes.toBytes(column),Bytes.toBytes(value)); &#x2F;&#x2F;④ 插入数据 table.put(put); &#x2F;&#x2F;⑤ 关闭资源 table.close(); &#125; 测试12345678public static void main(String[] args) throws IOException &#123; &#x2F;&#x2F;5、向表中插入数据 putData(&quot;student&quot;,&quot;1005&quot;,&quot;info&quot;,&quot;name&quot;,&quot;zhangsan&quot;); &#x2F;&#x2F;关闭资源 close(); &#125; 2.6、获取数据（get）1234567891011121314151617181920212223242526272829303132&#x2F;** * 6、获取数据（get） * @param * @throws IOException *&#x2F; public static void getData(String tableName,String rowKey,String columnFamily,String column) throws IOException &#123; &#x2F;&#x2F;① 获取表对象 Table table &#x3D; connection.getTable(TableName.valueOf(tableName)); &#x2F;&#x2F;② 创建Get对象 Get get &#x3D; new Get(Bytes.toBytes(rowKey)); &#x2F;&#x2F;2.1 指定获取的列族&#x2F;&#x2F; get.addFamily(Bytes.toBytes(columnFamily)); &#x2F;&#x2F;2.2 指定列族和列 get.addColumn(Bytes.toBytes(columnFamily),Bytes.toBytes(column)); &#x2F;&#x2F;2.3 指定获取的版本 get.setMaxVersions(); &#x2F;&#x2F;③ 获取数据 Result集合 Result result &#x3D; table.get(get); &#x2F;&#x2F;④ 解析result并打印 for (Cell cell : result.rawCells()) &#123; &#x2F;&#x2F;⑤ 打印数据 System.out.println(&quot;columnFamily:&quot;+Bytes.toString(CellUtil.cloneFamily(cell))+ &quot;, column:&quot;+Bytes.toString(CellUtil.cloneQualifier(cell))+ &quot;, value:&quot;+Bytes.toString(CellUtil.cloneValue(cell))); &#125; &#x2F;&#x2F;⑥ 关闭表连接 table.close(); &#125; 测试12345public static void main(String[] args) throws IOException &#123; &#x2F;&#x2F;6、获取单行数据 getData(&quot;student&quot;,&quot;1001&quot;,&quot;info&quot;,&quot;name&quot;); &#125; 2.7、扫描全表123456789101112131415161718192021222324252627&#x2F;** * 7、获取数据(扫描全表) * @param * @throws IOException *&#x2F; public static void scanTable(String tableName) throws IOException &#123; &#x2F;&#x2F;① 获取表对象 Table table &#x3D; connection.getTable(TableName.valueOf(tableName)); &#x2F;&#x2F;② 构建scan对象 （这里可以指定获取的rowKey,左闭右开） Scan scan &#x3D; new Scan(Bytes.toBytes(&quot;1001&quot;),Bytes.toBytes(&quot;1003&quot; )); &#x2F;&#x2F;③ 扫描表 ResultScanner results &#x3D; table.getScanner(scan); &#x2F;&#x2F;④ 解析results for (Result result : results) &#123; &#x2F;&#x2F;⑤ 解析result并打印 for (Cell cell : result.rawCells()) &#123; &#x2F;&#x2F;⑥ 打印数据 System.out.println(&quot;rowKey:&quot;+Bytes.toString(CellUtil.cloneRow(cell))+ &quot;, columnFamily:&quot;+Bytes.toString(CellUtil.cloneFamily(cell))+ &quot;, column:&quot;+Bytes.toString(CellUtil.cloneQualifier(cell))+ &quot;, value:&quot;+Bytes.toString(CellUtil.cloneValue(cell))); &#125; &#125; &#x2F;&#x2F;⑦ 关闭表连接 table.close(); &#125; 测试1234567public static void main(String[] args) throws IOException &#123; &#x2F;&#x2F;7、获取数据(扫描全表) scanTable(&quot;student1&quot;); &#x2F;&#x2F;关闭资源 close(); &#125; 运行结果123456789101112log4j:WARN No appenders could be found for logger (org.apache.hadoop.security.Groups).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http:&#x2F;&#x2F;logging.apache.org&#x2F;log4j&#x2F;1.2&#x2F;faq.html#noconfig for more info.rowKey:1001, columnFamily:info, column:age, value:16rowKey:1001, columnFamily:info, column:name, value:sunliurowKey:1001, columnFamily:info, column:sex, value:manrowKey:1002, columnFamily:info, column:age, value:18rowKey:1002, columnFamily:info, column:name, value:lucyrowKey:1002, columnFamily:info, column:sex, value:female进程已结束，退出代码 0 2.8、数据删除 数据的删除在Hbase中是一个比较麻烦的问题，因为涉及到版本和内存刷新的问题，因此要多注意 123456789101112131415161718192021222324&#x2F;** * 8、删除数据 * @param * @throws IOException *&#x2F; public static void deleteTable(String tableName,String rowKey,String columnFamily,String column) throws IOException &#123; &#x2F;&#x2F;① 获取表对象 Table table &#x3D; connection.getTable(TableName.valueOf(tableName)); &#x2F;&#x2F;② 构建删除对象 Delete delete &#x3D; new Delete(Bytes.toBytes(rowKey)); &#x2F;&#x2F;2.1设置删除的列 &#x2F;* 这是删除一个版本的数据 delete.addColumn(Bytes.toBytes(columnFamily),Bytes.toBytes(column));*&#x2F; &#x2F;&#x2F;不指定时间戳就会删除所有版本的数据，不建议用addColumn &#x2F;&#x2F; 也可以删除指定的时间戳timestamp&#x3D;1575080075098,报错，对于int类型来说数值太大，改成timestamp&#x3D;1575080075098L &#x2F;&#x2F;delete.addColumns(Bytes.toBytes(columnFamily),Bytes.toBytes(column)); &#x2F;&#x2F;2.2删除指定的列族 delete.addFamily(Bytes.toBytes(columnFamily)); &#x2F;&#x2F;③ 执行删除操作 table.delete(delete); &#x2F;&#x2F;④ 关闭连接 table.close(); &#125; 测试1234567public static void main(String[] args) throws IOException &#123; &#x2F;&#x2F;8、测试删除 deleteTable(&quot;student1&quot;,&quot;1005&quot;,&quot;info&quot;,&quot;name&quot;); &#x2F;&#x2F;关闭资源 close(); &#125; 三、报错信息java.io.IOException: Failed to get result within timeout, timeout=60000ms 解决办法；将虚拟机的IP地址写到C:\\Windows\\System32\\drivers\\etc\\hosts文件里就可以了 四、HbaseAPI测试实例源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298package com.lizhi.test;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.*;import org.apache.hadoop.hbase.client.*;import org.apache.hadoop.hbase.util.Bytes;import java.io.IOException;&#x2F;** * DDL: * 1、判断表是否存在 * 2、创建表 * 3、创建命名空间 * 4、删除表 * * DDM: * 5、插入数据 * 6、查数据(get) * 7、查数据(scan) * 8、删除数据 * *&#x2F;public class testAPI &#123; private static Connection connection &#x3D; null; private static Admin admin &#x3D; null; public static Configuration configuration &#x3D;null; static &#123; try &#123; &#x2F;&#x2F;① 获取配置文件信息 Configuration configuration &#x3D; HBaseConfiguration.create(); configuration.set(&quot;hbase.zookeeper.quorum&quot;,&quot;master&quot;); configuration.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;); &#x2F;&#x2F;② 创建连接对象 connection &#x3D; ConnectionFactory.createConnection(configuration); &#x2F;&#x2F;③ 创建admin对象 admin &#x3D; connection.getAdmin(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public static void close()&#123; if (admin!&#x3D;null)&#123; try &#123; admin.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; if (connection!&#x3D;null)&#123; try &#123; connection.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#x2F;** * 1、判断表是否存在 * @param *&#x2F; public static boolean isTableExist(String tableNme) throws IOException &#123; &#x2F;* &#x2F;&#x2F;① 获取配置文件信息 &#x2F;&#x2F;HBaseConfiguration configuration &#x3D; new HBaseConfiguration();&#x2F;&#x2F;(过时的方法) Configuration configuration &#x3D; HBaseConfiguration.create(); configuration.set(&quot;hbase.zookeeper.quorum&quot;,&quot;master&quot;); configuration.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;); &#x2F;&#x2F;② 获取管理员对象 &#x2F;&#x2F;HBaseAdmin admin &#x3D; new HBaseAdmin(configuration);&#x2F;&#x2F;(过时的方法) Connection connection &#x3D; ConnectionFactory.createConnection(configuration); Admin admin &#x3D; connection.getAdmin();*&#x2F; &#x2F;&#x2F;③ 判断表是否存在 TableName.valueOf(tableName)是获取表名 boolean exists &#x3D; admin.tableExists(TableName.valueOf(tableNme)); &#x2F;&#x2F;④ 关闭连接 &#x2F;&#x2F;admin.close(); &#x2F;&#x2F;⑤ 返回结果 return exists; &#125; &#x2F;** * 2、创建表 * @param * @throws IOException * 注: String... :是可变形参，由于列族可以有多个所以用可变形参 *&#x2F; public static void creatTable(String tableName,String... cfs) throws IOException &#123; &#x2F;&#x2F;① 判断是否存在列族 if (cfs.length&lt;&#x3D;0)&#123; System.out.println(&quot;请设置列族信息!&quot;); return; &#125; &#x2F;&#x2F;② 判断表是否存在 if (isTableExist(tableName))&#123; System.out.println(&quot;表&quot;+tableName+&quot;已存在!&quot;); return; &#125; &#x2F;&#x2F;③ 创建表描述器 TableName.valueOf(tableName)是获取表名 &#x2F;** * 代码逻辑:创建表需要admin对象，然后又发现admin对象需要创建列族描述器，添加列族信息，勿死记硬背 *&#x2F; HTableDescriptor hTableDescriptor &#x3D; new HTableDescriptor(TableName.valueOf(tableName)); &#x2F;&#x2F;④ 循环添加列族信息 for (String cf:cfs)&#123; &#x2F;&#x2F;⑤ 创建列族描述器 HColumnDescriptor hColumnDescriptor &#x3D; new HColumnDescriptor(cf); &#x2F;&#x2F;⑥ 添加列族信息 hTableDescriptor.addFamily(hColumnDescriptor); &#125; &#x2F;&#x2F;⑦ 创建表 admin.createTable(hTableDescriptor); &#125; &#x2F;** * 3、删除表 * @param * @throws IOException *&#x2F; public static void dropTable(String tableName) throws IOException &#123; &#x2F;&#x2F;① 判断表是否存在 if (!isTableExist(tableName))&#123; System.out.println(&quot;表&quot;+tableName+&quot;不存在!&quot;); return; &#125; &#x2F;&#x2F;② 使表下线 admin.disableTable(TableName.valueOf(tableName)); &#x2F;&#x2F;③ 删除表 admin.deleteTable(TableName.valueOf(tableName)); System.out.println(&quot;表&quot;+tableName+&quot;已成功删除！&quot;); &#125; &#x2F;** * 4、创建命名空间 * @param * @throws IOException * NamespaceExistException这个异常是已存在是抛出的，复制就行了 *&#x2F; public static void creatNameSpace(String ns)&#123; &#x2F;&#x2F;① 创建命名空间描述器 NamespaceDescriptor namespaceDescriptor &#x3D; NamespaceDescriptor.create(ns).build(); &#x2F;&#x2F;② 创建命名空间 try &#123; admin.createNamespace(namespaceDescriptor); &#125;catch (NamespaceExistException e)&#123; System.out.println(ns+&quot;命名空间已存在！&quot;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#x2F;** * 5、向表插入数据 * @param * @throws IOException *&#x2F; public static void putData(String tableName,String rowKey,String columnFamily,String column,String value) throws IOException &#123; &#x2F;&#x2F;① 获取表对象 Table table &#x3D; connection.getTable(TableName.valueOf(tableName)); &#x2F;&#x2F;② 创建put对象 Bytes是Hbase下的一个util Put put &#x3D; new Put(Bytes.toBytes(rowKey)); &#x2F;&#x2F;③ 向put数组中装入对象 put.addColumn(Bytes.toBytes(columnFamily),Bytes.toBytes(column),Bytes.toBytes(value)); &#x2F;&#x2F;④ 插入数据 table.put(put); &#x2F;&#x2F;⑤ 关闭资源 table.close(); System.out.println(&quot;插入数据成功！&quot;); &#125; &#x2F;** * 6、获取数据（get） * @param * @throws IOException *&#x2F; public static void getData(String tableName,String rowKey,String columnFamily,String column) throws IOException &#123; &#x2F;&#x2F;① 获取表对象 Table table &#x3D; connection.getTable(TableName.valueOf(tableName)); &#x2F;&#x2F;② 创建Get对象 Get get &#x3D; new Get(Bytes.toBytes(rowKey)); &#x2F;&#x2F;2.1 指定获取的列族&#x2F;&#x2F; get.addFamily(Bytes.toBytes(columnFamily)); &#x2F;&#x2F;2.2 指定列族和列 get.addColumn(Bytes.toBytes(columnFamily),Bytes.toBytes(column)); &#x2F;&#x2F;2.3 指定获取的版本 get.setMaxVersions(); &#x2F;&#x2F;③ 获取数据 Result集合 Result result &#x3D; table.get(get); &#x2F;&#x2F;④ 解析result并打印 for (Cell cell : result.rawCells()) &#123; &#x2F;&#x2F;⑤ 打印数据 System.out.println(&quot;columnFamily:&quot;+Bytes.toString(CellUtil.cloneFamily(cell))+ &quot;, column:&quot;+Bytes.toString(CellUtil.cloneQualifier(cell))+ &quot;, value:&quot;+Bytes.toString(CellUtil.cloneValue(cell))); &#125; &#x2F;&#x2F;⑥ 关闭表连接 table.close(); &#125; &#x2F;** * 7、获取数据(扫描全表) * @param * @throws IOException *&#x2F; public static void scanTable(String tableName) throws IOException &#123; &#x2F;&#x2F;① 获取表对象 Table table &#x3D; connection.getTable(TableName.valueOf(tableName)); &#x2F;&#x2F;② 构建scan对象 （这里可以指定获取的rowKey,左闭右开） Scan scan &#x3D; new Scan(Bytes.toBytes(&quot;1001&quot;),Bytes.toBytes(&quot;1003&quot; )); &#x2F;&#x2F;③ 扫描表 ResultScanner results &#x3D; table.getScanner(scan); &#x2F;&#x2F;④ 解析results for (Result result : results) &#123; &#x2F;&#x2F;⑤ 解析result并打印 for (Cell cell : result.rawCells()) &#123; &#x2F;&#x2F;⑥ 打印数据 System.out.println(&quot;rowKey:&quot;+Bytes.toString(CellUtil.cloneRow(cell))+ &quot;, columnFamily:&quot;+Bytes.toString(CellUtil.cloneFamily(cell))+ &quot;, column:&quot;+Bytes.toString(CellUtil.cloneQualifier(cell))+ &quot;, value:&quot;+Bytes.toString(CellUtil.cloneValue(cell))); &#125; &#125; &#x2F;&#x2F;⑦ 关闭表连接 table.close(); &#125; &#x2F;** * 8、删除数据 * @param * @throws IOException *&#x2F; public static void deleteTable(String tableName,String rowKey,String columnFamily,String column) throws IOException &#123; &#x2F;&#x2F;① 获取表对象 Table table &#x3D; connection.getTable(TableName.valueOf(tableName)); &#x2F;&#x2F;② 构建删除对象 Delete delete &#x3D; new Delete(Bytes.toBytes(rowKey)); &#x2F;&#x2F;2.1设置删除的列 &#x2F;* 这是删除一个版本的数据 delete.addColumn(Bytes.toBytes(columnFamily),Bytes.toBytes(column));*&#x2F; &#x2F;&#x2F;不指定时间戳就会删除所有版本的数据，不建议用addColumn &#x2F;&#x2F; 也可以删除指定的时间戳timestamp&#x3D;1575080075098,报错，对于int类型来说数值太大，改成timestamp&#x3D;1575080075098L &#x2F;&#x2F;delete.addColumns(Bytes.toBytes(columnFamily),Bytes.toBytes(column)); &#x2F;&#x2F;2.2删除指定的列族 delete.addFamily(Bytes.toBytes(columnFamily)); &#x2F;&#x2F;③ 执行删除操作 table.delete(delete); &#x2F;&#x2F;④ 关闭连接 table.close(); &#125; public static void main(String[] args) throws IOException &#123; &#x2F;* &#x2F;&#x2F;1、测试表是否存在 System.out.println(isTableExist(&quot;stu&quot;)); &#x2F;&#x2F;2、创建表 &quot;0408:stu&quot;在0408命名空间创建表 creatTable(&quot;0408:stu&quot;,&quot;info1&quot;,&quot;info2&quot;); System.out.println(isTableExist(&quot;stu&quot;)); &#x2F;&#x2F;3、删除表 dropTable(&quot;stu&quot;); System.out.println(isTableExist(&quot;stu&quot;)); &#x2F;&#x2F;4、创建命名空间 creatNameSpace(&quot;0408&quot;); &#x2F;&#x2F;5、向表中插入数据 putData(&quot;student&quot;,&quot;1005&quot;,&quot;info&quot;,&quot;name&quot;,&quot;zhangsan&quot;); &#x2F;&#x2F;6、获取单行数据 getData(&quot;student&quot;,&quot;1001&quot;,&quot;info&quot;,&quot;name&quot;); &#x2F;&#x2F;7、获取数据(扫描全表) scanTable(&quot;student1&quot;); *&#x2F; &#x2F;&#x2F;8、测试删除 deleteTable(&quot;student1&quot;,&quot;1005&quot;,&quot;info&quot;,&quot;name&quot;); &#x2F;&#x2F;关闭资源 close(); &#125;&#125; pom.xml123456789101112&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;&#x2F;groupId&gt; &lt;artifactId&gt;hbase-server&lt;&#x2F;artifactId&gt; &lt;version&gt;1.3.5&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;&#x2F;groupId&gt; &lt;artifactId&gt;hbase-client&lt;&#x2F;artifactId&gt; &lt;version&gt;1.3.5&lt;&#x2F;version&gt; &lt;&#x2F;dependency&gt; &lt;&#x2F;dependencies&gt;","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/categories/hadoop/"},{"name":"hbaseAPI","slug":"hadoop/hbaseAPI","permalink":"http://www.studyz.club/categories/hadoop/hbaseAPI/"}],"tags":[{"name":"-hbase - hbaseAPI","slug":"hbase-hbaseAPI","permalink":"http://www.studyz.club/tags/hbase-hbaseAPI/"}]},{"title":"Maven项目本地仓库配置","slug":"Maven项目配置","date":"2019-11-24T01:33:10.745Z","updated":"2019-12-07T15:52:22.173Z","comments":true,"path":"posts/1d94394d/","link":"","permalink":"http://www.studyz.club/posts/1d94394d/","excerpt":"","text":"一、eclipse配置maven 二、IDEA配置maven 一、eclipse配置maven1、下载Maven下载网址 下载到本地后解压， 2、环境变量配置有的环境需要配置两项MAVEN_HOME和M2_HOME有的只要配置一项MAVEN_HOME 路径为bin目录的上一级 在conf/settings.xml里，将&lt;localRepository&gt;改成我们设置的本地仓库 E:\\Program\\maven-repository 这里配置的时候报了个错，大意是maven的运行需要java环境，而java环境已经搭建完成，然后发现Path里的java环境的位置与javahome里不一样，于是将path的环境改成和javahome一样后就可以了 用命令行输入 4、在eclipse中Window–&gt;Perferences–&gt;Maven 先选择配置文件 然后选中我们创建的本地仓库的文件夹，将jar包导入就可以了 5、在项目上右键Buid Path创建用户库就可以全部导入了二、IDEA配置maven Idea 自带了apache maven，默认使用的是内置maven，所以我们可以配置全局setting，来调整一下配置，比如远程仓库地址，本地编译环境变量等。 1.打开Settings,在输入框输入maven，如图 2.如果本地设置了MAVEN_OPTS 系统环境变量，这个步骤可以忽略。1-Xms128m -Xmx512m -Duser.language&#x3D;zh -Dfile.encoding&#x3D;UTF-8 3、如果配置了本地apache-maven setting.xml 中的软件源，这步骤可以忽略。4、创建一个maven项目 根据不同的需求选择不同的模板，按照实际情况 在 POM 中，groupId, artifactId, packaging, version 叫作 maven 坐标，它能唯一的确定一个项目。有了 maven 坐标，我们就可以用它来指定我们的项目所依赖的其他项目，插件，或者父项目。 参数 说明 groupId 代表组织和整个项目的唯一标志。比如说所有的Maven组件的groupId都是org.apache.maven。 artifactId 具体项目的名称，它于groupId共同确定一个项目在maven repo中的位置，例如，groupId=org.codehaus.mojo, artifactId=my-project的项目，在maven repo中的位置为：$M2_REPO/org/codehaus/mojo/my-project version 用于说明目前项目的版本，在引用依赖的时候确定具体依赖的版本号。 packaging 规定项目的输出格式，包括jar、war、pom、apk等，根据实际需要确定。例如，开发一般的java库，可以使用jar packaging；开发android则是apk packaging。 一般 maven 坐标写成如下的格式： 1groupId:artifactId:packaging:version 虽然在不将项目提交到Maven官方仓库的情况下，这不是强制约束，但还是建议不论大小项目一律遵守Maven的命名标准。 选择Maven配置、仓库等。 这个在上一篇中已经配置过。保留默认即可。","categories":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"},{"name":"Maven项目本地仓库配置","slug":"软件开发框架/Maven项目本地仓库配置","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/Maven%E9%A1%B9%E7%9B%AE%E6%9C%AC%E5%9C%B0%E4%BB%93%E5%BA%93%E9%85%8D%E7%BD%AE/"}],"tags":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/tags/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"},{"name":"Maven项目本地仓库配置","slug":"Maven项目本地仓库配置","permalink":"http://www.studyz.club/tags/Maven%E9%A1%B9%E7%9B%AE%E6%9C%AC%E5%9C%B0%E4%BB%93%E5%BA%93%E9%85%8D%E7%BD%AE/"}]},{"title":"SSM整合报错记录","slug":"SSM整合报错记录","date":"2019-11-23T14:54:18.964Z","updated":"2019-12-07T15:52:22.174Z","comments":true,"path":"posts/ae8b2344/","link":"","permalink":"http://www.studyz.club/posts/ae8b2344/","excerpt":"","text":"缺少jar包123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960信息: Initializing Spring DispatcherServlet &#39;springDispatcherServlet&#39;19:52:58.179 [main] ERROR org.springframework.web.servlet.DispatcherServlet - Context initialization failedorg.springframework.beans.factory.BeanCreationException: Error creating bean with name &#39;org.springframework.validation.beanvalidation.OptionalValidatorFactoryBean#0&#39;: Invocation of init method failed; nested exception is java.lang.NoSuchMethodError: javax.validation.Configuration.getDefaultParameterNameProvider()Ljavax&#x2F;validation&#x2F;ParameterNameProvider; at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1803) ~[spring-beans-5.2.0.RELEASE.jar:5.2.0.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:595) ~[spring-beans-5.2.0.RELEASE.jar:5.2.0.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.0.RELEASE.jar:5.2.0.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.0.RELEASE.jar:5.2.0.RELEASE] at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.2.0.RELEASE.jar:5.2.0.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.0.RELEASE.jar:5.2.0.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.0.RELEASE.jar:5.2.0.RELEASE] at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:879) ~[spring-beans-5.2.0.RELEASE.jar:5.2.0.RELEASE] at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:878) ~[spring-context-5.2.0.RELEASE.jar:5.2.0.RELEASE] at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:550) ~[spring-context-5.2.0.RELEASE.jar:5.2.0.RELEASE] at org.springframework.web.servlet.FrameworkServlet.configureAndRefreshWebApplicationContext(FrameworkServlet.java:702) ~[spring-webmvc-5.2.0.RELEASE.jar:5.2.0.RELEASE] at org.springframework.web.servlet.FrameworkServlet.createWebApplicationContext(FrameworkServlet.java:668) ~[spring-webmvc-5.2.0.RELEASE.jar:5.2.0.RELEASE] at org.springframework.web.servlet.FrameworkServlet.createWebApplicationContext(FrameworkServlet.java:716) ~[spring-webmvc-5.2.0.RELEASE.jar:5.2.0.RELEASE] at org.springframework.web.servlet.FrameworkServlet.initWebApplicationContext(FrameworkServlet.java:591) ~[spring-webmvc-5.2.0.RELEASE.jar:5.2.0.RELEASE] at org.springframework.web.servlet.FrameworkServlet.initServletBean(FrameworkServlet.java:530) [spring-webmvc-5.2.0.RELEASE.jar:5.2.0.RELEASE] at org.springframework.web.servlet.HttpServletBean.init(HttpServletBean.java:170) [spring-webmvc-5.2.0.RELEASE.jar:5.2.0.RELEASE] at javax.servlet.GenericServlet.init(GenericServlet.java:158) [servlet-api.jar:4.0.FR] at org.apache.catalina.core.StandardWrapper.initServlet(StandardWrapper.java:1123) [catalina.jar:9.0.17] at org.apache.catalina.core.StandardWrapper.loadServlet(StandardWrapper.java:1078) [catalina.jar:9.0.17] at org.apache.catalina.core.StandardWrapper.load(StandardWrapper.java:971) [catalina.jar:9.0.17] at org.apache.catalina.core.StandardContext.loadOnStartup(StandardContext.java:4868) [catalina.jar:9.0.17] at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5184) [catalina.jar:9.0.17] at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) [catalina.jar:9.0.17] at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1377) [catalina.jar:9.0.17] at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1367) [catalina.jar:9.0.17] at java.util.concurrent.FutureTask.run(Unknown Source) [?:1.8.0_192] at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75) [tomcat-util.jar:9.0.17] at java.util.concurrent.AbstractExecutorService.submit(Unknown Source) [?:1.8.0_192] at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:902) [catalina.jar:9.0.17] at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:831) [catalina.jar:9.0.17] at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) [catalina.jar:9.0.17] at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1377) [catalina.jar:9.0.17] at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1367) [catalina.jar:9.0.17] at java.util.concurrent.FutureTask.run(Unknown Source) [?:1.8.0_192] at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75) [tomcat-util.jar:9.0.17] at java.util.concurrent.AbstractExecutorService.submit(Unknown Source) [?:1.8.0_192] at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:902) [catalina.jar:9.0.17] at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:262) [catalina.jar:9.0.17] at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) [catalina.jar:9.0.17] at org.apache.catalina.core.StandardService.startInternal(StandardService.java:423) [catalina.jar:9.0.17] at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) [catalina.jar:9.0.17] at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:928) [catalina.jar:9.0.17] at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) [catalina.jar:9.0.17] at org.apache.catalina.startup.Catalina.start(Catalina.java:634) [catalina.jar:9.0.17] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_192] at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[?:1.8.0_192] at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[?:1.8.0_192] at java.lang.reflect.Method.invoke(Unknown Source) ~[?:1.8.0_192] at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:350) [bootstrap.jar:9.0.17] at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:492) [bootstrap.jar:9.0.17]Caused by: java.lang.NoSuchMethodError: javax.validation.Configuration.getDefaultParameterNameProvider()Ljavax&#x2F;validation&#x2F;ParameterNameProvider; at org.springframework.validation.beanvalidation.LocalValidatorFactoryBean.configureParameterNameProvider(LocalValidatorFactoryBean.java:315) ~[spring-context-5.2.0.RELEASE.jar:5.2.0.RELEASE] at org.springframework.validation.beanvalidation.LocalValidatorFactoryBean.afterPropertiesSet(LocalValidatorFactoryBean.java:291) ~[spring-context-5.2.0.RELEASE.jar:5.2.0.RELEASE] at org.springframework.validation.beanvalidation.OptionalValidatorFactoryBean.afterPropertiesSet(OptionalValidatorFactoryBean.java:40) ~[spring-context-5.2.0.RELEASE.jar:5.2.0.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1862) ~[spring-beans-5.2.0.RELEASE.jar:5.2.0.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1799) ~[spring-beans-5.2.0.RELEASE.jar:5.2.0.RELEASE] ... 49 more jar包","categories":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"},{"name":"SSM","slug":"软件开发框架/SSM","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/SSM/"}],"tags":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/tags/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"},{"name":"SSM","slug":"SSM","permalink":"http://www.studyz.club/tags/SSM/"}]},{"title":"软件开发框架之SpringMVC-利用SpringMVC做一个符合REST风格的CRUD","slug":"软件开发框架之SpringMVC-利用SpringMVC做一个符合REST风格的CRUD","date":"2019-11-21T14:21:44.358Z","updated":"2019-12-07T15:52:22.179Z","comments":true,"path":"posts/3874c9/","link":"","permalink":"http://www.studyz.club/posts/3874c9/","excerpt":"","text":"一、目标 一、目标利用SpringMVC做一个CRUD（增删改查）符合Rest风格的； C：Create：创建 R：Retrieve：查询 U：Update：更新 D：Delete：删除 数据库：保存数据； 使用Map，List保存数据之类 员工列表: 员工添加： 员工更新: 员工删除-点击完成删除来到列表页面对于REST风格 增删改查的URL地址； /资源名/资源标识 /emp/1 GET：查询id为1的员工 /emp/1 PUT：更新id为1的员工 /emp/1 DELETE：删除id为1的员工 /emp POST：新增员工； /emps GET：查询所有员工 1234员工列表展示；查询所有员工； 员工列表展示：访问index.jsp----直接发送&#x2F;emps------控制器查询所有员工------放在请求域中-----转发到list页面展示 员工添加： 在list页面点击“”员工添加“”----（查询出所有的部门信息要展示在页面）----来到添加页面（add.jsp）--------输入员工数据--------点击保存（&#x2F;emp ）------处理器收到员工保存请求（保存员工）--------保存完成以后还是来到列表页面； 通过 SpringMVC的表单标签可以实现将模型数据中的属性和 HTML 表单元素相绑定，以实现表单数据更便捷编辑和表单值的回显如果写完SpringMVC自带的表单属性可能会报如下错误：123java.lang.IllegalStateException: Neither BindingResult nor plain target object for bean name &#39;command&#39; available as request attribute org.springframework.web.servlet.support.BindStatus.&lt;init&gt;(BindStatus.java:153) org.springframework.web.servlet.tags.form.AbstractDataBoundFormElementTag.getBindStatus(AbstractDataBoundFormElementTag.java:178 意思是请求域中没有一个command类型的对象； 解决办法 : 来到页面之前一定要给请求域中放这个对象；1234567891011121314151617181920212223242526272829303132333435&lt;!-- 表单标签 通过 SpringMVC的&#96;表单标签&#96;可以实现将模型数据中的属性和 HTML 表单元素相绑定， 以实现表单数据&#96;更便捷编辑和表单值的回显&#96; 1、SpringMVC认为，表单数据中的每一项最终都是要显回的； path指定的是一个属性；这个属性是从隐含模型（请求域中取出的某个对象中的属性） path指定的每一个属性，请求域中必须有一个对象，拥有这个属性； 请求域中没有一个command类型的对象；来到页面之前一定要给请求域中放这个对象； modelAttribute&#x3D;&quot;&quot;: 1、以前我们表单标签会从请求域中获取一个command对象；把这个对象中的每一个竖向对应的显示出来 2、可以告诉SpringMVC不要去取command的值了，我放了一个modelAttribute指定的key, 取对象用的key就要modelAttribute指定的--&gt;&lt;form:form action&#x3D;&quot;&quot; modelAttribute&#x3D;&quot;employee&quot;&gt; &lt;!-- path&#x3D;&quot;&quot;就是原来HTML里input的 name项 ，需要写 path: 1、当做原生的name项 2、自动回显隐含模型中某个对象对应的这个属性的值 --&gt; lastName: &lt;form:input path&#x3D;&quot;LastName&quot;&#x2F;&gt;&lt;br&gt; email:&lt;form:input path&#x3D;&quot;email&quot;&#x2F;&gt;&lt;br&gt; gender:&lt;br&gt; 男:&lt;form:radiobutton path&#x3D;&quot;gender&quot; value&#x3D;&quot;1&quot;&#x2F;&gt;&lt;br&gt; 女:&lt;form:radiobutton path&#x3D;&quot;gender&quot; value&#x3D;&quot;0&quot;&#x2F;&gt;&lt;br&gt; dept: &lt;!-- items&#x3D;&quot;&quot;:指定要遍历的集合;自动遍历；遍历出每一个元素是一个department对象 itemLabel&#x3D;&quot;属性&quot; : 指定遍历出的这个对象的哪个属性是作为option标签的值 itemValue&#x3D;&quot;属性名&quot; : 指定刚才遍历出的这个对象的那个属性作为要提交的属性值 --&gt; &lt;form:select path&#x3D;&quot;department.id&quot; items&#x3D;&quot;$&#123;depts &#125;&quot; itemLabel&#x3D;&quot;departmentName&quot; itemValue&#x3D;&quot;id&quot;&gt; &lt;&#x2F;form:select&gt; &lt;input type&#x3D;&quot;submit&quot; value&#x3D;&quot;保存&quot;&gt;&lt;&#x2F;form:form&gt; 在EmployeeController.java中12345678910111213141516&#x2F;** * 去员工添加页面，去页面之前需要查出所有部门信息，进行展示的 * * @return *&#x2F; @RequestMapping(&quot;&#x2F;toAddPage&quot;) public String toAddPage(Model model) &#123; &#x2F;&#x2F; 1、先查出所有部门 Collection&lt;Department&gt; departments &#x3D; departmentDao.getDepartments(); &#x2F;&#x2F; 2、放在请求域中 model.addAttribute(&quot;depts&quot;, departments); model.addAttribute(&quot;employee&quot;, new Employee()); &#x2F;&#x2F; 3、去添加页面 return &quot;add&quot;; &#125;","categories":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"},{"name":"利用SpringMVC做一个符合REST风格的CRUD","slug":"软件开发框架/利用SpringMVC做一个符合REST风格的CRUD","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/%E5%88%A9%E7%94%A8SpringMVC%E5%81%9A%E4%B8%80%E4%B8%AA%E7%AC%A6%E5%90%88REST%E9%A3%8E%E6%A0%BC%E7%9A%84CRUD/"}],"tags":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/tags/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"},{"name":"利用SpringMVC做一个符合REST风格的CRUD","slug":"利用SpringMVC做一个符合REST风格的CRUD","permalink":"http://www.studyz.club/tags/%E5%88%A9%E7%94%A8SpringMVC%E5%81%9A%E4%B8%80%E4%B8%AA%E7%AC%A6%E5%90%88REST%E9%A3%8E%E6%A0%BC%E7%9A%84CRUD/"}]},{"title":"软件开发框架之SpringMVC-视图解析","slug":"软件开发框架之SpringMVC-视图解析","date":"2019-11-21T01:20:04.715Z","updated":"2019-12-07T15:52:22.177Z","comments":true,"path":"posts/a55778d0/","link":"","permalink":"http://www.studyz.club/posts/a55778d0/","excerpt":"","text":"一、问题: 让hello请求处理完后到hello.jsp页面 ①、用相对路径的写法 ②、用forward:转发到一个页面 ③、利用redirect:重定向到hello.jsp页面 一、问题: 让hello请求处理完后到hello.jsp页面 hello.jsp页面的位置/WebContent 而处理完hello请求的位置如果采用拼串就会到 /WebContent/WEB-INF/pages/下面找hello.jsp ①、用相对路径的写法1234@RequestMapping(&quot;&#x2F;hello&quot;) public String Hello() &#123; return &quot;..&#x2F;..&#x2F;hello&quot;; &#125; ②、用forward:转发到一个页面12345678910&#x2F;** * 2、用forward:转发到一个页面 * &#x2F;Hello.jsp : 转发当前项目下的hello * * forward:&#x2F;hello.jsp 转发到当前项目下的hello.jsp * 且这种方法不会被springDispatcherServlet-servlet.xml里的配置拼串， * 是一个独立的系统 * hello前面一定要加‘&#x2F;’,如果不加就是相对路径，容易出问题 * @return *&#x2F; 测试实例12345678910111213141516@RequestMapping(&quot;&#x2F;handle01&quot;) public String handle01() &#123; System.out.println(&quot;handle01...&quot;); return &quot;forward:&#x2F;hello.jsp&quot;; &#125; &#x2F;** * forward:可以将请求转发给别的请求 * * @return *&#x2F; @RequestMapping(&quot;&#x2F;handle02&quot;) public String handle02() &#123; System.out.println(&quot;handle02...&quot;); return &quot;forward:&#x2F;handle01&quot;; &#125; ③、利用redirect:重定向到hello.jsp页面12345678910111213&#x2F;** * 3、利用redirect:重定向到hello.jsp页面 * 有前缀的转发和重定向操作, * springDispatcherServlet-servlet.xml里配置的视图解析器就不会拼串 * forward: 转发的路径 * 重定向 redirect:重定向的路径 * &#x2F;hello.jsp : 代表就是从当前项目下开始；SpringMVC会为路径自动的拼接上项目名 * * * 原生的Servlet重定向&#x2F;路径需要加上项目名才能成功 * response.sendRedirect(&quot;&#x2F;hello.jsp&quot;) * @return *&#x2F; 测试实例12345678910111213141516@RequestMapping(&quot;&#x2F;handle03&quot;) public String handle03() &#123; System.out.println(&quot;handle03......&quot;); return &quot;redirect:&#x2F;hello.jsp&quot;; &#125; &#x2F;** * 转发请求 * @return *&#x2F; @RequestMapping(&quot;&#x2F;handle04&quot;) public String handle04() &#123; System.out.println(&quot;handle04......&quot;); return &quot;redirect:&#x2F;handle03&quot;; &#125; index.jsp页面配置12345678910111213141516&lt;body&gt;&lt;a href&#x3D;&quot;hello&quot;&gt;HELLO&lt;&#x2F;a&gt;&lt;br&#x2F;&gt;&lt;a href&#x3D;&quot;handle01&quot;&gt;handle01来到hello页面&lt;&#x2F;a&gt;&lt;br&#x2F;&gt;&lt;a href&#x3D;&quot;handle02&quot;&gt;forward:handle02请求转发请求给了handle01&lt;&#x2F;a&gt;&lt;br&#x2F;&gt;&lt;a href&#x3D;&quot;handle03&quot;&gt;redirect:重定向到hello.jsp页面&lt;&#x2F;a&gt;&lt;br&#x2F;&gt;&lt;a href&#x3D;&quot;handle04&quot;&gt;利用redirect:将handle04请求转发到handle03请求&lt;&#x2F;a&gt;&lt;br&#x2F;&gt;&lt;&#x2F;body&gt; SpringMVC视图解析器源码分析以及自定义视图解析器以及国际化（现在时间不够了，看到后请及时补充）","categories":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"},{"name":"SpringMVC-视图解析","slug":"软件开发框架/SpringMVC-视图解析","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/SpringMVC-%E8%A7%86%E5%9B%BE%E8%A7%A3%E6%9E%90/"}],"tags":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/tags/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"},{"name":"SpringMVC-视图解析","slug":"SpringMVC-视图解析","permalink":"http://www.studyz.club/tags/SpringMVC-%E8%A7%86%E5%9B%BE%E8%A7%A3%E6%9E%90/"}]},{"title":"软件开发框架之SpringMVC-数据输出","slug":"软件开发框架之SpringMVC-数据输出","date":"2019-11-20T04:25:03.078Z","updated":"2019-12-07T15:52:22.183Z","comments":true,"path":"posts/622ce00e/","link":"","permalink":"http://www.studyz.club/posts/622ce00e/","excerpt":"","text":"一、Map、或者Model或者ModelMap 二、ModelAndView方法 三、利用注解给Session域携带数据 四、ModelAttribute注解 4.1、全字段更新引发的问题以及解决思想 五、测试实例源码 一、Map、或者Model或者ModelMap1234567891011121314151617181920212223242526&#x2F;** * SpringMVC除过在方法上传入原生API的request和session外还能怎样把数据带给页面? * 1、在方法处传入Map、或者Model或者ModelMap。给这些参数里面保存的所有数据都会放在域中。 * 可以把数据带给页面。注意：这三种方法携带的数据都在请求域中(request) Map，Model，ModelMap：最终都是BindingAwareModelMap在工作； 相当于给BindingAwareModelMap中保存的东西都会被放在请求域中； org.springframework.validation.support.BindingAwareModelMap Map(interface(jdk里的)) Model(interface(spring里的)) || &#x2F;&#x2F; || &#x2F;&#x2F; \\&#x2F; &#x2F;&#x2F; ModelMap(clas) &#x2F;&#x2F; \\\\ &#x2F;&#x2F; \\\\ &#x2F;&#x2F; ExtendedModelMap || \\&#x2F; BindingAwareModelMap * * request、session、application； * @author Administrator * *&#x2F; Map方法123456@RequestMapping(&quot;&#x2F;handle01&quot;)public String handle01(Map&lt;String, Object&gt; map) &#123; map.put(&quot;msg&quot;, &quot;HELLO&quot;); System.out.println(&quot;Map的类型:&quot;+map.getClass()); return &quot;success&quot;;&#125; Model方法12345678910&#x2F;** * ②、Model : 一个接口 *&#x2F;@RequestMapping(&quot;&#x2F;handle02&quot;)public String handle02(Model model) &#123; model.addAttribute(&quot;msg&quot;, &quot; HELLO WORLD&quot;); System.out.println(&quot;Model的类型:&quot;+model.getClass()); return &quot;success&quot;; &#125; ModelAndView方法123456789&#x2F;** * ③、ModelMap *&#x2F; @RequestMapping(&quot;&#x2F;handle03&quot;) public String handle03(ModelMap modelMap) &#123; modelMap.addAttribute(&quot;msg&quot;, &quot;HELLO DOG&quot;); System.out.println(&quot;ModelMap的类型:&quot;+modelMap.getClass()); return &quot;success&quot;; &#125; index.jsp123456789101112131415161718192021222324252627&lt;%@ page language&#x3D;&quot;java&quot; contentType&#x3D;&quot;text&#x2F;html; charset&#x3D;UTF-8&quot; pageEncoding&#x3D;&quot;UTF-8&quot;%&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv&#x3D;&quot;Content-Type&quot; content&#x3D;&quot;text&#x2F;html; charset&#x3D;UTF-8&quot;&gt;&lt;title&gt;Insert title here&lt;&#x2F;title&gt;&lt;&#x2F;head&gt;&lt;body&gt;&lt;a href&#x3D;&quot;hello&quot;&gt;HELLO&lt;&#x2F;a&gt;&lt;br&#x2F;&gt;&lt;!-- SpringMVC如何携带数据进来 --&gt;&lt;br&#x2F;&gt;&lt;a href&#x3D;&quot;handle01&quot;&gt;handle01:Map方法&lt;&#x2F;a&gt;&lt;br&#x2F;&gt;&lt;a href&#x3D;&quot;handle02&quot;&gt;handle02:Model方法&lt;&#x2F;a&gt;&lt;br&#x2F;&gt;&lt;a href&#x3D;&quot;handle03&quot;&gt;handle03:ModelMap方法&lt;&#x2F;a&gt;&lt;br&#x2F;&gt;&lt;a href&#x3D;&quot;handle04&quot;&gt;handle04:ModelAndView方法&lt;&#x2F;a&gt;&lt;br&#x2F;&gt;&lt;&#x2F;body&gt;&lt;&#x2F;html&gt; success.jsp12345678910111213141516&lt;%@ page language&#x3D;&quot;java&quot; contentType&#x3D;&quot;text&#x2F;html; charset&#x3D;UTF-8&quot; pageEncoding&#x3D;&quot;UTF-8&quot;%&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv&#x3D;&quot;Content-Type&quot; content&#x3D;&quot;text&#x2F;html; charset&#x3D;UTF-8&quot;&gt;&lt;title&gt;Insert title here&lt;&#x2F;title&gt;&lt;&#x2F;head&gt;&lt;body&gt;&lt;h1&gt;成功了&lt;&#x2F;h1&gt;pageContext: $&#123;pageScope.msg &#125;&lt;br&#x2F;&gt;在request域中获取 : $&#123;requestScope.msg &#125;&lt;br&#x2F;&gt;在session域中获取 : $&#123;sessionScope.msg &#125;&lt;br&#x2F;&gt;在application域中获取 : $&#123;applicationScope.msg &#125;&lt;br&#x2F;&gt;&lt;&#x2F;body&gt;&lt;&#x2F;html&gt; 二、ModelAndView12345678&#x2F;** * 2、方法的返回值可以变为ModelAndView类型； 既包含视图信息（页面地址）也包含模型数据（给页面带的数据）； 而且数据是放在请求域中； 各种域： request、session、application； request用的最多也最好，当请求结束，域中的数据也被清除有利于维护系统性能 *&#x2F; 测试实例1234567@RequestMapping(&quot;&#x2F;handle04&quot;) public ModelAndView handle04() &#123; &#x2F;&#x2F;之前的返回值(&quot;success&quot;)就是我们说的视图名;视图解析器是会帮我们拼串得到页面的真实地址 ModelAndView view &#x3D; new ModelAndView(&quot;success&quot;); view.addObject(&quot;msg&quot;, &quot;HELLO TOMCAT &quot;); return view; &#125; 三、利用注解给Session域携带数据12345678910111213141516&#x2F;** * 3、SpringMVC提供了一种可以临时给Session域中保存数据的方式； * 使用一个注解 @SessionAttributes() 注：这个注解只能标在类上 * @SessionAttributes(value&#x3D;&#123;&quot;msg&quot;,&quot;haha&quot;&#125;,types &#x3D; &#123;String.class&#125;): * 给BindingAwareModelMap或ModelAndView中保存的数据，同时给Session中放一份 * value指定保存数据时要给session中放的key,即key为msg或haha时才会保存一份到Session中， * key为其他时不保存 * * value&#x3D;&#123;&quot;msg&quot;,&quot;haha&quot;&#125;:保存指定key * types &#x3D; &#123;String.class&#125;:只要保存的是String类型的数据都缓存到Session中 * * 后来推荐不用@SessionAttributes注解，可能会引发异常，给Session中放数据 * 推荐使用原生API * *&#x2F; 测试实例1234567891011121314151617181920212223242526272829303132333435363738394041424344@SessionAttributes(value&#x3D;&quot;msg&quot;)@Controllerpublic class OutputController &#123; @RequestMapping(&quot;&#x2F;handle01&quot;) public String handle01(Map&lt;String, Object&gt; map) &#123; map.put(&quot;msg&quot;, &quot;HELLO&quot;); System.out.println(&quot;Map的类型:&quot;+map.getClass()); return &quot;success&quot;; &#125; &#x2F;** * ②、Model : 一个接口 *&#x2F; @RequestMapping(&quot;&#x2F;handle02&quot;) public String handle02(Model model) &#123; model.addAttribute(&quot;msg&quot;, &quot; HELLO WORLD&quot;); System.out.println(&quot;Model的类型:&quot;+model.getClass()); return &quot;success&quot;; &#125; &#x2F;** * ③、ModelMap *&#x2F; @RequestMapping(&quot;&#x2F;handle03&quot;) public String handle03(ModelMap modelMap) &#123; modelMap.addAttribute(&quot;msg&quot;, &quot;HELLO DOG&quot;); System.out.println(&quot;ModelMap的类型:&quot;+modelMap.getClass()); return &quot;success&quot;; &#125; @RequestMapping(&quot;&#x2F;handle04&quot;) public ModelAndView handle04() &#123; &#x2F;&#x2F;之前的返回值(&quot;success&quot;)就是我们说的视图名;视图解析器是会帮我们拼串得到页面的真实地址 &#x2F;&#x2F;ModelAndView view &#x3D; new ModelAndView(&quot;success&quot;); ModelAndView view &#x3D; new ModelAndView(); view.setViewName(&quot;success&quot;); view.addObject(&quot;msg&quot;, &quot;HELLO TOMCAT &quot;); return view; &#125; &#125; 四、ModelAttribute注解4.1、全字段更新引发的问题以及解决思想12345678910111213141516171819202122232425262728293031323334353637383940&#x2F;** * 测试ModelAttribute注解； * 使用场景：书城的图书修改为例； * 1）页面端； * 显示要修改的图书的信息，图书的所有字段都在 * 2）servlet收到修改请求，调用dao； * String sql&#x3D;&quot;update bs_book set title&#x3D;?, * author&#x3D;?,price&#x3D;?, * sales&#x3D;?,stock&#x3D;?,img_path&#x3D;? * where id&#x3D;?&quot;; * 3）实际场景？ * 并不是全字段修改；只会修改部分字段，以修改用户信息为例； * username password address; * 1）、不修改的字段可以在页面进行展示但是不要提供修改输入框； * 2）、为了简单，Controller直接在参数位置来写Book对象 * 3）、SpringMVC为我们自动封装book；（没有带的值是null） * 4）、如果接下来调用了一个全字段更新的dao操作；会将其他的字段可能变为null； * sql &#x3D; &quot;update bs_book set&quot; * if(book.getBookName())&#123; * sql +&#x3D;&quot;bookName&#x3D;?,&quot; * &#125; * if(book.getPrice())&#123; * sql +&#x3D;&quot;price&#x3D;?&quot; * &#125; * * 4）、如何能保证全字段更新的时候，只更新了页面携带的数据； * 1）、修改dao；代价大？ * 2）、Book对象是如何封装的？ * 1）、SpringMVC创建一个book对象，每个属性都有默认值，bookName就是null； * 1、让SpringMVC别创建book对象，直接从数据库中先取出一个id&#x3D;100的book对象的信息 * 2、Book [id&#x3D;100, bookName&#x3D;西游记, author&#x3D;张三, stock&#x3D;12, sales&#x3D;32, price&#x3D;98.98] * * 2）、将请求中所有与book对应的属性一一设置过来； * 3、使用刚才从数据库取出的book对象，给它 的里面设置值；（请求参数带了哪些值就覆盖之前的值） * 4、带了的字段就改为携带的值，没带的字段就保持之前的值 * 3）、调用全字段更新就有问题； * 5、将之前从数据库中查到的对象，并且封装了请求参数的对象。进行保存； * * @author *&#x2F; 也就是说，每次我们更新的时候是新见一个对象，去数据库更新而不是，更新数据库已存在的信息，这种更新会覆盖之前数据库已有的字段，而在这次更新操作中没有赋值的字段会变成null。 我们如何让修改的数据不是新建的而是从数据库中取出信息修改后再提交到数据库12345678910111213&#x2F;** * 1）、SpringMVC要封装请求参数的Book对象不应该是自己new出来的。 * 而应该是【从数据库中】拿到的准备好的对象 * 2）、再来使用这个对象封装请求参数 * * @ModelAttribute： * 参数：取出刚才保存的数据 * 方法位置：这个方法就会提前于目标方法先运行； * 1)我们可以在这里提前查出数据库中图书的信息 * 2)将这个图书信息保存起来（方便下一个方法还能使用） * * 参数的map：BindingAwareModelMap *&#x2F; 测试实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package com.lizhi.controller;import java.util.Map;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.ModelAttribute;import org.springframework.web.bind.annotation.RequestMapping;@Controllerpublic class ModelAttributeTestController &#123; private Object o1;&#x2F;&#x2F;判断两个方法里的map是不是同一个 private Object o2; private Object b1;&#x2F;&#x2F;判断ModelAttribute的值能否传到update方法 private Object b2; &#x2F;** * 全字段更新会出现问题的方法 * @param book * @return *&#x2F; @RequestMapping(&quot;&#x2F;updateBook&quot;) &#x2F;&#x2F;@ModelAttribute的值取自这里map.put(&quot;haha&quot;, book); public String updateBook(@ModelAttribute(&quot;haha&quot;)Book book,Map&lt;String, Object&gt; map) &#123; o2&#x3D;map; b2&#x3D;book; System.out.println(&quot;更新方法里的Map类型:&quot;+map.getClass()); System.out.println(&quot;o1&#x3D;&#x3D;o2?&quot;+(o1&#x3D;&#x3D;o2)); System.out.println(&quot;b1&#x3D;&#x3D;b2?&quot;+(b1&#x3D;&#x3D;b2)); System.out.println(&quot;页面提交过来的图书信息:&quot;+book); return &quot;success&quot;; &#125; &#x2F;** * 1）、SpringMVC要封装请求参数的Book对象不应该是自己new出来的。 * 而应该是【从数据库中】拿到的准备好的对象 * 2）、再来使用这个对象封装请求参数 * * @ModelAttribute： * 参数：取出刚才保存的数据 * 方法位置：这个方法就会提前于目标方法先运行； * 1)我们可以在这里提前查出数据库中图书的信息 * 2)将这个图书信息保存起来（方便下一个方法还能使用） * * 参数的map：BindingAwareModelMap *&#x2F; &#x2F;** * ModelAttribute可以告诉SpringMVC不要new这个book了我刚才保存了一个book； * 哪个就是从数据库中查询出来的；用我这个book?@ModelAttribute(&quot;haha&quot;) * * * 同都是BindingAwareModelMap * @param book * @return *&#x2F; @ModelAttribute public void MyModelAttribute(Map&lt;String, Object&gt; map) &#123; &#x2F;&#x2F;这里没有连接数据库。假装连上了嘻嘻 Book book &#x3D; new Book(100,&quot;西游记&quot;,&quot;吴承恩&quot;,98,10,98.98); System.out.println(&quot;数据库中查到的图书信息是:&quot;+book); map.put(&quot;haha&quot;, book); System.out.println(&quot;ModelAttribute的方法......查询到了图书并保存了&quot;); b1&#x3D;book; o1&#x3D;map; System.out.println(&quot;ModelAttribute里map的类型:&quot;+map.getClass()); &#125;&#125; 运行结果 对于结果我们可以看到，ModelAttribute在update目标方法之前运行，且两个方法里调用的携带数据的map方法是来自同一个类并且是同一个(o1==o2)，并且ModelAttribute里的值是传到update目标方法，并且是一个值(b1==b2) 原理图 1234567891011121314151617两件事1）、运行流程简单版；2）、确定方法每个参数的值； 1）、标注解：保存注解的信息；最终得到这个注解应该对应解析的值； 2）、没标注解： 1）、看是否是原生API； 2）、看是否是Model或者是Map，xxxx 3）、都不是，看是否是简单类型；paramName； 4）、给attrName赋值；attrName（参数标了@ModelAttribute(&quot;&quot;)就是指定的，没标就是&quot;&quot;） 确定自定义类型参数： 1）、attrName使用参数的类型首字母小写；或者使用之前@ModelAttribute(&quot;&quot;)的值 2）、先看隐含模型中有每个这个attrName作为key对应的值；如果有就从隐含模型中获取并赋值 3）、看是否是@SessionAttributes(value&#x3D;&quot;haha&quot;)；标注的属性，如果是从session中拿； 如果拿不到就会抛异常； 4）、不是@SessionAttributes标注的，利用反射创建一个对象； 5）、拿到之前创建好的对象，使用数据绑定器(WebDataBinder)将请求中的每个数据绑定到这个对象中； 五、测试实例源码 /4.SpringMVC_output/WebContent/WEB-INF/pages/success.jsp12345678910111213141516&lt;%@ page language&#x3D;&quot;java&quot; contentType&#x3D;&quot;text&#x2F;html; charset&#x3D;UTF-8&quot; pageEncoding&#x3D;&quot;UTF-8&quot;%&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv&#x3D;&quot;Content-Type&quot; content&#x3D;&quot;text&#x2F;html; charset&#x3D;UTF-8&quot;&gt;&lt;title&gt;Insert title here&lt;&#x2F;title&gt;&lt;&#x2F;head&gt;&lt;body&gt;&lt;h1&gt;成功了&lt;&#x2F;h1&gt;pageContext: $&#123;pageScope.msg &#125;&lt;br&#x2F;&gt;在request域中获取 : $&#123;requestScope.msg &#125;&lt;br&#x2F;&gt;在session域中获取 : $&#123;sessionScope.msg &#125;&lt;br&#x2F;&gt;在application域中获取 : $&#123;applicationScope.msg &#125;&lt;br&#x2F;&gt;&lt;&#x2F;body&gt;&lt;&#x2F;html&gt; /4.SpringMVC_output/WebContent/WEB-INF/springDispatcherServlet-servlet.xml12345678910111213141516&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xmlns:context&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans-2.0.xsd http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&#x2F;spring-context-4.3.xsd&quot;&gt; &lt;context:component-scan base-package&#x3D;&quot;com.lizhi&quot;&gt;&lt;&#x2F;context:component-scan&gt; &lt;bean class&#x3D;&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name&#x3D;&quot;prefix&quot; value&#x3D;&quot;&#x2F;WEB-INF&#x2F;pages&#x2F;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;suffix&quot; value&#x3D;&quot;.jsp&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt; &lt;&#x2F;beans&gt; /4.SpringMVC_output/WebContent/WEB-INF/web.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;web-app xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xmlns&#x3D;&quot;http:&#x2F;&#x2F;xmlns.jcp.org&#x2F;xml&#x2F;ns&#x2F;javaee&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;xmlns.jcp.org&#x2F;xml&#x2F;ns&#x2F;javaee http:&#x2F;&#x2F;xmlns.jcp.org&#x2F;xml&#x2F;ns&#x2F;javaee&#x2F;web-app_4_0.xsd&quot; id&#x3D;&quot;WebApp_ID&quot; version&#x3D;&quot;4.0&quot;&gt; &lt;display-name&gt;4.SpringMVC_output&lt;&#x2F;display-name&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.html&lt;&#x2F;welcome-file&gt; &lt;welcome-file&gt;index.htm&lt;&#x2F;welcome-file&gt; &lt;welcome-file&gt;index.jsp&lt;&#x2F;welcome-file&gt; &lt;welcome-file&gt;default.html&lt;&#x2F;welcome-file&gt; &lt;welcome-file&gt;default.htm&lt;&#x2F;welcome-file&gt; &lt;welcome-file&gt;default.jsp&lt;&#x2F;welcome-file&gt; &lt;&#x2F;welcome-file-list&gt; &lt;!-- The front controller of this Spring Web application, responsible for handling all application requests --&gt; &lt;servlet&gt; &lt;!-- springDispatcherServlet必须和SpringMVC配置文件的名一样，不指定路径就要在WEB-INF目录下建配置文件 且配置文件名得是XXX-servlet.xml --&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;&#x2F;servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;&#x2F;servlet-class&gt; &lt;load-on-startup&gt;1&lt;&#x2F;load-on-startup&gt; &lt;&#x2F;servlet&gt; &lt;!-- Map all requests to the DispatcherServlet for handling --&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;&#x2F;servlet-name&gt; &lt;url-pattern&gt;&#x2F;&lt;&#x2F;url-pattern&gt; &lt;&#x2F;servlet-mapping&gt; &lt;!-- 字符编码过滤器 --&gt; &lt;!-- 配置一个字符编码的Filter 注意:这个字符编码Filter必须再其他Filter之前，不然无法解决问题 --&gt; &lt;filter&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;&#x2F;filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;&#x2F;filter-class&gt; &lt;!-- 由于该方法支持多种编码，在下面指定我们要用的编码形式 --&gt; &lt;!-- init-param初始化参数 --&gt; &lt;!-- encoding : 解决POST乱码 --&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;&#x2F;param-name&gt; &lt;param-value&gt;UTF-8&lt;&#x2F;param-value&gt; &lt;&#x2F;init-param&gt; &lt;!-- forceEncoding : 顺手解决响应乱码 --&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;&#x2F;param-name&gt; &lt;param-value&gt;true&lt;&#x2F;param-value&gt; &lt;&#x2F;init-param&gt; &lt;&#x2F;filter&gt; &lt;!-- 配置所有请求采用此编码 --&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;&#x2F;filter-name&gt; &lt;url-pattern&gt;&#x2F;*&lt;&#x2F;url-pattern&gt; &lt;&#x2F;filter-mapping&gt; &lt;&#x2F;web-app&gt; /4.SpringMVC_output/WebContent/index.jsp123456789101112131415161718192021222324252627282930313233343536373839&lt;%@ page language&#x3D;&quot;java&quot; contentType&#x3D;&quot;text&#x2F;html; charset&#x3D;UTF-8&quot; pageEncoding&#x3D;&quot;UTF-8&quot;%&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv&#x3D;&quot;Content-Type&quot; content&#x3D;&quot;text&#x2F;html; charset&#x3D;UTF-8&quot;&gt;&lt;title&gt;Insert title here&lt;&#x2F;title&gt;&lt;&#x2F;head&gt;&lt;body&gt;&lt;a href&#x3D;&quot;hello&quot;&gt;HELLO&lt;&#x2F;a&gt;&lt;br&#x2F;&gt;&lt;!-- SpringMVC如何携带数据进来 --&gt;&lt;br&#x2F;&gt;&lt;a href&#x3D;&quot;handle01&quot;&gt;handle01:Map方法&lt;&#x2F;a&gt;&lt;br&#x2F;&gt;&lt;a href&#x3D;&quot;handle02&quot;&gt;handle02:Model方法&lt;&#x2F;a&gt;&lt;br&#x2F;&gt;&lt;a href&#x3D;&quot;handle03&quot;&gt;handle03:ModelMap方法&lt;&#x2F;a&gt;&lt;br&#x2F;&gt;&lt;a href&#x3D;&quot;handle04&quot;&gt;handle04:ModelAndView方法&lt;&#x2F;a&gt;&lt;br&#x2F;&gt;&lt;h1&gt;修改图书-不能修改书名:&lt;&#x2F;h1&gt;&lt;form action&#x3D;&quot;updateBook&quot; method&#x3D;&quot;post&quot;&gt; &lt;!-- 指定修改1号 --&gt; &lt;input type&#x3D;&quot;hidden&quot; name&#x3D;&quot;id&quot; value&#x3D;&quot;1&quot;&#x2F;&gt; 书名: 西游记：&lt;br&#x2F;&gt; 作者: &lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;author&quot; &#x2F;&gt;&lt;br&#x2F;&gt; 价格: &lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;price&quot; &#x2F;&gt;&lt;br&#x2F;&gt; 库存: &lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;stock&quot; &#x2F;&gt;&lt;br&#x2F;&gt; 销量: &lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;sales&quot; &#x2F;&gt;&lt;br&#x2F;&gt; &lt;input type&#x3D;&quot;submit&quot; value&#x3D;&quot;修改图书&quot; &#x2F;&gt;&lt;&#x2F;form&gt;&lt;&#x2F;body&gt;&lt;&#x2F;html&gt; com.lizhi.controllerBook.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package com.lizhi.controller;&#x2F;** * 承接传来的属性 * @author Administrator * *&#x2F;public class Book &#123; private Integer id; private String bookName; private String author; private Integer stock; private Integer sales; private double price; public Book() &#123; super(); &#x2F;&#x2F; TODO Auto-generated constructor stub &#125; public Book(Integer id, String bookName, String author, Integer stock, Integer sales, double price) &#123; super(); this.id &#x3D; id; this.bookName &#x3D; bookName; this.author &#x3D; author; this.stock &#x3D; stock; this.sales &#x3D; sales; this.price &#x3D; price; &#125; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id &#x3D; id; &#125; public String getBookName() &#123; return bookName; &#125; public void setBookName(String bookName) &#123; this.bookName &#x3D; bookName; &#125; public String getAuthor() &#123; return author; &#125; public void setAuthor(String author) &#123; this.author &#x3D; author; &#125; public Integer getStock() &#123; return stock; &#125; public void setStock(Integer stock) &#123; this.stock &#x3D; stock; &#125; public Integer getSales() &#123; return sales; &#125; public void setSales(Integer sales) &#123; this.sales &#x3D; sales; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price &#x3D; price; &#125; @Override public String toString() &#123; return &quot;Book [id&#x3D;&quot; + id + &quot;, bookName&#x3D;&quot; + bookName + &quot;, author&#x3D;&quot; + author + &quot;, stock&#x3D;&quot; + stock + &quot;, sales&#x3D;&quot; + sales + &quot;, price&#x3D;&quot; + price + &quot;]&quot;; &#125; &#125; HelloController.java123456789101112131415package com.lizhi.controller;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;@Controllerpublic class HelloController &#123; @RequestMapping(&quot;&#x2F;hello&quot;) public String hello() &#123; System.out.println(&quot;hello......&quot;); return &quot;success&quot;; &#125;&#125; ModelAttributeTestController.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126package com.lizhi.controller;import java.util.Map;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.ModelAttribute;import org.springframework.web.bind.annotation.RequestMapping;&#x2F;** * 测试ModelAttribute注解； * 使用场景：书城的图书修改为例； * 1）页面端； * 显示要修改的图书的信息，图书的所有字段都在 * 2）servlet收到修改请求，调用dao； * String sql&#x3D;&quot;update bs_book set title&#x3D;?, * author&#x3D;?,price&#x3D;?, * sales&#x3D;?,stock&#x3D;?,img_path&#x3D;? * where id&#x3D;?&quot;; * 3）实际场景？ * 并不是全字段修改；只会修改部分字段，以修改用户信息为例； * username password address; * 1）、不修改的字段可以在页面进行展示但是不要提供修改输入框； * 2）、为了简单，Controller直接在参数位置来写Book对象 * 3）、SpringMVC为我们自动封装book；（没有带的值是null） * 4）、如果接下来调用了一个全字段更新的dao操作；会将其他的字段可能变为null； * sql &#x3D; &quot;update bs_book set&quot; * if(book.getBookName())&#123; * sql +&#x3D;&quot;bookName&#x3D;?,&quot; * &#125; * if(book.getPrice())&#123; * sql +&#x3D;&quot;price&#x3D;?&quot; * &#125; * * 4）、如何能保证全字段更新的时候，只更新了页面携带的数据； * 1）、修改dao；代价大？ * 2）、Book对象是如何封装的？ * 1）、SpringMVC创建一个book对象，每个属性都有默认值，bookName就是null； * 1、让SpringMVC别创建book对象，直接从数据库中先取出一个id&#x3D;100的book对象的信息 * 2、Book [id&#x3D;100, bookName&#x3D;西游记, author&#x3D;张三, stock&#x3D;12, sales&#x3D;32, price&#x3D;98.98] * * 2）、将请求中所有与book对应的属性一一设置过来； * 3、使用刚才从数据库取出的book对象，给它 的里面设置值；（请求参数带了哪些值就覆盖之前的值） * 4、带了的字段就改为携带的值，没带的字段就保持之前的值 * 3）、调用全字段更新就有问题； * 5、将之前从数据库中查到的对象，并且封装了请求参数的对象。进行保存； * * @author *&#x2F;&#x2F;** * 目标方法 * &#x2F;&#x2F;bookDao.update(book); &#x2F;&#x2F;页面提交过来的图书信息:Book [id&#x3D;1, bookName&#x3D;null, author&#x3D;safd, stock&#x3D;34, sales&#x3D;21, price&#x3D;21.0] &#x2F;**调用这个sql语句 * String sql&#x3D;&quot;update bs_book set bookName&#x3D;?, author&#x3D;?,price&#x3D;?, sales&#x3D;?,stock&#x3D;?,img_path&#x3D;? where id&#x3D;?&quot;; * @param book * @return *&#x2F;@Controllerpublic class ModelAttributeTestController &#123; private Object o1;&#x2F;&#x2F;判断两个方法里的map是不是同一个 private Object o2; private Object b1;&#x2F;&#x2F;判断ModelAttribute的值能否传到update方法 private Object b2; &#x2F;** * 全字段更新会出现问题的方法 * @param book * @return *&#x2F; @RequestMapping(&quot;&#x2F;updateBook&quot;) &#x2F;&#x2F;@ModelAttribute的值取自这里map.put(&quot;haha&quot;, book); public String updateBook(@ModelAttribute(&quot;haha&quot;)Book book,Map&lt;String, Object&gt; map) &#123; o2&#x3D;map; b2&#x3D;book; System.out.println(&quot;更新方法里的Map类型:&quot;+map.getClass()); System.out.println(&quot;o1&#x3D;&#x3D;o2?&quot;+(o1&#x3D;&#x3D;o2)); System.out.println(&quot;b1&#x3D;&#x3D;b2?&quot;+(b1&#x3D;&#x3D;b2)); System.out.println(&quot;页面提交过来的图书信息:&quot;+book); return &quot;success&quot;; &#125; &#x2F;** * 1）、SpringMVC要封装请求参数的Book对象不应该是自己new出来的。 * 而应该是【从数据库中】拿到的准备好的对象 * 2）、再来使用这个对象封装请求参数 * * @ModelAttribute： * 参数：取出刚才保存的数据 * 方法位置：这个方法就会提前于目标方法先运行； * 1)我们可以在这里提前查出数据库中图书的信息 * 2)将这个图书信息保存起来（方便下一个方法还能使用） * * 参数的map：BindingAwareModelMap *&#x2F; &#x2F;** * ModelAttribute可以告诉SpringMVC不要new这个book了我刚才保存了一个book； * 哪个就是从数据库中查询出来的；用我这个book?@ModelAttribute(&quot;haha&quot;) * * * 同都是BindingAwareModelMap * @param book * @return *&#x2F; @ModelAttribute public void MyModelAttribute(Map&lt;String, Object&gt; map) &#123; &#x2F;&#x2F;这里没有连接数据库。假装连上了嘻嘻 Book book &#x3D; new Book(100,&quot;西游记&quot;,&quot;吴承恩&quot;,98,10,98.98); System.out.println(&quot;数据库中查到的图书信息是:&quot;+book); map.put(&quot;haha&quot;, book); System.out.println(&quot;ModelAttribute的方法......查询到了图书并保存了&quot;); b1&#x3D;book; o1&#x3D;map; System.out.println(&quot;ModelAttribute里map的类型:&quot;+map.getClass()); &#125;&#125; OutputController.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112package com.lizhi.controller;import java.util.Map;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.ui.ModelMap;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.SessionAttributes;import org.springframework.web.servlet.ModelAndView;&#x2F;** * SpringMVC除过在方法上传入原生API的request和session外还能怎样把数据带给页面? * 1、在方法处传入Map、或者Model或者ModelMap。给这些参数里面保存的所有数据都会放在域中。 * 可以把数据带给页面。注意：这三种方法携带的数据都在请求域中(request) Map，Model，ModelMap：最终都是BindingAwareModelMap在工作； 相当于给BindingAwareModelMap中保存的东西都会被放在请求域中； org.springframework.validation.support.BindingAwareModelMap Map(interface(jdk里的)) Model(interface(spring里的)) || &#x2F;&#x2F; || &#x2F;&#x2F; \\&#x2F; &#x2F;&#x2F; ModelMap(clas) &#x2F;&#x2F; \\\\ &#x2F;&#x2F; \\\\ &#x2F;&#x2F; ExtendedModelMap || \\&#x2F; BindingAwareModelMap * * request、session、application； * @author Administrator * *&#x2F;&#x2F;** * ①、Map方法 * @author Administrator * *&#x2F;@SessionAttributes(value&#x3D; &#123;&quot;msg&quot;,&quot;haha&quot;&#125;,types &#x3D; &#123;String.class&#125;)@Controllerpublic class OutputController &#123; @RequestMapping(&quot;&#x2F;handle01&quot;) public String handle01(Map&lt;String, Object&gt; map) &#123; map.put(&quot;msg&quot;, &quot;HELLO&quot;); System.out.println(&quot;Map的类型:&quot;+map.getClass()); return &quot;success&quot;; &#125; &#x2F;** * ②、Model : 一个接口 *&#x2F; @RequestMapping(&quot;&#x2F;handle02&quot;) public String handle02(Model model) &#123; model.addAttribute(&quot;msg&quot;, &quot; HELLO WORLD&quot;); System.out.println(&quot;Model的类型:&quot;+model.getClass()); return &quot;success&quot;; &#125; &#x2F;** * ③、ModelMap *&#x2F; @RequestMapping(&quot;&#x2F;handle03&quot;) public String handle03(ModelMap modelMap) &#123; modelMap.addAttribute(&quot;msg&quot;, &quot;HELLO DOG&quot;); System.out.println(&quot;ModelMap的类型:&quot;+modelMap.getClass()); return &quot;success&quot;; &#125; &#x2F;** * 2、方法的返回值可以变为ModelAndView类型； 既包含视图信息（页面地址）也包含模型数据（给页面带的数据）； 而且数据是放在请求域中； 各种域： request、session、application； request用的最多也最好，当请求结束，域中的数据也被清除有利于维护系统性能 *&#x2F; @RequestMapping(&quot;&#x2F;handle04&quot;) public ModelAndView handle04() &#123; &#x2F;&#x2F;之前的返回值(&quot;success&quot;)就是我们说的视图名;视图解析器是会帮我们拼串得到页面的真实地址 &#x2F;&#x2F;ModelAndView view &#x3D; new ModelAndView(&quot;success&quot;); ModelAndView view &#x3D; new ModelAndView(); view.setViewName(&quot;success&quot;); view.addObject(&quot;msg&quot;, &quot;HELLO TOMCAT &quot;); return view; &#125; &#x2F;** * 3、SpringMVC提供了一种可以临时给Session域中保存数据的方式； * 使用一个注解 @SessionAttributes() 注：这个注解只能标在类上 * @SessionAttributes(value&#x3D;&#123;&quot;msg&quot;,&quot;haha&quot;&#125;,types &#x3D; &#123;String.class&#125;): * 给BindingAwareModelMap或ModelAndView中保存的数据，同时给Session中放一份 * value指定保存数据时要给session中放的key,即key为msg或haha时才会保存一份到Session中， * key为其他时不保存 * * value&#x3D;&#123;&quot;msg&quot;,&quot;haha&quot;&#125;:保存指定key * types &#x3D; &#123;String.class&#125;:只要保存的是String类型的数据都缓存到Session中 * * 后来推荐不用@SessionAttributes注解，可能会引发异常，给Session中放数据 * 推荐使用原生API * *&#x2F; &#125;","categories":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"},{"name":"SpringMVC-数据输出","slug":"软件开发框架/SpringMVC-数据输出","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/SpringMVC-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%87%BA/"}],"tags":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/tags/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"},{"name":"SpringMVC-数据输出","slug":"SpringMVC-数据输出","permalink":"http://www.studyz.club/tags/SpringMVC-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%87%BA/"}]},{"title":"软件开发框架之SpringMVC-请求处理(入参)","slug":"软件开发框架之SpringMVC-请求处理入参","date":"2019-11-19T12:51:18.017Z","updated":"2019-11-20T01:02:24.216Z","comments":true,"path":"posts/12dcd29a/","link":"","permalink":"http://www.studyz.club/posts/12dcd29a/","excerpt":"","text":"一、如何获得请求 1.1、 @RequestParam 1.2、@RequestHeader 1.3、@CookieValue 二、POJO,级联属性 三、原生API的请求 四、中文乱码问题 五、测试实例源码 一、如何获得请求1.1、 @RequestParam 1234567891011121314151617181920212223&#x2F;** * SpringMVC如何获取请求带来的各种信息(三个注解) * * 默认方式获取请求参数(使用@RequestMapping) * 直接给方法入参写一个和请求参数名相同的变量，这个变量就来接受请求参数的值； * URL带什么就是什么值 * 1、@RequestParam : 获取请求参数,默认参数是必须带的 * @RequestParam(&quot;user&quot;) : * 即 username&#x3D;request.getParameter(&quot;user&quot;) * value ： 指定获取的参数的Key * required : 这个参数是否必须得，意思是这个值不带不会报错 * defaultValue : 指定默认值，即如果没带参数，默认是什么，之前默认值是null * 区别 * @RequestParam(&quot;user&quot;) : ?user&#x3D;admin * @PathVariable(&quot;user&quot;) : 获取路径上的值主要是占位符，&#x2F;book&#x2F;&#123;user&#125;?user&#x3D;admin * &#x2F;book&#x2F;&#123;user&#125;?user&#x3D;admin * &#123;user&#125; -&gt; PathVariable * ?user&#x3D;admin -&gt; RequestParam * 两个一个是管理路径上的参数一个是管理携带的参数，二者互相不能访问到对方的参数 * * * *&#x2F; 测试实例12345@RequestMapping(&quot;&#x2F;handle02&quot;) public String handle02(@RequestParam(value &#x3D; &quot;user&quot;,required &#x3D; false,defaultValue &#x3D; &quot;未带参数&quot;)String username) &#123; System.out.println(&quot;这个变量的值；&quot;+username); return &quot;success&quot;; &#125; 1.2、@RequestHeader123456789101112131415161718192021222324&#x2F;** * 2、@RequestHeader * request.getHeader(&quot;User-Agent&quot;); * @RequestHeader(&quot;User-Agent&quot;)String userAgent * 相当于 userAgent &#x3D; request.getHeader(&quot;User-Agent&quot;); * 请求头这么多都可以写 * Host: localhost:8080 * User-Agent: Mozilla&#x2F;5.0 (Windows NT 6.1; Win64; x64; rv:70.0) Gecko&#x2F;20100101 Firefox&#x2F;70.0 * Accept: * Accept-Language: zh-CN,zh;q&#x3D;0.8,zh-TW;q&#x3D;0.7,zh-HK;q&#x3D;0.5,en-US;q&#x3D;0.3,en;q&#x3D;0.2 * Accept-Encoding: gzip, deflate * Referer: http:&#x2F;&#x2F;localhost:8080&#x2F;3.SpringMVC_request&#x2F;index.jsp * Connection: keep-alive * Cookie: JSESSIONID&#x3D;3BF8EA368A3847CB387AF85822B6A11D; Webstorm-2f8f75da&#x3D;87e0117e-f750-4f39-8dac-88bee5e6c49d; _xsrf&#x3D;2|61c17969|b92c2c2c65739014c32f1328c1db12b1|1573649253; username-localhost-8889&#x3D;&quot;2|1:0|10:1573687317|23:username-localhost-8889|44:MzA2MTEyNjQ5MTRhNDU2Mjk3Nzg1M2E0ZmFjNDRhNDE&#x3D;|3fc35b809b91f316e3c271928cb7456bdcd43a3903664be69d1f349cd2cba191&quot;; username-localhost-8890&#x3D;&quot;2|1:0|10:1573650131|23:username-localhost-8890|44:MTg3ZWFlYjIxNzNkNGMyODljMzU0MzkyZmEyM2E3NDQ&#x3D;|9fa7610ceafa454743ec6e9a4d61e6f5aa0dc8033d1c99443c75186b4ea7ce50&quot;; username-localhost-8888&#x3D;&quot;2|1:0|10:1573687724|23:username-localhost-8888|44:NTI3NmIzNWFkYjIyNGViM2IyNDdlOWZjZmUyNzg2MDE&#x3D;|0673e2f877b17955e35fe690341b4556cb00e760f40499317b095c68157edd62&quot; * Upgrade-Insecure-Requests: 1 * Cache-Control: max-age&#x3D;0 * * 如果带的值是请求头中没有的会报错，与@RequestParam一样也有三个参数 * value ： 指定获取的参数的Key * required : 指定这个参数是否带，请求头是否带这个参数，不带也不报错 * defaultValue : 指定默认值，即如果没带参数，默认是什么，之前默认值是null * * @return *&#x2F; 测试实例源码12345678@RequestMapping(&quot;&#x2F;handle03&quot;) public String handle03(@RequestParam(value &#x3D; &quot;user&quot;,required &#x3D; false,defaultValue &#x3D; &quot;未带参数&quot;)String username ,@RequestHeader(value&#x3D;&quot;User-Agent&quot;)String userAgent,@RequestHeader(&quot;Cookie&quot;)String cookie) &#123; System.out.println(&quot;这个变量的值；&quot;+username); System.out.println(&quot;请求头中的浏览器信息:&quot;+userAgent); System.out.println(&quot;请求头中的Cookie:&quot;+cookie); return &quot;success&quot;; &#125; 1.3、@CookieValue1234567891011121314&#x2F;** * 3、@CookieValue : 获取某个cookie的值 * 以前的操作获取某个cookie * Cookie[] cookies &#x3D; request.getCookies(); * for(Cookie c:cookies)&#123; * if(c.getName().equals(&quot;JSESSIONID&quot;))&#123; * String cv &#x3D; c.getValue(); * &#125; * &#125; * * value ： 指定获取的参数的Key * required : 指定这个参数是否带，请求头是否带这个参数，不带也不报错 * defaultValue : 指定默认值，即如果没带参数，默认是什么，之前默认值是null *&#x2F; 测试实例12345@RequestMapping(&quot;handle04&quot;) public String handle04(@CookieValue(&quot;JSESSIONID&quot;)String jid) &#123; System.out.println(&quot;Cookie中的jid的值&quot;+jid); return &quot;success&quot;; &#125; 二、POJO,级联属性123456&#x2F;** * 如果我们的请求参数是POJO * SpringMVC会自动为这个POJO进行赋值？ * 1、将POJO中的每一个属性，从request参数中尝试获取出来，并封装即可； * 2、还可以级联封装，级联属性就是属性的属性,看index.jsp里的省市街道 *&#x2F; 测试实例12345@RequestMapping(&quot;&#x2F;book&quot;) public String addBook(Book book) &#123; System.out.println(&quot;我要保存的图书:&quot;+book); return &quot;success&quot;; &#125; index.jsp配置文件12345678910111213&lt;form action&#x3D;&quot;book&quot; method&#x3D;&quot;post&quot;&gt; 书名:&lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;bookName&quot;&#x2F;&gt;&lt;br&#x2F;&gt; 作者:&lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;author&quot;&#x2F;&gt;&lt;br&#x2F;&gt; 价格:&lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;price&quot;&#x2F;&gt;&lt;br&#x2F;&gt; 库存:&lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;stock&quot;&#x2F;&gt;&lt;br&#x2F;&gt; 销量:&lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;sales&quot;&#x2F;&gt;&lt;br&#x2F;&gt; &lt;hr&#x2F;&gt; 省:&lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;address.province&quot; &#x2F;&gt; 市:&lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;address.city&quot; &#x2F;&gt; 街道:&lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;address.street&quot; &#x2F;&gt; &lt;input type&#x3D;&quot;submit&quot; &#x2F;&gt;&lt;&#x2F;form&gt;&lt;br&#x2F;&gt; 三、原生API的请求1234567891011121314151617181920&#x2F;*** * SpringMVC可以直接在参数上写原生API * 只能使用一下几种API * HTTPServletRequest * HttpServletResponse * HTTPSession * * * Java.security.Principal * Locale : 跟国际化有关的区域信息对象 * InputStream : 请求的流(字节流) * ServletInputStream inputStream &#x3D; request.getInputStream(); * OutputStream : 输出流文件下载的时候能用到(字节流) * ServletOutputStream outputStream &#x3D; response.getOutputStream(); * Reader : 从请求中拿到字符流 * BufferedReader reader &#x3D; request.getReader(); * Write : 从Reponse中拿到的往外写的字符流 * PrintWriter writer &#x3D; response.getWriter(); * @throws IOException *&#x2F; 测试实例12345678@RequestMapping(&quot;&#x2F;handle05&quot;) public String handle05(HttpSession session,HttpServletRequest request ,HttpServletResponse response) throws IOException &#123; request.setAttribute(&quot;reqParam&quot;, &quot;我是请求域中的&quot;); session.setAttribute(&quot;sessionParam&quot;, &quot;我是Session域中的&quot;); return &quot;success&quot;; &#125; 四、中文乱码问题12345678910111213141516171819202122232425262728293031323334353637383940414243&#x2F;**提交的数据可能有乱码: * 请求乱码: * GET请求:在服务器配置文件(配置服务器后出来的Servers项目)的server.xml，在里面添加URIEncoding&#x3D;&quot;UTF-8&quot; * &lt;Connector URIEncoding&#x3D;&quot;UTF-8&quot; connectionTimeout&#x3D;&quot;20000&quot; port&#x3D;&quot;8080&quot; protocol&#x3D;&quot;HTTP&#x2F;1.1&quot; redirectPort&#x3D;&quot;8443&quot;&#x2F;&gt; * POST请求:在第一次获取请求参数之前设置 * ①、request.setCharacterEncoding(&quot;UTF-8&quot;); * ②、自己写一个filter,SpringMVC提供了一个filter,在Open Type里搜CharacterEncodingFilter,然后再web.xml中配置 * &lt;!-- 配置一个字符编码的Filter 注意:这个字符编码Filter必须再其他Filter之前，不然无法解决问题 --&gt; &lt;filter&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;&#x2F;filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;&#x2F;filter-class&gt; &lt;!-- 由于该方法支持多种编码，在下面指定我们要用的编码形式 --&gt; &lt;!-- init-param初始化参数 --&gt; &lt;!-- encoding : 解决POST乱码 --&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;&#x2F;param-name&gt; &lt;param-value&gt;UTF-8&lt;&#x2F;param-value&gt; &lt;&#x2F;init-param&gt; &lt;!-- forceEncoding : 顺手解决响应乱码 --&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;&#x2F;param-name&gt; &lt;param-value&gt;true&lt;&#x2F;param-value&gt; &lt;&#x2F;init-param&gt; &lt;&#x2F;filter&gt; &lt;!-- 配置所有请求采用此编码 --&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;&#x2F;filter-name&gt; &lt;url-pattern&gt;&#x2F;*&lt;&#x2F;url-pattern&gt; &lt;&#x2F;filter-mapping&gt; &lt;!-- 使用SpringMVC前端控制器写完就直接写字符编码过滤器 Tomcat一装上就，上手就像该server.xml里的8080处添加URIEncoding&#x3D;&quot;UTF-8&quot; --&gt; * * (我们REST风格发的DELETE，PUT请求都是由这两个转换来的，前面REST里的Filter原理有介绍) * 响应乱码: * reponse.setContentType(&quot;Text&#x2F;html;charset&#x3D;utf-8&quot;); * * @param book * @return *&#x2F; 五、测试实例源码 /3.SpringMVC_request/WebContent/WEB-INF/springmvc-servlet.xml12345678910111213141516&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xmlns:context&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans.xsd http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&#x2F;spring-context-4.3.xsd&quot;&gt; &lt;context:component-scan base-package&#x3D;&quot;com.lizhi&quot;&gt;&lt;&#x2F;context:component-scan&gt; &lt;!-- 视图解析器 --&gt; &lt;bean class&#x3D;&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name&#x3D;&quot;prefix&quot; value&#x3D;&quot;&#x2F;WEB-INF&#x2F;pages&#x2F;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;suffix&quot; value&#x3D;&quot;.jsp&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt;&lt;&#x2F;beans&gt; /3.SpringMVC_request/WebContent/WEB-INF/pages/success.jsp1234567891011121314151617&lt;%@ page language&#x3D;&quot;java&quot; contentType&#x3D;&quot;text&#x2F;html; charset&#x3D;UTF-8&quot; pageEncoding&#x3D;&quot;UTF-8&quot;%&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv&#x3D;&quot;Content-Type&quot; content&#x3D;&quot;text&#x2F;html; charset&#x3D;UTF-8&quot;&gt;&lt;title&gt;Insert title here&lt;&#x2F;title&gt;&lt;&#x2F;head&gt;&lt;body&gt;&lt;h1&gt;成功&lt;&#x2F;h1&gt;&lt;!-- 测试原生API --&gt;请求: $&#123;requestScope.reqParam &#125;&lt;br&#x2F;&gt;session: $&#123;sessionScope.sessionParam &#125;&lt;br&#x2F;&gt;&lt;&#x2F;body&gt;&lt;&#x2F;html&gt; /3.SpringMVC_request/WebContent/WEB-INF/web.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;web-app xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xmlns&#x3D;&quot;http:&#x2F;&#x2F;xmlns.jcp.org&#x2F;xml&#x2F;ns&#x2F;javaee&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;xmlns.jcp.org&#x2F;xml&#x2F;ns&#x2F;javaee http:&#x2F;&#x2F;xmlns.jcp.org&#x2F;xml&#x2F;ns&#x2F;javaee&#x2F;web-app_4_0.xsd&quot; id&#x3D;&quot;WebApp_ID&quot; version&#x3D;&quot;4.0&quot;&gt; &lt;display-name&gt;3.SpringMVC_request&lt;&#x2F;display-name&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.html&lt;&#x2F;welcome-file&gt; &lt;welcome-file&gt;index.htm&lt;&#x2F;welcome-file&gt; &lt;welcome-file&gt;index.jsp&lt;&#x2F;welcome-file&gt; &lt;welcome-file&gt;default.html&lt;&#x2F;welcome-file&gt; &lt;welcome-file&gt;default.htm&lt;&#x2F;welcome-file&gt; &lt;welcome-file&gt;default.jsp&lt;&#x2F;welcome-file&gt; &lt;&#x2F;welcome-file-list&gt; &lt;!-- 提示选#DispatcherServlet --&gt; &lt;!-- The front controller of this Spring Web application, responsible for handling all application requests --&gt; &lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;&#x2F;servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;&#x2F;servlet-class&gt; &lt;!-- &lt;init-param&gt; 卸载WEB-INF下面和web.xml一起，就不用配置路径了 &lt;param-name&gt;contextConfigLocation&lt;&#x2F;param-name&gt; &lt;param-value&gt;location&lt;&#x2F;param-value&gt; &lt;&#x2F;init-param&gt; --&gt; &lt;load-on-startup&gt;1&lt;&#x2F;load-on-startup&gt; &lt;&#x2F;servlet&gt; &lt;!-- Map all requests to the DispatcherServlet for handling --&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;&#x2F;servlet-name&gt; &lt;url-pattern&gt;&#x2F;&lt;&#x2F;url-pattern&gt; &lt;&#x2F;servlet-mapping&gt; &lt;!-- 配置一个字符编码的Filter 注意:这个字符编码Filter必须再其他Filter之前，不然无法解决问题 --&gt; &lt;filter&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;&#x2F;filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;&#x2F;filter-class&gt; &lt;!-- 由于该方法支持多种编码，在下面指定我们要用的编码形式 --&gt; &lt;!-- init-param初始化参数 --&gt; &lt;!-- encoding : 解决POST乱码 --&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;&#x2F;param-name&gt; &lt;param-value&gt;UTF-8&lt;&#x2F;param-value&gt; &lt;&#x2F;init-param&gt; &lt;!-- forceEncoding : 顺手解决响应乱码 --&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;&#x2F;param-name&gt; &lt;param-value&gt;true&lt;&#x2F;param-value&gt; &lt;&#x2F;init-param&gt; &lt;&#x2F;filter&gt; &lt;!-- 配置所有请求采用此编码 --&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;&#x2F;filter-name&gt; &lt;url-pattern&gt;&#x2F;*&lt;&#x2F;url-pattern&gt; &lt;&#x2F;filter-mapping&gt; &lt;!-- 使用SpringMVC前端控制器写完就直接写字符编码过滤器 Tomcat一装上就，上手就像该server.xml里的8080处添加URIEncoding&#x3D;&quot;UTF-8&quot; --&gt; &lt;!-- 支持REST风格的Filter --&gt; &lt;filter&gt; &lt;filter-name&gt;HiddenHttpMethodFilter&lt;&#x2F;filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.HiddenHttpMethodFilter&lt;&#x2F;filter-class&gt; &lt;&#x2F;filter&gt; &lt;!-- 拦截哪些请求 &#x2F;*拦截所有请求 --&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;HiddenHttpMethodFilter&lt;&#x2F;filter-name&gt; &lt;url-pattern&gt;&#x2F;*&lt;&#x2F;url-pattern&gt; &lt;&#x2F;filter-mapping&gt; &lt;&#x2F;web-app&gt; /3.SpringMVC_request/WebContent/index.jsp12345678910111213141516171819202122232425262728293031323334353637&lt;%@ page language&#x3D;&quot;java&quot; contentType&#x3D;&quot;text&#x2F;html; charset&#x3D;UTF-8&quot; pageEncoding&#x3D;&quot;UTF-8&quot;%&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv&#x3D;&quot;Content-Type&quot; content&#x3D;&quot;text&#x2F;html; charset&#x3D;UTF-8&quot;&gt;&lt;title&gt;Insert title here&lt;&#x2F;title&gt;&lt;&#x2F;head&gt;&lt;body&gt;&lt;a href&#x3D;&quot;handle01&quot;&gt;Handle01&lt;&#x2F;a&gt;&lt;br&#x2F;&gt;&lt;a href&#x3D;&quot;handle02?username&#x3D;tomcat&quot;&gt;handle02&lt;&#x2F;a&gt;&lt;br&#x2F;&gt;&lt;a href&#x3D;&quot;handle03?username&#x3D;tomcat&quot;&gt;handle03&lt;&#x2F;a&gt;&lt;br&#x2F;&gt;&lt;a href&#x3D;&quot;handle04&quot;&gt;handle04&lt;&#x2F;a&gt;&lt;br&#x2F;&gt;&lt;form action&#x3D;&quot;book&quot; method&#x3D;&quot;post&quot;&gt; 书名:&lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;bookName&quot;&#x2F;&gt;&lt;br&#x2F;&gt; 作者:&lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;author&quot;&#x2F;&gt;&lt;br&#x2F;&gt; 价格:&lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;price&quot;&#x2F;&gt;&lt;br&#x2F;&gt; 库存:&lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;stock&quot;&#x2F;&gt;&lt;br&#x2F;&gt; 销量:&lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;sales&quot;&#x2F;&gt;&lt;br&#x2F;&gt; &lt;hr&#x2F;&gt; 省:&lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;address.province&quot; &#x2F;&gt; 市:&lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;address.city&quot; &#x2F;&gt; 街道:&lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;address.street&quot; &#x2F;&gt; &lt;input type&#x3D;&quot;submit&quot; &#x2F;&gt;&lt;&#x2F;form&gt;&lt;br&#x2F;&gt;&lt;h1&gt;测试原生API&lt;&#x2F;h1&gt;&lt;a href&#x3D;&quot;handle05&quot;&gt;handle05&lt;&#x2F;a&gt;&lt;&#x2F;body&gt;&lt;&#x2F;html&gt; com.lizhi.bookBook.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100package com.lizhi.book;public class Book &#123; &#x2F;** * 书名:&lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;bookName&quot;&#x2F;&gt;&lt;br&#x2F;&gt; 作者:&lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;author&quot;&#x2F;&gt;&lt;br&#x2F;&gt; 价格:&lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;price&quot;&#x2F;&gt;&lt;br&#x2F;&gt; 库存:&lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;stock&quot;&#x2F;&gt;&lt;br&#x2F;&gt; 销量:&lt;input type&#x3D;&quot;text&quot; name&#x3D;&quot;sales&quot;&#x2F;&gt;&lt;br&#x2F;&gt; *&#x2F; private String bookName; private String author; private Double price; private Integer stock; private Integer sales; private Address address; &#x2F;** * 一定要写无参的构造器 *&#x2F; public Book() &#123; super(); &#x2F;&#x2F; TODO Auto-generated constructor stub &#125; &#x2F;** * 写有参构造器，一定要写无参构造器，默认是由有参的，写有参后，无参就没了 * @return *&#x2F; public Book(String bookName, String author, Double price, Integer stock, Integer sales, Address address) &#123; super(); this.bookName &#x3D; bookName; this.author &#x3D; author; this.price &#x3D; price; this.stock &#x3D; stock; this.sales &#x3D; sales; this.address &#x3D; address; &#125; &#x2F;* * 有参构造器 *&#x2F; public String getBookName() &#123; return bookName; &#125; public Address getAddress() &#123; return address; &#125; public void setAddress(Address address) &#123; this.address &#x3D; address; &#125; public void setBookName(String bookName) &#123; this.bookName &#x3D; bookName; &#125; public String getAuthor() &#123; return author; &#125; public void setAuthor(String author) &#123; this.author &#x3D; author; &#125; public Double getPrice() &#123; return price; &#125; public void setPrice(Double price) &#123; this.price &#x3D; price; &#125; public Integer getStock() &#123; return stock; &#125; public void setStock(Integer stock) &#123; this.stock &#x3D; stock; &#125; public Integer getSales() &#123; return sales; &#125; public void setSales(Integer sales) &#123; this.sales &#x3D; sales; &#125; @Override public String toString() &#123; return &quot;Book [bookName&#x3D;&quot; + bookName + &quot;, author&#x3D;&quot; + author + &quot;, price&#x3D;&quot; + price + &quot;, stock&#x3D;&quot; + stock + &quot;, sales&#x3D;&quot; + sales + &quot;, address&#x3D;&quot; + address + &quot;]&quot;; &#125; &#125; Address.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.lizhi.book;public class Address &#123; private String province; private String city; private String street; &#x2F;** * 一定要写无参的构造器 *&#x2F; public Address() &#123; super(); &#x2F;&#x2F; TODO Auto-generated constructor stub &#125; public Address(String province, String city, String street) &#123; super(); this.province &#x3D; province; this.city &#x3D; city; this.street &#x3D; street; &#125; public String getProvince() &#123; return province; &#125; public void setProvince(String province) &#123; this.province &#x3D; province; &#125; public String getCity() &#123; return city; &#125; public void setCity(String city) &#123; this.city &#x3D; city; &#125; public String getStreet() &#123; return street; &#125; public void setStreet(String street) &#123; this.street &#x3D; street; &#125; @Override public String toString() &#123; return &quot;Address [province&#x3D;&quot; + province + &quot;, city&#x3D;&quot; + city + &quot;, street&#x3D;&quot; + street + &quot;]&quot;; &#125; &#125; com.lizhi.controllerHelloController.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204package com.lizhi.controller;import java.io.BufferedReader;import java.io.IOException;import java.io.PrintWriter;import javax.servlet.ServletInputStream;import javax.servlet.ServletOutputStream;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import javax.servlet.http.HttpSession;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.CookieValue;import org.springframework.web.bind.annotation.RequestHeader;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import com.lizhi.book.Book;@Controllerpublic class HelloController &#123; &#x2F;** * request.getParameter(&quot;&quot;)...... * @return *&#x2F; @RequestMapping(&quot;handle01&quot;) public String handle01() &#123; System.out.println(&quot;handle01......&quot;); return &quot;success&quot;; &#125; &#x2F;** * SpringMVC如何获取请求带来的各种信息(三个注解) * * 默认方式获取请求参数(使用@RequestMapping) * 直接给方法入参写一个和请求参数名相同的变量，这个变量就来接受请求参数的值； * URL带什么就是什么值 * 1、@RequestParam : 获取请求参数,默认参数是必须带的 * @RequestParam(&quot;user&quot;) : * 即 username&#x3D;request.getParameter(&quot;user&quot;) * value ： 指定获取的参数的Key * required : 这个参数是否必须得，意思是这个值不带不会报错 * defaultValue : 指定默认值，即如果没带参数，默认是什么，之前默认值是null * 区别 * @RequestParam(&quot;user&quot;) : ?user&#x3D;admin * @PathVariable(&quot;user&quot;) : 获取路径上的值主要是占位符，&#x2F;book&#x2F;&#123;user&#125;?user&#x3D;admin * &#x2F;book&#x2F;&#123;user&#125;?user&#x3D;admin * &#123;user&#125; -&gt; PathVariable * ?user&#x3D;admin -&gt; RequestParam * 两个一个是管理路径上的参数一个是管理携带的参数，二者互相不能访问到对方的参数 * * * *&#x2F; @RequestMapping(&quot;&#x2F;handle02&quot;) public String handle02(@RequestParam(value &#x3D; &quot;user&quot;,required &#x3D; false,defaultValue &#x3D; &quot;未带参数&quot;)String username) &#123; System.out.println(&quot;这个变量的值；&quot;+username); return &quot;success&quot;; &#125; &#x2F;** * 2、@RequestHeader * request.getHeader(&quot;User-Agent&quot;); * @RequestHeader(&quot;User-Agent&quot;)String userAgent * 相当于 userAgent &#x3D; request.getHeader(&quot;User-Agent&quot;); * 请求头这么多都可以写 * Host: localhost:8080 * User-Agent: Mozilla&#x2F;5.0 (Windows NT 6.1; Win64; x64; rv:70.0) Gecko&#x2F;20100101 Firefox&#x2F;70.0 * Accept: * Accept-Language: zh-CN,zh;q&#x3D;0.8,zh-TW;q&#x3D;0.7,zh-HK;q&#x3D;0.5,en-US;q&#x3D;0.3,en;q&#x3D;0.2 * Accept-Encoding: gzip, deflate * Referer: http:&#x2F;&#x2F;localhost:8080&#x2F;3.SpringMVC_request&#x2F;index.jsp * Connection: keep-alive * Cookie: JSESSIONID&#x3D;3BF8EA368A3847CB387AF85822B6A11D; Webstorm-2f8f75da&#x3D;87e0117e-f750-4f39-8dac-88bee5e6c49d; _xsrf&#x3D;2|61c17969|b92c2c2c65739014c32f1328c1db12b1|1573649253; username-localhost-8889&#x3D;&quot;2|1:0|10:1573687317|23:username-localhost-8889|44:MzA2MTEyNjQ5MTRhNDU2Mjk3Nzg1M2E0ZmFjNDRhNDE&#x3D;|3fc35b809b91f316e3c271928cb7456bdcd43a3903664be69d1f349cd2cba191&quot;; username-localhost-8890&#x3D;&quot;2|1:0|10:1573650131|23:username-localhost-8890|44:MTg3ZWFlYjIxNzNkNGMyODljMzU0MzkyZmEyM2E3NDQ&#x3D;|9fa7610ceafa454743ec6e9a4d61e6f5aa0dc8033d1c99443c75186b4ea7ce50&quot;; username-localhost-8888&#x3D;&quot;2|1:0|10:1573687724|23:username-localhost-8888|44:NTI3NmIzNWFkYjIyNGViM2IyNDdlOWZjZmUyNzg2MDE&#x3D;|0673e2f877b17955e35fe690341b4556cb00e760f40499317b095c68157edd62&quot; * Upgrade-Insecure-Requests: 1 * Cache-Control: max-age&#x3D;0 * * 如果带的值是请求头中没有的会报错，与@RequestParam一样也有三个参数 * value ： 指定获取的参数的Key * required : 指定这个参数是否带，请求头是否带这个参数，不带也不报错 * defaultValue : 指定默认值，即如果没带参数，默认是什么，之前默认值是null * * @return *&#x2F; @RequestMapping(&quot;&#x2F;handle03&quot;) public String handle03(@RequestParam(value &#x3D; &quot;user&quot;,required &#x3D; false,defaultValue &#x3D; &quot;未带参数&quot;)String username ,@RequestHeader(value&#x3D;&quot;User-Agent&quot;)String userAgent,@RequestHeader(&quot;Cookie&quot;)String cookie) &#123; System.out.println(&quot;这个变量的值；&quot;+username); System.out.println(&quot;请求头中的浏览器信息:&quot;+userAgent); System.out.println(&quot;请求头中的Cookie:&quot;+cookie); return &quot;success&quot;; &#125; &#x2F;** * 3、@CookieValue : 获取某个cookie的值 * 以前的操作获取某个cookie * Cookie[] cookies &#x3D; request.getCookies(); * for(Cookie c:cookies)&#123; * if(c.getName().equals(&quot;JSESSIONID&quot;))&#123; * String cv &#x3D; c.getValue(); * &#125; * &#125; * * value ： 指定获取的参数的Key * required : 指定这个参数是否带，请求头是否带这个参数，不带也不报错 * defaultValue : 指定默认值，即如果没带参数，默认是什么，之前默认值是null *&#x2F; @RequestMapping(&quot;handle04&quot;) public String handle04(@CookieValue(&quot;JSESSIONID&quot;)String jid) &#123; System.out.println(&quot;Cookie中的jid的值&quot;+jid); return &quot;success&quot;; &#125; &#x2F;** * 如果我们的请求参数是POJO * SpringMVC会自动为这个POJO进行赋值？ * 1、将POJO中的每一个属性，从request参数中尝试获取出来，并封装即可； * 2、还可以级联封装，级联属性就是属性的属性,看index.jsp里的省市街道 * * * 提交的数据可能有乱码: * 请求乱码: * GET请求:在服务器配置文件(配置服务器后出来的Servers项目)的server.xml，在里面添加URIEncoding&#x3D;&quot;UTF-8&quot; * &lt;Connector URIEncoding&#x3D;&quot;UTF-8&quot; connectionTimeout&#x3D;&quot;20000&quot; port&#x3D;&quot;8080&quot; protocol&#x3D;&quot;HTTP&#x2F;1.1&quot; redirectPort&#x3D;&quot;8443&quot;&#x2F;&gt; * POST请求:在第一次获取请求参数之前设置 * ①、request.setCharacterEncoding(&quot;UTF-8&quot;); * ②、自己写一个filter,SpringMVC提供了一个filter,在Open Type里搜CharacterEncodingFilter * &lt;!-- 配置一个字符编码的Filter 注意:这个字符编码Filter必须再其他Filter之前，不然无法解决问题 --&gt; &lt;filter&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;&#x2F;filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;&#x2F;filter-class&gt; &lt;!-- 由于该方法支持多种编码，在下面指定我们要用的编码形式 --&gt; &lt;!-- init-param初始化参数 --&gt; &lt;!-- encoding : 解决POST乱码 --&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;&#x2F;param-name&gt; &lt;param-value&gt;UTF-8&lt;&#x2F;param-value&gt; &lt;&#x2F;init-param&gt; &lt;!-- forceEncoding : 顺手解决响应乱码 --&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;&#x2F;param-name&gt; &lt;param-value&gt;true&lt;&#x2F;param-value&gt; &lt;&#x2F;init-param&gt; &lt;&#x2F;filter&gt; &lt;!-- 配置所有请求采用此编码 --&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;&#x2F;filter-name&gt; &lt;url-pattern&gt;&#x2F;*&lt;&#x2F;url-pattern&gt; &lt;&#x2F;filter-mapping&gt; &lt;!-- 使用SpringMVC前端控制器写完就直接写字符编码过滤器 Tomcat一装上就，上手就像该server.xml里的8080处添加URIEncoding&#x3D;&quot;UTF-8&quot; --&gt; * * (我们REST风格发的DELETE，PUT请求都是由这两个转换来的，前面REST里的Filter原理有介绍) * 响应乱码: * reponse.setContentType(&quot;Text&#x2F;html;charset&#x3D;utf-8&quot;); * * @param book * @return *&#x2F; @RequestMapping(&quot;&#x2F;book&quot;) public String addBook(Book book) &#123; System.out.println(&quot;我要保存的图书:&quot;+book); return &quot;success&quot;; &#125; &#x2F;*** * SpringMVC可以直接在参数上写原生API * 只能使用一下几种API * HTTPServletRequest * HttpServletResponse * HTTPSession * * * Java.security.Principal * Locale : 跟国际化有关的区域信息对象 * InputStream : 请求的流(字节流) * ServletInputStream inputStream &#x3D; request.getInputStream(); * OutputStream : 输出流文件下载的时候能用到(字节流) * ServletOutputStream outputStream &#x3D; response.getOutputStream(); * Reader : 从请求中拿到字符流 * BufferedReader reader &#x3D; request.getReader(); * Write : 从Reponse中拿到的往外写的字符流 * PrintWriter writer &#x3D; response.getWriter(); * @throws IOException *&#x2F; @RequestMapping(&quot;&#x2F;handle05&quot;) public String handle05(HttpSession session,HttpServletRequest request ,HttpServletResponse response) throws IOException &#123; request.setAttribute(&quot;reqParam&quot;, &quot;我是请求域中的&quot;); session.setAttribute(&quot;sessionParam&quot;, &quot;我是Session域中的&quot;); return &quot;success&quot;; &#125; &#125;","categories":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"},{"name":"SpringMVC-请求处理","slug":"软件开发框架/SpringMVC-请求处理","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/SpringMVC-%E8%AF%B7%E6%B1%82%E5%A4%84%E7%90%86/"}],"tags":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/tags/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"},{"name":"SpringMVC-请求处理","slug":"SpringMVC-请求处理","permalink":"http://www.studyz.club/tags/SpringMVC-%E8%AF%B7%E6%B1%82%E5%A4%84%E7%90%86/"}]},{"title":"软件开发框架之SpringMVC-REST风格请求处理","slug":"软件开发框架之SpringMVC-请求处理REST","date":"2019-11-19T08:06:45.185Z","updated":"2019-11-20T00:54:12.527Z","comments":true,"path":"posts/ab0b5f3a/","link":"","permalink":"http://www.studyz.club/posts/ab0b5f3a/","excerpt":"","text":"一、REST风格请求方式 二、使用Rest来构建一个增删改查系统 2.1、原理及出现的问题 2.2、测试实例源码 一、REST风格请求方式 REST：即 Representational State Transfer。（资源）表现层状态转化。是目前最流行的一种互联网软件架构。它结构清晰、符合标准、易于理解、扩展方便，所以正得到越来越多网站的采用 资源（Resources）：网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，总之就是一个具体的存在。可以用一个URI（统一资源定位符）指向它，每种资源对应一个特定的 URI 。要获取这个资源，访问它的URI就可以，因此 URI 即为每一个资源的独一无二的识别符。 表现层（Representation）：把资源具体呈现出来的形式，叫做它的表现层（Representation）。比如，文本可以用 txt 格式表现，也可以用 HTML 格式、XML 格式、JSON 格式表现，甚至可以采用二进制格式。 状态转化（State Transfer）：每发出一个请求，就代表了客户端和服务器的一次交互过程。HTTP协议，是一个无状态协议，即所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生“状态转化”（State Transfer）。而这种转化是建立在表现层之上的，所以就是 “表现层状态转化”。具体说，就是 HTTP 协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。它们分别对应四种基本操作：GET 用来获取资源，POST 用来新建资源，PUT 用来更新资源，DELETE 用来删除资源。 比如对book进行增删改查 核心思想:不将getBook?id=1，等等这些查询方式写进URL，而是用HTTP里的四种请求方式来完成区分及增删改查1234&#x2F;getBook?id&#x3D;1 ：查询图书&#x2F;deleteBook?id&#x3D;1：删除1号图书&#x2F;updateBook?id&#x3D;1:更新1号图书&#x2F;addBook ：添加图书 12345678910Rest推荐； url地址这么起名； &#x2F;资源名&#x2F;资源标识符&#x2F;book&#x2F;1 ：GET-----查询1号图书&#x2F;book&#x2F;1 :PUT------更新1号图书&#x2F;book&#x2F;1 :DELETE-----删除1号图书&#x2F;book :POST-----添加图书系统的URL地址就这么来设计即可； 简洁的URL提交请求，以请求方式区分对资源操作；问题：从页面上只能发起两种请求，GET、POST；其他的请求方式没法使用； 二、使用Rest来构建一个增删改查系统2.1、原理及出现的问题123456789101112131415161718192021222324252627&lt;!-- 发起图书的增删改查请求，使用REST风格的URL地址 请求URL 请求方式 表示含义&#x2F;book&#x2F;1 GET: 查询1号图书&#x2F;book&#x2F;1 DELETE: 删除1号图书&#x2F;book&#x2F;1 PUT: 更新1号图书&#x2F;book POST: 添加1号图书由于页面不支持发起PUT、DELETE形式的请求，该怎么办呢？ Spring提供了对REST风格的支持 1、SpringMVC中有一个Filter,他可以把普通的请求转化为规定形式的请求； 首先配置这个filter,在web.xml中 &lt;filter&gt; &lt;filter-name&gt;HiddenHttpMethodFilter&lt;&#x2F;filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.HiddenHttpMethodFilter&lt;&#x2F;filter-class&gt; &lt;&#x2F;filter&gt; 拦截哪些请求 &#x2F;*拦截所有请求 &lt;filter-mapping&gt; &lt;filter-name&gt;HiddenHttpMethodFilter&lt;&#x2F;filter-name&gt; &lt;url-pattern&gt;&#x2F;*&lt;&#x2F;url-pattern&gt; &lt;&#x2F;filter-mapping&gt; 2、如何发其他形式的请求，按照以下请求 ①、创建一个post类型的表单 ②、表单项中携带一个_method的参数 ③、这个_method的值就是DELETE和PUT--&gt; Filter的实现原理123456789101112131415161718192021@Override protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException &#123; &#x2F;&#x2F;获取表单上_method带来的值（delete\\put） String paramValue &#x3D; request.getParameter(this.methodParam); &#x2F;&#x2F;判断如过表单是一个post而且_method有值 if (&quot;POST&quot;.equals(request.getMethod()) &amp;&amp; StringUtils.hasLength(paramValue)) &#123; &#x2F;&#x2F;转为PUT、DELETE String method &#x3D; paramValue.toUpperCase(Locale.ENGLISH); &#x2F;&#x2F;重写了request.getMethod()； HttpServletRequest wrapper &#x3D; new HttpMethodRequestWrapper(request, method); &#x2F;&#x2F;wrapper.getMethod()&#x3D;&#x3D;&#x3D;PUT； filterChain.doFilter(wrapper, response); &#125; else &#123; &#x2F;&#x2F;直接放行 filterChain.doFilter(request, response); &#125; &#125; 出现的问题 tomcat8以上的版本会出现这个问题，对请求头的支持问题 解决办法 2.2、测试实例源码 com.lizhi.controllerBookController.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.lizhi.controller;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;@Controllerpublic class BookController &#123; &#x2F;** * 查询图书 *&#x2F; @RequestMapping(value&#x3D;&quot;&#x2F;book&#x2F;&#123;bid&#125;&quot;,method &#x3D; RequestMethod.GET) public String getBook(@PathVariable(&quot;bid&quot;)Integer id) &#123; System.out.println(&quot;你查询到了&quot;+id+&quot;几号图书&quot;); return &quot;success&quot;; &#125; &#x2F;** * 删除图书 *&#x2F; @RequestMapping(value &#x3D; &quot;&#x2F;book&#x2F;&#123;bid&#125;&quot;,method &#x3D; RequestMethod.DELETE) public String deleteBook(@PathVariable(&quot;bid&quot;)Integer id) &#123; System.out.println(&quot;你删除了&quot;+id+&quot;号图书&quot;); return &quot;success&quot;; &#125; &#x2F;** * 更新图书 *&#x2F; @RequestMapping(value &#x3D; &quot;&#x2F;book&#x2F;&#123;bid&#125;&quot;,method &#x3D; RequestMethod.PUT) public String updateBook(@PathVariable(&quot;bid&quot;)Integer id) &#123; System.out.println(&quot;你更新了&quot;+id+&quot;号图书&quot;); return &quot;success&quot;; &#125; &#x2F;** * 添加图书 *&#x2F; @RequestMapping(value &#x3D; &quot;&#x2F;book&quot;,method &#x3D; RequestMethod.POST) public String addBook() &#123; System.out.println(&quot;你添加了新的图书&quot;); return &quot;success&quot;; &#125; &#125; /2.SpringMVC_REST/WebContent/WEB-INF/springmvc-servlet.xml1234567891011121314151617&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xmlns:context&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans-4.3.xsd http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&#x2F;spring-context-4.3.xsd&quot;&gt; &lt;!-- 扫描包 --&gt; &lt;context:component-scan base-package&#x3D;&quot;com.lizhi&quot;&gt;&lt;&#x2F;context:component-scan&gt; &lt;!-- --&gt; &lt;bean class&#x3D;&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name&#x3D;&quot;prefix&quot; value&#x3D;&quot;&#x2F;WEB-INF&#x2F;pages&#x2F;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;suffix&quot; value&#x3D;&quot;.jsp&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt; &lt;&#x2F;beans&gt; /2.SpringMVC_REST/WebContent/WEB-INF/pages/success.jsp12345678910111213141516&lt;%@ page language&#x3D;&quot;java&quot; contentType&#x3D;&quot;text&#x2F;html; charset&#x3D;UTF-8&quot; pageEncoding&#x3D;&quot;UTF-8&quot; isErrorPage&#x3D;&quot;true&quot;%&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv&#x3D;&quot;Content-Type&quot; content&#x3D;&quot;text&#x2F;html; charset&#x3D;UTF-8&quot;&gt;&lt;title&gt;Insert title here&lt;&#x2F;title&gt;&lt;&#x2F;head&gt;&lt;body&gt;&lt;!-- 为了解决高版本tomcat不能传PUT,DELETE的问题 isErrorPage&#x3D;&quot;true&quot;--&gt;&lt;h1&gt;成功&lt;&#x2F;h1&gt;&lt;&#x2F;body&gt;&lt;&#x2F;html&gt; web.xml1234567891011121314151617181920212223242526272829303132333435363738&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;web-app xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xmlns&#x3D;&quot;http:&#x2F;&#x2F;xmlns.jcp.org&#x2F;xml&#x2F;ns&#x2F;javaee&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;xmlns.jcp.org&#x2F;xml&#x2F;ns&#x2F;javaee http:&#x2F;&#x2F;xmlns.jcp.org&#x2F;xml&#x2F;ns&#x2F;javaee&#x2F;web-app_4_0.xsd&quot; id&#x3D;&quot;WebApp_ID&quot; version&#x3D;&quot;4.0&quot;&gt; &lt;display-name&gt;2.SpringMVC_REST&lt;&#x2F;display-name&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.html&lt;&#x2F;welcome-file&gt; &lt;welcome-file&gt;index.htm&lt;&#x2F;welcome-file&gt; &lt;welcome-file&gt;index.jsp&lt;&#x2F;welcome-file&gt; &lt;welcome-file&gt;default.html&lt;&#x2F;welcome-file&gt; &lt;welcome-file&gt;default.htm&lt;&#x2F;welcome-file&gt; &lt;welcome-file&gt;default.jsp&lt;&#x2F;welcome-file&gt; &lt;&#x2F;welcome-file-list&gt; &lt;!-- &lt;#然后自动提示选#DispatcherServlet --&gt; &lt;!-- The front controller of this Spring Web application, responsible for handling all application requests --&gt; &lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;&#x2F;servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;&#x2F;servlet-class&gt; &lt;load-on-startup&gt;1&lt;&#x2F;load-on-startup&gt; &lt;&#x2F;servlet&gt; &lt;!-- Map all requests to the DispatcherServlet for handling --&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;&#x2F;servlet-name&gt; &lt;url-pattern&gt;&#x2F;&lt;&#x2F;url-pattern&gt; &lt;&#x2F;servlet-mapping&gt; &lt;filter&gt; &lt;filter-name&gt;HiddenHttpMethodFilter&lt;&#x2F;filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.HiddenHttpMethodFilter&lt;&#x2F;filter-class&gt; &lt;&#x2F;filter&gt; &lt;!-- 拦截哪些请求 &#x2F;*拦截所有请求 --&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;HiddenHttpMethodFilter&lt;&#x2F;filter-name&gt; &lt;url-pattern&gt;&#x2F;*&lt;&#x2F;url-pattern&gt; &lt;&#x2F;filter-mapping&gt;&lt;&#x2F;web-app&gt; index.jsp123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&lt;%@ page language&#x3D;&quot;java&quot; contentType&#x3D;&quot;text&#x2F;html; charset&#x3D;UTF-8&quot; pageEncoding&#x3D;&quot;UTF-8&quot;%&gt;&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv&#x3D;&quot;Content-Type&quot; content&#x3D;&quot;text&#x2F;html; charset&#x3D;UTF-8&quot;&gt;&lt;title&gt;Insert title here&lt;&#x2F;title&gt;&lt;&#x2F;head&gt;&lt;body&gt;&lt;!-- 发起图书的增删改查请求，使用REST风格的URL地址 请求URL 请求方式 表示含义&#x2F;book&#x2F;1 GET: 查询1号图书&#x2F;book&#x2F;1 DELETE: 删除1号图书&#x2F;book&#x2F;1 PUT: 更新1号图书&#x2F;book POST: 添加1号图书由于页面不支持发起PUT、DELETE形式的请求，该怎么办呢？ Spring提供了对REST风格的支持 1、SpringMVC中有一个Filter,他可以把普通的请求转化为规定形式的请求； 首先配置这个filter,在web.xml中 &lt;filter&gt; &lt;filter-name&gt;HiddenHttpMethodFilter&lt;&#x2F;filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.HiddenHttpMethodFilter&lt;&#x2F;filter-class&gt; &lt;&#x2F;filter&gt; 拦截哪些请求 &#x2F;*拦截所有请求 &lt;filter-mapping&gt; &lt;filter-name&gt;HiddenHttpMethodFilter&lt;&#x2F;filter-name&gt; &lt;url-pattern&gt;&#x2F;*&lt;&#x2F;url-pattern&gt; &lt;&#x2F;filter-mapping&gt; 2、如何发其他形式的请求，按照以下请求 ①、创建一个post类型的表单 ②、表单项中携带一个_method的参数 ③、这个_method的值就是DELETE和PUT--&gt;&lt;a href&#x3D;&quot;book&#x2F;1 &quot;&gt;查询图书&lt;&#x2F;a&gt;&lt;br&#x2F;&gt;&lt;form action&#x3D;&quot;book&quot; method&#x3D;&quot;post&quot;&gt; &lt;input type&#x3D;&quot;submit&quot; value&#x3D;&quot;添加1号图书&quot;&#x2F;&gt;&lt;&#x2F;form&gt;&lt;br&gt;&lt;!-- 发送DELETE请求 --&gt;&lt;form action&#x3D;&quot;book&#x2F;1&quot; method&#x3D;&quot;post&quot;&gt; &lt;input name&#x3D;&quot;_method&quot; value&#x3D;&quot;delete&quot;&#x2F;&gt; &lt;input type&#x3D;&quot;submit&quot; value&#x3D;&quot;删除1号图书&quot;&#x2F;&gt;&lt;&#x2F;form&gt;&lt;!-- &lt;a href&#x3D;&quot;book&#x2F;1 &quot;&gt;删除1号图书&lt;&#x2F;a&gt; --&gt;&lt;br&#x2F;&gt;&lt;!-- 发送PUT请求 --&gt;&lt;form action&#x3D;&quot;book&#x2F;1&quot; method&#x3D;&quot;post&quot;&gt; &lt;input name&#x3D;&quot;_method&quot; value&#x3D;&quot;put&quot; &#x2F;&gt; &lt;input type&#x3D;&quot;submit&quot; value&#x3D;&quot;更新1号图书&quot;&#x2F;&gt;&lt;&#x2F;form&gt;&lt;!-- &lt;a href&#x3D;&quot;book&#x2F;1 &quot;&gt;更新1号图书&lt;&#x2F;a&gt; --&gt;&lt;&#x2F;body&gt;&lt;&#x2F;html&gt;","categories":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"},{"name":"SpringMVC-REST","slug":"软件开发框架/SpringMVC-REST","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/SpringMVC-REST/"}],"tags":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/tags/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"},{"name":"SpringMVC-REST","slug":"SpringMVC-REST","permalink":"http://www.studyz.club/tags/SpringMVC-REST/"}]},{"title":"Hadoop-Hbase基本操作","slug":"Hadoop-Hbase基本操作","date":"2019-11-16T06:41:48.820Z","updated":"2019-11-30T03:04:53.647Z","comments":true,"path":"posts/f9acb5d3/","link":"","permalink":"http://www.studyz.club/posts/f9acb5d3/","excerpt":"","text":"一、基本操作 二、常用的 Shell 操作 三、表的操作 四、版本的确界 五、数据的备份与恢复 六、节点管理 一、基本操作1.1、进入 HBase 客户端命令行1hbase shell 1.2、查看帮助命令1hbase(main):015:0&gt; help 1.3、查看当前数据库中有哪些表1hbase(main):016:0&gt; list 二、常用的 Shell 操作2.1、satus显示服务器状态 2.2、whoami显示 HBase 当前用户 2.3、create创建表 创建一个table表cf为列族 创建多个有列族的表 2.4、list显示当前所有的表 2.5、count统计指定表的记录数 这里要注意，在Hbase中行间RowKey是标识一行，有几个行键就有几行 2.6、describe展示表结构信息 2.7、exist检查表是否存在，适用于表量特别多的情况 2.8、is_enabled/is_disabled检查表是否启用或禁用 2.9、alter改变表和列族的模式 为当前student表增加teacher列族： versions 默认是 1 这个参数的意思是数据保留 1 个 版本，如果我们认为我们的数据没有这么大 的必要保留这么多，随时都在更新，而老版本的数据对我们毫无价值，那将此参数设为 1 能 节约 2/3 的空间。使用方法: create ‘table’,{VERSIONS=&gt;’2’} 附： MIN_VERSIONS =&gt; ‘0’是说在 compact 操作执行之后，至少要保留的版本 为当前表删除列族：(实际生产慎用删除) 2.10、disable禁用一张表(有时删除表之前需要先禁用) 2.11、drop删除一张表，记得在删除表之前必须先禁用(实际生产中慎用，禁用删除) 2.12、delete删除一行中一个单元格的值 hbase&gt; delete ‘t4’, ‘rowKey’, ‘CF:C’ 2.13、truncate清空表数据，即禁用表-删除表-创建表 三、表的操作1、创建表 名称 命令表达式 查看hbase状态 status 创建表 create ‘表名’,’列族名1’,’列族名2’,’列族名N’ 查看所有表 list 描述表 describe ‘表名’ 判断表存在 exists ‘表名’ 判断是否禁用启用表 is_enabled ‘表名’ 和 is_disabled ‘表名’ 添加记录 put ‘表名’,’rowkey’,’列族：列’，’值’ 查看记录rowkey下的所有数据 get ‘表名’,’rowkey’ 查看所有记录 scan ‘表名’ 查看表中的记录总数 count ‘表名’ 获取某个列族 get ‘表名’,’rowkey’,’列族：列’ 获取某个列族的某个列 get ‘表名’,’rowkey’,’列族：列’ 删除记录 delete ‘表名’,’行名’,’列族：列’ 删除整行 deleteall ‘表名’,’rowkey’ 删除一张表 先要屏蔽该表，才能对该表进行删除，第一步 disable ‘表名’，第二步 drop ‘表名’ 清空表 truncate ‘表名’ 查看某个表某个列中所有数据 scan ‘表名’,{COLUMNS=&gt;’列族名：列名’} 更新记录 就是重新一遍，进行覆盖，hbase没有修改，都是追加 1create &#39;student&#39;,&#39;info&#39; 2、插入数据到表 3、扫描查看表的数据 4、查看表结构 12345678910111213NAME &#x3D;&gt; &#39;info&#39;, &#x2F;&#x2F;列族DATA_BLOCK_ENCODING &#x3D;&gt; &#39;NONE&#39;, &#x2F;&#x2F;数据块编码方式设置BLOOMFILT &#x3D;&gt; &#39;ROW&#39;,REPLICATION_SCOPE &#x3D;&gt; &#39;0&#39;, &#x2F;&#x2F;配置HBase集群replication时需要将该参数设置为1.VERSIONS &#x3D;&gt; &#39;1&#39;, &#x2F;&#x2F;设置保存的版本数COMPRESSION &#x3D;&gt; &#39;NONE&#39;, &#x2F;&#x2F;设置压缩算法MIN_VERSIONS &#x3D;&gt; &#39;0&#39;, &#x2F;&#x2F;最小存储版本数TTL &#x3D;&gt; &#39;FOREVER&#39;, &#x2F;&#x2F;&#39;ColumnFamilies can set a TTL length in seconds, and HBase reached. This applies to all versions of a row - even the current one. The TTL time encoded in the HBase for the row is specified in UTC.&#39;KEEP_DELETED_CELLS &#x3D;&gt; &#39;false&#39;, BLOCKSIZE &#x3D;&gt; &#39;65536&#39;, &#x2F;&#x2F;设置HFile数据块大小（默认64kb）IN_MEMORY &#x3D;&gt; &#39;false&#39;,&#x2F;&#x2F;设置激进缓存，优先考虑将该列族放入块缓存中， &#x2F;&#x2F;针对随机读操作相对较多的列族可以设置该属性为trueBLOCKCACHE &#x3D;&gt; &#39;true&#39; &#x2F;&#x2F;数据块缓存属性 5、更新指定字段的数据 6、查看“指定行”或“指定列族:列” 的数据 根据时间戳查询 7、删除数据 删除某rowkey的全部数据 删除某rowkey的某一列数据 删除最新时间戳的数据 删除某一列下的所有数据 8、清空表数据注意：清空表（truncate）的操作本质为先disable，然后再drop-create。1hbase(main) &gt; truncate &#39;student&#39; 9、删除表注意：删除表的操作需要先disable，然后再drop，否则会报错 首先需要先让该表为 disable 状态1disable &#39;student&#39; 然后才能 drop 这个表1drop &#39;student&#39; 10、统计表数据行数1count &#39;student&#39; 四、版本的确界1、版本的下界默认的版本下界是 0，即禁用。 row 版本使用的最小数目是与生存时间（ TTL Time To Live）相结合的，并且我们根据实际需求可以有 0 或更多的版本，使用 0，即只有 1 个版本的值写入 cell。 2、 版本的上界之前默认的版本上界是 3，也就是一个 row 保留 3 个副本（基于时间戳的插入）。该值不要设计的过大，一般的业务不会超过 100。如果 cell 中存储的数据版本号超过了 3 个，再次插入数据时，最新的值会将最老的值覆盖。 （现版本已默认为 1） 3、 变更表信息将 info 列族中的数据存放3个版本：12hbase(main) &gt; alter &#39;student&#39;,&#123;NAME&#x3D;&gt;&#39;info&#39;,VERSIONS&#x3D;&gt;3&#125;get &#39;student&#39;,&#39;1001&#39;,&#123;COLUMN&#x3D;&gt;&#39;info:name&#39;, VERSIONS&#x3D;&gt;3&#125; 更改版本上界 再查询的时候可以用这个方法查询到所有版本 注意：当版本超过指定的三个版本后就会覆盖，删除删最新的，覆盖从最老的开始覆盖 五、数据的备份与恢复1、备份 停止HBase服务后，使用distcp命令运行MapReduce任务进行备份，将数据备份到另一个地方，可以是同一个集群，也可以是专用的备份集群。即，把数据转移到当前集群的其他目录下（也可以不在同一个集群中） 注意：执行此操作时， 一定要开启 Yarn 服务六、节点管理服役（commissioning） 当启动regionserver时， regionserver会向HMaster注册并开始接收本地数据，开始的时候，新加入的节点不会有任何数据，平衡器开启的情况下，将会有新的region移动到开启的RegionServer上。如果启动和停止进程是使用 ssh 和 HBase 脚本，那么会将新添加的节点的主机名加入到 conf/regionservers 文件中。 退役（decommissioning） 顾名思义，就是从当前 HBase 集群中删除某个 RegionServer，这个过程分为如下几个过程： 停止负载平衡器1hbase&gt; balance_switch false 在退役节点上停止 RegionServer1hbase&gt; hbase-daemon.sh stop regionserver RegionServer 一旦停止，会关闭维护的所有 region Zookeeper 上的该 RegionServer 节点消失 Master 节点检测到该 RegionServer 下线 RegionServer 的 region 服务得到重新分配 该关闭方法比较传统，需要花费一定的时间，而且会造成部分 region 短暂的不可用。另一种方案： RegionServer先卸载所管理的region$ bin/graceful_stop.sh &lt;RegionServer-hostname&gt; 自动平衡数据 和之前的 2~6 步是一样的","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/categories/hadoop/"},{"name":"hbase","slug":"hadoop/hbase","permalink":"http://www.studyz.club/categories/hadoop/hbase/"},{"name":"hbase基本操作","slug":"hadoop/hbase/hbase基本操作","permalink":"http://www.studyz.club/categories/hadoop/hbase/hbase%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"}],"tags":[{"name":"-hbase - hbase基本操作","slug":"hbase-hbase基本操作","permalink":"http://www.studyz.club/tags/hbase-hbase%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"}]},{"title":"软件开发框架之SpringMVC","slug":"软件开发框架之SpringMVC","date":"2019-11-14T11:33:27.369Z","updated":"2019-11-19T15:46:48.522Z","comments":true,"path":"posts/e7a41835/","link":"","permalink":"http://www.studyz.club/posts/e7a41835/","excerpt":"","text":"一、Spring MVC概述 二、第一个Spring MVC应用 三、Spring MVC的核心类和注解 3.1、DispatcherServlet 3.2、@Controller注解类型 3.3、@RequestMapping注解的使用 处理不同的请求 3.4、测试RequestMapping的模糊匹配功能(路径问题,ant风格) 3.5、REST风格请求方式 一、Spring MVC概述1、什么是Spring MVC？ Spring MVC是Spring提供的一个实现了Web MVC设计模式的轻量级Web框架。它与Struts2框架一样，都属于MVC框架，但其使用和性能等方面比Struts2更加优异。 web模块就是SpringMVC POJO : Plain Old Java Object; 2、Spring MVC具有以下特点： 是Spring框架的一部分，可以方便的利用Spring所提供的其他功能。 灵活性强，易于与其他框架集成。 提供了一个前端控制器DispatcherServlet，使开发人员无需额外开发控制器对象。 可自动绑定用户输入，并能正确的转换数据类型。 内置了常见的校验器，可以校验用户输入。如果校验不能通过，那么就会重定向到输入表单。 支持国际化。可以根据用户区域显示多国语言。 支持多种视图技术。它支持JSP、Velocity和FreeMarker等视图技术。 使用基于XML的配置文件，在编辑后，不需要重新编译应用程序。 3、Spring MVC的工作流程 在实际开发中，我们的实际工作主要集中在控制器和视图页面上，但Spring MVC内部完成了很多工作，这些程序在项目中具体是怎么执行的呢？接下来，将通过一张图来展示Spring MVC程序的执行情况。 MVC提倡 : 每一层只编写自己的东西，不写任何其他的代码控制器{ 调用业务逻辑处理 调整到某个页面}分层就是为了解耦，解耦为了维护方便和分工合作 ① 用户通过浏览器向服务器发送请求，请求会被Spring MVC的前端控制器DispatcherServlet所拦截; ② DispatcherServlet拦截到请求后，会调用HandlerMapping处理器映射器; ③ 处理器映射器根据请求URL找到具体的处理器，生成处理器对象及处理器拦截器（如果有则生成）一并返回给DispatcherServlet; ④ DispatcherServlet会通过返回信息选择合适的HandlerAdapter（处理器适配器）; ⑤ HandlerAdapter会调用并执行Handler（处理器），这里的处理器指的就是程序中编写的Controller类，也被称之为后端控制器; ⑥ Controller执行完成后，会返回一个ModelAndView对象，该对象中会包含视图名或包含模型和视图名; ⑦ HandlerAdapter将ModelAndView对象返回给DispatcherServlet; ⑧ DispatcherServlet会根据ModelAndView对象选择一个合适的ViewReslover（视图解析器）; ⑨ ViewReslover解析后，会向DispatcherServlet中返回具体的View（视图）; ⑩ DispatcherServlet对View进行渲染（即将模型数据填充至视图中）; 最后视图渲染结果会返回给客户端浏览器显示。 二、第一个Spring MVC应用 1、导包1、SpringMVC是Spring的web模块;所有模块运行都依赖核心模块（IOC模块） 1、核心容器模块 commons-logging-1.2.jar spring-aop-5.2.0.RELEASE.jar spring-beans-5.2.0.RELEASE.jar spring-context-5.2.0.RELEASE.jar spring-core-5.2.0.RELEASE.jar spring-expression-5.2.0.RELEASE.jar 2、web模块 spring-web-5.2.0.RELEASE.jar spring-webmvc-5.2.0.RELEASE.jar 2、写配置； 1、web.xml可能要写什么 配置springmvc的前端控制器，指定springmvc配置文件位置 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;web-app xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://xmlns.jcp.org/xml/ns/javaee&quot; xsi:schemaLocation=&quot;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd&quot; id=&quot;WebApp_ID&quot; version=&quot;4.0&quot;&gt; &lt;display-name&gt;55&lt;/display-name&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt; &lt;/welcome-file-list&gt; &lt;!-- SpringMVC思想是有一个前端控制器能拦截所有请求，并只能转发 这个前端控制器是一个servlet;所以应该在web.xml中配置这个servlet来拦截这个请求 --&gt; &lt;!-- ALT+\\提示有个#dispatcherServlet会自动补全 相应和处理应用的所有请求 --&gt; &lt;servlet&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- servlet初始化参数 如果没有通过&lt;init-param&gt;元素配置，则应用程序会默认去WEB-INF目录下寻找以servletName-servlet.xml方式命名的配置文件，这里的servletName指下面的springmvc 如果&lt;init-param&gt;元素存在并且通过其子元素配置了Spring MVC配置文件的路径，则应用程序在启动时会加载配置路径下的配置文件 --&gt; &lt;init-param&gt; &lt;!-- contextConfigLocation : 指定SpringMVC配置文件的位置 --&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;!-- 由于不是在一级目录src --&gt; &lt;param-value&gt;classpath*:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;!-- servlet启动加载，servlet原本是第一次访问创建对象 load-on-startup : 服务器启动的时候创建对象；值越小，优先级越高，月先创建对象 --&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;!-- Map all requests to the DispatcherServlet for handling --&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;/servlet-name&gt; &lt;!-- /*和/都是拦截所有请求 /*的范围更大，还会拦截到*.jsp这些请求，但是拦截jsp页面就不能显示了 /也会拦截所有请求，但不会拦截*.jsp;jsp显示正常 处理*.jsp是tomcat做的事所有项目的小web.xml都继承与大web.xml DefaultServlet是Tomcat中处理静态资源的？ 除过jsp，和servlet外剩下的都是静态资源； index.html：静态资源，tomcat就会在服务器下找到这个资源并返回; 我们前端控制器的/禁用了tomcat服务器中的DefaultServlet 1、服务器的大web.xml(Servers/Tomcat v9.0 Server at localhost-config/web.xml) 中有一个DefaultServlet是url-pattern=/ 2、我们的配置中前端控制器url-pattern=/ 静态资源会来到DispatcherServlet（前端控制器）看那个方法的RequestMapping是这个index.html 3）为什么jsp又能访问；因为我们没有覆盖服务器中的JspServlet的配置 4） /* 直接就是拦截所有请求；我们写/；也是为了迎合后来Rest风格的URL地址 --&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 2、框架自身可能要写什么springmvc.com &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.3.xsd&quot;&gt; &lt;!-- 扫描所有组件 --&gt; &lt;context:component-scan base-package=&quot;com.lizhi.controller&quot;&gt;&lt;/context:component-scan&gt; &lt;!-- 配置一个视图解析器，能帮我们拼接页面地址 --&gt; &lt;!-- 这样在测试的时候只要写jsp名就行，不用写地址 --&gt; &lt;bean class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/pages/&quot;&gt;&lt;/property&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;&gt;&lt;/property&gt; &lt;/bean&gt; &lt;/beans&gt; 3、测试HelloWorld细节：一：运行流程； 1）、客户端点击链接会发送 http://localhost:8080/springmvc/hello 请求 2）、来到tomcat服务器； 3）、SpringMVC的前端控制器收到所有请求； 4）、来看请求地址和@RequestMapping标注的哪个匹配，来找到到底使用那个类的哪个方法来处理 5）、前端控制器找到了目标处理器类和目标方法，直接利用返回执行目标方法； 6）、方法执行完成以后会有一个返回值；SpringMVC认为这个返回值就是要去的页面地址 7）、拿到方法返回值以后；用视图解析器进行拼串得到完整的页面地址； 8）、拿到页面地址，前端控制器帮我们转发到页面；一个方法处理一个请求；123456789101112131415161718192021222324252627282930*2、@RequestMapping； * 就是告诉SpringMVC；这个方法用来处理什么请求； * 这个&#x2F;是可以省略，即使省略了，也是默认从当前项目下开始； * 习惯加上比较好 &#x2F;hello &#x2F;hello * RequestMapping的使用：？ *3、如果不指定配置文件位置？ * &#x2F;WEB-INF&#x2F;springDispatcherServlet-servlet.xml * 如果不指定也会默认去找一个文件；(在web.xml中) * &#x2F;WEB-INF&#x2F;springDispatcherServlet-servlet.xml * 就在web应用的&#x2F;WEB-INF、下创建一个名叫 前端控制器名-servlet.xml&lt;!-- &#x2F;：拦截所有请求，不拦截jsp页面，*.jsp请求 &#x2F;*：拦截所有请求，拦截jsp页面，*.jsp请求 处理*.jsp是tomcat做的事；所有项目的小web.xml都是继承于大web.xml DefaultServlet是Tomcat中处理静态资源的？ 除过jsp，和servlet外剩下的都是静态资源； index.html：静态资源，tomcat就会在服务器下找到这个资源并返回; 我们前端控制器的&#x2F;禁用了tomcat服务器中的DefaultServlet 1）服务器的大web.xml中有一个DefaultServlet是url-pattern&#x3D;&#x2F; 2）我们的配置中前端控制器 url-pattern&#x3D;&#x2F; 静态资源会来到DispatcherServlet（前端控制器）看那个方法的RequestMapping是这个index.html 3）为什么jsp又能访问；因为我们没有覆盖服务器中的JspServlet的配置 4） &#x2F;* 直接就是拦截所有请求；我们写&#x2F;；也是为了迎合后来Rest风格的URL地址 --&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.lizhi.controller;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;&#x2F;** * 1.告诉SpringMVC这是一个处理器，可以处理请求 * @Controller;标识哪个组件是控制器 * *&#x2F;@Controllerpublic class MyFirstController &#123; &#x2F;** * &#x2F;代表从当前项目先开始，处理当前项目下的hello请求（在index.jsp中） *&#x2F; @RequestMapping(&quot;&#x2F;hello&quot;) public String myfirstRequest() &#123; System.out.println(&quot;请求收到了。。。。。。正在处理中&quot;); &#x2F;&#x2F;在springmvc.xml中配置了视图解析器，所以只要写名就行 &#x2F;** * &lt;property name&#x3D;&quot;prefix&quot; value&#x3D;&quot;&#x2F;WEB-INF&#x2F;pages&#x2F;&quot;&gt;&lt;&#x2F;property&gt; * &lt;property name&#x3D;&quot;suffix&quot; value&#x3D;&quot;.jsp&quot;&gt;&lt;&#x2F;property&gt; *prefix是前缀，suffix是后缀 * *&#x2F; return &quot;success&quot;; &#125; &#x2F;* * 这个hello01没有写到index.jsp中，但是在地址栏输入可以直接跳转 *&#x2F; @RequestMapping(&quot;&#x2F;hello01&quot;) public String hanle01()&#123; System.out.println(&quot;hanle01....被调用&quot;); return &quot;error&quot;; &#125; @RequestMapping(&quot;&#x2F;hello02&quot;) public String hanle02()&#123; return &quot;success&quot;; &#125; &#125; 三、Spring MVC的核心类和注解3.1、DispatcherServlet DispatcherServlet的全名是org.springframework.web.servlet.DispatcherServlet，它在程序中充当着前端控制器的角色。在使用时，只需将其配置在项目的web.xml文件中，其配置代码如下： 12345678910111213141516171819202122232425&lt;servlet&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;&#x2F;servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;&#x2F;servlet-class&gt; &lt;!-- servlet初始化参数 --&gt; &lt;init-param&gt; &lt;!-- contextConfigLocation : 指定SpringMVC配置文件的位置 --&gt; &lt;param-name&gt;contextConfigLocation&lt;&#x2F;param-name&gt; &lt;!-- 由于不是在一级目录src --&gt; &lt;param-value&gt;classpath*:springmvc.xml&lt;&#x2F;param-value&gt; &lt;&#x2F;init-param&gt; &lt;!-- servlet启动加载，servlet原本是第一次访问创建对象 load-on-startup : 服务器启动的时候创建对象；值越小，优先级越高，月先创建对象 --&gt; &lt;load-on-startup&gt;1&lt;&#x2F;load-on-startup&gt;&lt;&#x2F;servlet&gt; !-- Map all requests to the DispatcherServlet for handling --&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;&#x2F;servlet-name&gt; &lt;!-- &#x2F;*和&#x2F;都是拦截所有请求 &#x2F;*的范围更大，还会拦截到*.jsp这些请求，但是拦截jsp页面就不能显示了 &#x2F;也会拦截所有请求，但不会拦截*.jsp;jsp显示正常 处理*.jsp是tomcat做的事所有项目的小web.xml都继承与大web.xml --&gt; &lt;url-pattern&gt;&#x2F;&lt;&#x2F;url-pattern&gt;&lt;&#x2F;servlet-mapping&gt; 如果没有通过&lt;init-param&gt;元素配置，则应用程序会默认去WEB-INF目录下寻找以servletName-servlet.xml方式命名的配置文件，这里的servletName指下面的springmvc 如果&lt;init-param&gt;元素存在并且通过其子元素配置了Spring MVC配置文件的路径，则应用程序在启动时会加载配置路径下的配置文件 3.2、@Controller注解类型 org.springframework.stereotype.Controller注解类型用于指示Spring类的实例是一个控制器，其注解形式为@Controller。该注解在使用时不需要再实现Controller接口，只需要将@Controller注解加入到控制器类上，然后通过Spring的扫描机制找到标注了该注解的控制器即可。 @Controller注解在控制器类中的使用示例如下123456789101112131415package com.lizhi.controller;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;&#x2F;** * 1.告诉SpringMVC这是一个处理器，可以处理请求 * @Controller;标识哪个组件是控制器 * *&#x2F;@Controllerpublic class MyFirstController &#123; ......&#125; 为了保证Spring能够找到控制器类，还需要在Spring MVC的配置文件中添加相应的扫描配置信息，一个完整的配置文件示例如下：1234567891011121314151617&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xmlns:context&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans.xsd http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&#x2F;spring-context-4.3.xsd&quot;&gt;&lt;!-- 扫描所有组件 --&gt; &lt;context:component-scan base-package&#x3D;&quot;com.lizhi.controller&quot;&gt;&lt;&#x2F;context:component-scan&gt; &lt;!-- 配置一个视图解析器，能帮我们拼接页面地址 --&gt; &lt;!-- 这样在测试的时候只要写jsp名就行，不用写地址 --&gt; &lt;bean class&#x3D;&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name&#x3D;&quot;prefix&quot; value&#x3D;&quot;&#x2F;WEB-INF&#x2F;pages&#x2F;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;suffix&quot; value&#x3D;&quot;.jsp&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt;&lt;&#x2F;beans&gt; 使用注解方式时，程序的运行需要依赖Spring的AOP包3.3、RequestMapping注解的使用一个方法处理一个请求 Spring通过@Controller注解找到相应的控制器类后，还需要知道控制器内部对每一个请求是如何处理的，这就需要使用@RequestMapping注解类型，它用于映射一个请求或一个方法。使用时，可以标注在一个方法或一个类上。 2、@RequestMapping； 就是告诉SpringMVC；这个方法用来处理什么请求； 这个/是可以省略，即使省略了，也是默认从当前项目下开始； 习惯加上比较好 /hello /hello RequestMapping的使用：？ 3、如果不指定配置文件位置？ /WEB-INF/springDispatcherServlet-servlet.xml 如果不指定也会默认去找一个文件；(在web.xml中) /WEB-INF/springDispatcherServlet-servlet.xml 就在web应用的/WEB-INF、下创建一个名叫 前端控制器名-servlet.xml 3.3.1、 标注在类上 该类中的所有方法都将映射为相对于类级别的请求，表示该控制器所处理的所有请求都被映射到value属性值所指定的路径下。 123456789101112131415161718** * @RequestMapping(&quot;haha&quot;)标在类上，为当前所有类的请求地址 * 指定一个基准路径都是从&#x2F;haha开始的 * 由http:&#x2F;&#x2F;localhost:8080&#x2F;1.SpringMVC_HelloWorld&#x2F;handle01 * 变成http:&#x2F;&#x2F;localhost:8080&#x2F;1.SpringMVC_HelloWorld&#x2F;haha&#x2F;handle01 * *&#x2F;@RequestMapping(&quot;&#x2F;haha&quot;)@Controllerpublic class RequestMappingTestController &#123; @RequestMapping(&quot;&#x2F;handle01&quot;) public String handle01() &#123; System.out.println(&quot;RequestMappingTestController...handle01&quot;); return &quot;success&quot;; &#125;&#125; 3.3.2、 标注在方法上：作为请求处理方法在程序接收到对应的URL请求时被调用12345@RequestMapping(&quot;&#x2F;handle01&quot;) public String handle01() &#123; System.out.println(&quot;RequestMappingTestController...handle01&quot;); return &quot;success&quot;; &#125; 3.3.3、其他 @RequestMapping注解除了可以指定value属性外，还可以指定其他一些属性，如下表所示。 表中所有属性都是可选的，但其默认属性是value。当value是其唯一属性时，可以省略属性名。例如，下面两种标注的含义相同： @RequestMapping(value=”/firstController”) @RequestMapping(“/firstController”) 对于请求头的新特性 Spring框架的4.3版本中，引入了新的组合注解，来帮助简化常用的HTTP方法的映射，并更好的表达被注解方法的语义。 123456@GetMapping：匹配GET方式的请求；@PostMapping：匹配POST方式的请求；@PutMapping：匹配PUT方式的请求；@DeleteMapping：匹配DELETE方式的请求；@PatchMapping：匹配PATCH方式的请求。 我们只需要将原来的@RequestMapping替换掉就行12345678910@Controllerpublic class RequestMappingTest &#123; @GetMapping(value &#x3D; &quot;&#x2F;antTest01&quot;) &#x2F;&#x2F;@RequestMapping(&quot;&#x2F;antTest01&quot;,method&#x3D;RequestMethod.GET) public String antTest01() &#123; System.out.println(&quot;antTest01......被调用了&quot;); return &quot;success&quot;; &#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677&#x2F;** * RequestMapping的其他属性 * 1、method : 限定请求方式 请求方式: 【GET】, HEAD, 【POST】, PUT, PATCH, DELETE, OPTIONS, TRACE * GET、POST * GET、POST * method &#x3D; RequestMethod.POST;只有POST类型的请求才接受，默认全接受 * 不规定的方式报错： 4XX:都是客户端错误 * HTTP Status 405 – 方法不允许 Type Status Report Request method &#39;GET&#39; not supported 请求行中接收的方法由源服务器知道，但目标资源不支持 * * * * consumes：只接受内容类型是哪种的请求，规定请求头中的Content-Type * produces：告诉浏览器返回的内容类型是什么，给响应头中加上Content-Type:text&#x2F;html;charset&#x3D;utf-8 * *&#x2F; @RequestMapping(value &#x3D; &quot;&#x2F;handle02&quot;,method &#x3D; RequestMethod.POST ) public String handle02() &#123; System.out.println(&quot;handle02...&quot;); return &quot;success&quot;; &#125; &#x2F;** * 2、params : 规定请求参数 params 和 headers支持简单的表达式： * ① param1: 表示请求必须包含名为 param1 的请求参数 * eg：params&#x3D;&#123;&quot;username&quot;&#125;: * 发送请求的时候必须带上一个名为username的参数；没带都会404 * 测试的时候直接在地址栏输入一个?username&#x3D;111(随便数字，?代表传参) * http:&#x2F;&#x2F;localhost:8080&#x2F;1.SpringMVC_HelloWorld&#x2F;haha&#x2F;handle03?username&#x3D;11 * ⑤ !param1: 表示请求不能包含名为 param1 的请求参数 * eg:params&#x3D;&#123;&quot;!username&quot;&#125; * 发送请求的时候必须不携带上一个名为username的参数；带了都会404 * ③ param1 !&#x3D; value1: 表示请求包含名为 param1 的请求参数，但其值不能为 value1 * eg：params&#x3D;&#123;&quot;username!&#x3D;123&quot;&#125; * 发送请求的时候;携带的username值必须不是123(不带username或者username不是123) * * &#123;“param1&#x3D;value1”, “param2”&#125;: 请求必须包含名为 param1 和param2 的两个请求参数，且 param1 参数的值必须为 value1 * eg:params&#x3D;&#123;&quot;username!&#x3D;123&quot;,&quot;pwd&quot;,&quot;!age&quot;&#125; * 请求参数必须满足以上规则； * 请求的username不能是123，必须有pwd的值，不能有age * * @return *&#x2F; @RequestMapping(value &#x3D; &quot;&#x2F;handle03&quot;,params &#x3D; &#123;&quot;username&quot;&#125;) public String handle03() &#123; System.out.println(&quot;handle03......&quot;); return &quot;success&quot;; &#125; &#x2F;** * 3、headers：规定请求头；也和params一样能写简单的表达式 提交 * User-Agent: 浏览器信息 * 让火狐能访问，360不能访问 * 360：User-Agent:Mozilla&#x2F;5.0 (Windows NT 6.1; WOW64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;63.0.3239.132 Safari&#x2F;537.36 * 火狐：User-Agent:Mozilla&#x2F;5.0 (Windows NT 6.1; Win64; x64; rv:70.0) Gecko&#x2F;20100101 Firefox&#x2F;70.0 * @return *&#x2F; @RequestMapping(value &#x3D; &quot;&#x2F;handle04&quot;,headers &#x3D; &#123;&quot;User-Agent&#x3D;Mozilla&#x2F;5.0 (Windows NT 6.1; Win64; x64; rv:70.0) Gecko&#x2F;20100101 Firefox&#x2F;70.0&quot;&#125;) public String handle04() &#123; System.out.println(&quot;handle04......&quot;); return &quot;success&quot;; &#125; &#x2F;** * 4.consumes : 只接收内容类型是哪种的请求，也就是规定请求头中的Content-Type * 5.produces : 告诉浏览器返回的内容类型是什么，给响应头中加上Content-Type:text&#x2F;html;charest&#x3D;utf-8 *&#x2F; 3.4、测试RequestMapping的模糊匹配功能在index.jsp中添加1234567&lt;hr&#x2F;&gt;&lt;h1&gt;RequestMapping-Ant风格的URL&lt;&#x2F;h1&gt;&lt;!-- href里写的是@RequestMapping里的 --&gt;&lt;a href&#x3D;&quot;antTest01&quot;&gt;精确请求-antTest01&lt;&#x2F;a&gt;&lt;br&#x2F;&gt;&lt;a href&#x3D;&quot;user&#x2F;admin&quot;&gt;测试PathVariable&lt;&#x2F;a&gt;&lt;br&#x2F;&gt; 测试实例RequestMappingTest.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697package com.lizhi.controller;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PatchMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.PostMapping;import org.springframework.web.bind.annotation.RequestMapping;&#x2F;** * 测试RequestMapping的模糊匹配功能 *URL可以写模糊的通配符 * 1、 ? : 能替代任意一个字符 * 2、 * : 能替代任意多个字符和一层路径 * 3、 ** : 能替代多层路径 * *&#x2F;@Controllerpublic class RequestMappingTest &#123; &#x2F;&#x2F;@PostMapping(value &#x3D; &quot;&#x2F;antTest01&quot;) @RequestMapping(&quot;&#x2F;antTest01&quot;) public String antTest01() &#123; System.out.println(&quot;antTest01......被调用了&quot;); return &quot;success&quot;; &#125; &#x2F;** * 加上?之后会发现在URL中antTes0后面可以跟任意一个字符 * 只能加一个字符，0个一个都不行 * http:&#x2F;&#x2F;localhost:8080&#x2F;1.SpringMVC_HelloWorld&#x2F;antTest0X * (在第一个精确匹配里测试就行，浏览器地址输入) * 模糊和精确多个情况下，精确优先（如果后面跟的是字符1,则优先上面的antTes01方法） * @return *&#x2F; @RequestMapping(&quot;&#x2F;antTest0?&quot;) public String antTest02() &#123; System.out.println(&quot;antTest02......被调用了&quot;); return &quot;success&quot;; &#125; &#x2F;** * 后面跟任何字符都可以访问 * http:&#x2F;&#x2F;localhost:8080&#x2F;1.SpringMVC_HelloWorld&#x2F;antTest01jksdgahfjkdfgv * @return *&#x2F; @RequestMapping(&quot;&#x2F;antTest0*&quot;) public String antTest03() &#123; System.out.println(&quot;antTest03......被调用了&quot;); return &quot;success&quot;; &#125; &#x2F;**但是只能匹配一层路径 * http:&#x2F;&#x2F;localhost:8080&#x2F;1.SpringMVC_HelloWorld&#x2F;a&#x2F;as&#x2F;antTestjdfhgk * @return *&#x2F; @RequestMapping(&quot;&#x2F;a&#x2F;*&#x2F;antTest*&quot;) public String antTest04() &#123; System.out.println(&quot;antTest04......被调用了&quot;); return &quot;success&quot;; &#125; &#x2F;** * 可以匹配多层路径 * http:&#x2F;&#x2F;localhost:8080&#x2F;1.SpringMVC_HelloWorld&#x2F;a&#x2F;as&#x2F;ss&#x2F;antTestjdfhgk *&#x2F; @RequestMapping(&quot;&#x2F;a&#x2F;**&#x2F;antTest*&quot;) public String antTest05() &#123; System.out.println(&quot;antTest05......被调用了&quot;); return &quot;success&quot;; &#125; &#x2F;*路径上可以有占位符： 占位符的使用可以再任意路径的地方写一个&#123;变量名&#125; * 假设发&#x2F;user&#x2F;admin 再发&#x2F;user&#x2F;lizhi * 即&#x2F;user下的内容一直在变&#123;username&#125;可以用大括号作为占位符 * http:&#x2F;&#x2F;localhost:8080&#x2F;1.SpringMVC_HelloWorld&#x2F;user&#x2F;lizhi * 注意路径上的占位符，只能占一层路径，当然也可以用两个占位符&#x2F;&#123;user&#125;&#x2F;&#123;username&#125; *&#x2F; @RequestMapping(&quot;&#x2F;&#123;user&#125;&#x2F;&#123;username&#125;&quot;) public String pathVariableTest(@PathVariable(&quot;user&quot;)String user ,@PathVariable(&quot;username&quot;)String username) &#123; System.out.println(&quot;路径上的占位符&quot;+user); System.out.println(&quot;路径上的占位符&quot;+username); return &quot;success&quot;; &#125; &#125;&#96;&#96;&#96;&#96;### &lt;h id&#x3D;&quot;4&quot;&gt;四、测试实例源码&lt;&#x2F;h&gt;#### 动态web工程，新建的时候点next，然后勾选创建web.xml![](软件开发框架之SpringMVC&#x2F;3-3-4.png)### &#x2F;1.SpringMVC_HelloWorld&#x2F;WebContent&#x2F;WEB-INF&#x2F;pages#### success.jsp &lt;%@ page language=”java” contentType=”text/html; charset=UTF-8” pageEncoding=”UTF-8”%&gt; Insert title here 成功！ 1#### error.jsp Insert title here 错误! 1### &#x2F;1.SpringMVC_HelloWorld&#x2F;WebContent&#x2F;WEB-INF&#x2F;web.xml 55 index.jsp &lt;servlet&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- servlet初始化参数 --&gt; &lt;init-param&gt; &lt;!-- contextConfigLocation : 指定SpringMVC配置文件的位置 --&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;!-- 由于不是在一级目录src --&gt; &lt;param-value&gt;classpath*:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;!-- servlet启动加载，servlet原本是第一次访问创建对象 load-on-startup : 服务器启动的时候创建对象；值越小，优先级越高，月先创建对象 --&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;!-- Map all requests to the DispatcherServlet for handling --&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;/servlet-name&gt; &lt;!-- /*和/都是拦截所有请求 /*的范围更大，还会拦截到*.jsp这些请求，但是拦截jsp页面就不能显示了 /也会拦截所有请求，但不会拦截*.jsp;jsp显示正常 处理*.jsp是tomcat做的事所有项目的小web.xml都继承与大web.xml DefaultServlet是Tomcat中处理静态资源的？ 除过jsp，和servlet外剩下的都是静态资源； index.html：静态资源，tomcat就会在服务器下找到这个资源并返回; 我们前端控制器的/禁用了tomcat服务器中的DefaultServlet 1、服务器的大web.xml(Servers/Tomcat v9.0 Server at localhost-config/web.xml) 中有一个DefaultServlet是url-pattern=/ 2、我们的配置中前端控制器url-pattern=/ 静态资源会来到DispatcherServlet（前端控制器）看那个方法的RequestMapping是这个index.html 3）为什么jsp又能访问；因为我们没有覆盖服务器中的JspServlet的配置 4） /* 直接就是拦截所有请求；我们写/；也是为了迎合后来Rest风格的URL地址 --&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 12#### &#x2F;1.SpringMVC_HelloWorld&#x2F;WebContent&#x2F;index.html Insert title here HelloWorld 12#### &#x2F;1.SpringMVC_HelloWorld&#x2F;WebContent&#x2F;index.jsp Insert title here HelloWorld RequestMapping测试 test01-写在发方法上的requestMapping test01-写在发方法上的requestMapping 测试RequestMapping的属性 handle02-测试method属性，下面提交的方法为POST handle03-测试params属性 handle04-测试headers RequestMapping-Ant风格的URL 精确请求-antTest01 测试PathVariable 1#### springmvc.xml &lt;context:component-scan base-package=&quot;com.lizhi.controller&quot;&gt;&lt;/context:component-scan&gt; &lt;!-- 这样在测试的时候只要写jsp名就行，不用写地址 --&gt; &lt;bean class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/pages/&quot;&gt;&lt;/property&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;&gt;&lt;/property&gt; &lt;/bean&gt; 1234### com.lizhi.controller#### MyFirstController.java package com.lizhi.controller; import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping; /** 1.告诉SpringMVC这是一个处理器，可以处理请求 @Controller;标识哪个组件是控制器 /@Controllerpublic class MyFirstController { /** /代表从当前项目先开始，处理当前项目下的hello请求（在index.jsp中） /@RequestMapping(“/hello”)public String myfirstRequest() { System.out.println(“请求收到了。。。。。。正在处理中”); //在springmvc.xml中配置了视图解析器，所以只要写名就行 /** prefix是前缀，suffix是后缀 /return “success”;} /* 这个hello01没有写到index.jsp中，但是在地址栏输入可以直接跳转 /@RequestMapping(“/hello01”)public String hanle01(){ System.out.println(“hanle01….被调用”); return “error”;}@RequestMapping(“/hello02”)public String hanle02(){ return “success”;} } 12#### RequestMappingTestController.java package com.lizhi.controller; import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod; /* 用来测试@RequestMapping / /** @RequestMapping(“haha”)标在类上，为当前所有类的请求地址 指定一个基准路径都是从/haha开始的 由http://localhost:8080/1.SpringMVC_HelloWorld/handle01 变成http://localhost:8080/1.SpringMVC_HelloWorld/haha/handle01 /@RequestMapping(“/haha”)@Controllerpublic class RequestMappingTestController { @RequestMapping(“/handle01”) public String handle01() { System.out.println(&quot;RequestMappingTestController...handle01&quot;); return &quot;success&quot;; } /** RequestMapping的其他属性 1、method : 限定请求方式 GET、POST method = RequestMethod.POST;只有POST类型的请求才接受，默认全接受 不规定的方式报错： 4XX:都是客户端错误 HTTP Status 405 – 方法不允许 Type Status Report Request method &#39;GET&#39; not supported 请求行中接收的方法由源服务器知道，但目标资源不支持 consumes：只接受内容类型是哪种的请求，规定请求头中的Content-Type produces：告诉浏览器返回的内容类型是什么，给响应头中加上Content-Type:text/html;charset=utf-8 /@RequestMapping(value = “/handle02”,method = RequestMethod.POST )public String handle02() { System.out.println(“handle02…”); return “success”;} /** 2、params : 规定请求参数 params 和 headers支持简单的表达式： ① param1: 表示请求必须包含名为 param1 的请求参数 eg：params=&#123;&quot;username&quot;&#125;: 发送请求的时候必须带上一个名为username的参数；没带都会404 测试的时候直接在地址栏输入一个?username=111(随便数字，?代表传参) http://localhost:8080/1.SpringMVC_HelloWorld/haha/handle03?username=11 ⑤ !param1: 表示请求不能包含名为 param1 的请求参数 eg:params=&#123;&quot;!username&quot;&#125; 发送请求的时候必须不携带上一个名为username的参数；带了都会404 ③ param1 != value1: 表示请求包含名为 param1 的请求参数，但其值不能为 value1 eg：params=&#123;&quot;username!=123&quot;&#125; 发送请求的时候;携带的username值必须不是123(不带username或者username不是123) &#123;“param1=value1”, “param2”&#125;: 请求必须包含名为 param1 和param2 的两个请求参数，且 param1 参数的值必须为 value1 eg:params=&#123;&quot;username!=123&quot;,&quot;pwd&quot;,&quot;!age&quot;&#125; 请求参数必须满足以上规则； 请求的username不能是123，必须有pwd的值，不能有age @return /@RequestMapping(value = “/handle03”,params = {“username”})public String handle03() { System.out.println(“handle03……”); return “success”;} /** 3、headers：规定请求头；也和params一样能写简单的表达式 User-Agent: 浏览器信息 让火狐能访问，360不能访问 360：User-Agent:Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36 火狐：User-Agent:Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:70.0) Gecko/20100101 Firefox/70.0 @return /@RequestMapping(value = “/handle04”,headers = {“User-Agent=Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:70.0) Gecko/20100101 Firefox/70.0”})public String handle04() { System.out.println(“handle04……”); return “success”; } /** 4.consumes : 只接收内容类型是哪种的请求，也就是规定请求头中的Content-Type 5.produces : 告诉浏览器返回的内容类型是什么，给响应头中加上Content-Type:text/html;charest=utf-8 / } 12#### RequestMappingTest.java package com.lizhi.controller; import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PatchMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.PostMapping;import org.springframework.web.bind.annotation.RequestMapping; /** 测试RequestMapping的模糊匹配功能 URL可以写模糊的通配符 1、 ? : 能替代任意一个字符 2、 * : 能替代任意多个字符和一层路径 3、 ** : 能替代多层路径 /@Controllerpublic class RequestMappingTest { //@PostMapping(value = “/antTest01”) @RequestMapping(“/antTest01”) public String antTest01() { System.out.println(&quot;antTest01......被调用了&quot;); return &quot;success&quot;; } /** 加上?之后会发现在URL中antTes0后面可以跟任意一个字符 只能加一个字符，0个一个都不行 http://localhost:8080/1.SpringMVC_HelloWorld/antTest0X (在第一个精确匹配里测试就行，浏览器地址输入) 模糊和精确多个情况下，精确优先（如果后面跟的是字符1,则优先上面的antTes01方法） @return /@RequestMapping(“/antTest0?”)public String antTest02() { System.out.println(“antTest02……被调用了”); return “success”;} /** 后面跟任何字符都可以访问 http://localhost:8080/1.SpringMVC_HelloWorld/antTest01jksdgahfjkdfgv @return /@RequestMapping(“/antTest0*”)public String antTest03() { System.out.println(“antTest03……被调用了”); return “success”;} /**但是只能匹配一层路径 http://localhost:8080/1.SpringMVC_HelloWorld/a/as/antTestjdfhgk @return /@RequestMapping(“/a//antTest“)public String antTest04() { System.out.println(“antTest04……被调用了”); return “success”;} /** 可以匹配多层路径 http://localhost:8080/1.SpringMVC_HelloWorld/a/as/ss/antTestjdfhgk /@RequestMapping(“/a/**/antTest*”)public String antTest05() { System.out.println(“antTest05……被调用了”); return “success”;} /*路径上可以有占位符： 占位符的使用可以再任意路径的地方写一个{变量名} 假设发/user/admin 再发/user/lizhi 即/user下的内容一直在变&#123;username&#125;可以用大括号作为占位符 http://localhost:8080/1.SpringMVC_HelloWorld/user/lizhi 注意路径上的占位符，只能占一层路径，当然也可以用两个占位符/&#123;user&#125;/&#123;username&#125; / @RequestMapping(“/{user}/{username}”) public String pathVariableTest(@PathVariable(“user”)String user ,@PathVariable(“username”)String username) { System.out.println(&quot;路径上的占位符&quot;+user); System.out.println(&quot;路径上的占位符&quot;+username); return &quot;success&quot;; } }","categories":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/tags/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"},{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://www.studyz.club/tags/SpringMVC/"}]},{"title":"Python数据分析与应用-numpy","slug":"Python数据分析与应用-numpy","date":"2019-11-13T23:48:43.645Z","updated":"2019-11-28T14:26:35.700Z","comments":true,"path":"posts/2cd80fbe/","link":"","permalink":"http://www.studyz.club/posts/2cd80fbe/","excerpt":"","text":"一、什么是numpy 二、 认识NumPy数组对象 三、创建numpy数组 3.1 数组的形状与变换&lt; 四、数组数据类型 4.1 random模块随机数生成函数 五、数组运算 六、ndarray的索引和切片 6.1 花式（数组）索引的基本使用 6.2 布尔型索引的基本使用 6.3、数组的转置与轴对称 6.4、高维数组的轴问题 七、Numpy通用函数 八、利用NumPy数组进行数据处理 8.1、将条件逻辑转为数组运算 8.2、数组统计运算 8.3、数组排序 8.4、检索数组元素 8.5、唯一化及其他集合逻辑 九、线性代数模块 一、什么是numpy 一个在Python中做科学计算的基础库，重在数值计算，也是大部分PYTHON科学计算库的基础库，多用于在大型、多维数组上执行数值运算 二、 认识NumPy数组对象 属性 说明 ndarray.ndim 返回 int。表示数组的维数 ndarray.shape 返回 tuple。表示数组的尺寸，对于 n 行 m 列的矩阵，形状为(n,m) ndarray.size 返回 int。表示数组的元素总数，等于数组形状的乘积 ndarray.dtype 返回 data-type。描述数组中元素的类型 ndarray.itemsize 返回 int。表示数组的每个元素的大小（以字节为单位）。 ndarray对象中存储元素的类型必须是相同的 三、创建numpy数组1、numpy.array(object, dtype=None, copy=True, order=’K’,subok=False, ndmin=0) 参数名称 说明 object 接收array。表示想要创建的数组。无默认。 dtype 接收data-type。表示数组所需的数据类型。如果未给定，则选择保存对象所需的最小类型。默认为None。 ndmin 接收int。指定生成数组应该具有的最小维数。默认为None。 3.1 数组的形状与变换 shape的第一个数表示行，第二个数表示列 四、数组数据类型 在使用array函数创建数组时，数组的数据类型默认是浮点型。自定义数组数据，则可以预先指定数据类型 4.1 random模块随机数生成函数 random模块常用随机数生成函数 函数 说明 seed 确定随机数生成器的种子。 permutation 返回一个序列的随机排列或返回一个随机排列的范围。 shuffle 对一个序列进行随机排序。 binomial 产生二项分布的随机数。 normal 产生正态（高斯）分布的随机数。 beta 产生beta分布的随机数。 chisquare 产生卡方分布的随机数。 gamma 产生gamma分布的随机数。 uniform 产生在[0,1)中均匀分布的随机数。 无约束条件下生成随机数 In[25]: print(‘生成的随机数组为：’,np.random.random(100)) Out[25]: 生成的随机数组为：[ 0.15343184 0.51581585 0.07228451 … 0.24418316 0.92510545 0.57507965] round取小数 生成服从均匀分布的随机数 生成服从正态分布的随机数 生成给定上下范围的随机数，如创建一个最小值不低于 2、最大值不高于 10 的 2 行 5 列数组 五、数组运算 5.1、数组广播 广播原则： 如果两个数组的后缘维度(即：从末尾开始算起的维度)的轴长相符或其中一方的长度为1，则认为它们是广播兼容的，广播会在缺失和(或)长度为1的轴上进行. 数组在矢量化运算时，要求数组的形状是相等的。当形状不相等的数组执行算术运算的时候，就会出现广播机制，该机制会对数组进行扩展，然后就可以进行矢量化运算了。 使用数组的广播机制需要满足一下两个条件中的任意一个 数组的某一维度等长 其中的一个数组的某一维度为1 广播机制需要扩展维度小的数组，使得它与维度最大的数组的shape值相同，以便使用元素级函数或者运算符进行运算。 根据广播原则：arr1的shape为(4,1),arr2的shape为(3,),所以会同时在两个轴发生广播，arr1的shape变成(4，3)，而arr2的shape变成(4,3),所以结果也为(4,3). 5.2、数组与标量间的运算 六、ndarray的索引和切片 ndarray除了使用整数进行索引以外，还可以使用整数数组和布尔数组进行索引。 对于一维数组 对于二维数组 在二维数组中，每个索引位置上的元素不再是一个标量了，而是一个一维数组。如图，如果我们想要获取数组的单个元素，必须同时指定这个元素的行索引和列索引 多维数组 相比于一维数组，二维数组的切片方式花样更多，多维数组的切片是沿着行或列的方向选取元素，我们可以传入一个切片，也可以传入多个切片，还可以将切片与整数索引混合使用。 6.1 花式（数组）索引的基本使用 花式索引是NumPy的一个术语，是指用整数数组，或列表进行索引，然后再将数组或列表中的每个元素作为下标进行取值。 当使用一个数组或列表作为索引时，如果使用索引要操作的对象是一维数组，则获取的结果是对应下标的元素。 如果要操作的对象是一个二维数组，则获取的结果就是对应下标的一行数据。 6.2 布尔型索引的基本使用 布尔型索引指的是将一个布尔数组作为数组索引,返回的数据是布尔数组中True对应位置的值。 6.3、数组的转置与轴对称 如果我们不输入任何参数，直接调用transpose()方法，则其执行的效果就是将数组进行转置，作用等价于transpose(2,1,0)。 6.4、高维数组的轴问题 七、Numpy通用函数 通用函数(ufunc) 是一-种针对ndarray中的数据执行元素级运算的函数，函数返回的是一个新的数组。Numpy提供了诸如“sin”、”cos”、”exp”等常见函数 我们将ufunc中接收一个数组参数的函数称为一元通用函数，接受两个数组参数的则称为二元通用函数。常见的一元通用函数 常见的二元通用函数 八、利用NumPy数组进行数据处理 NumPy数组可以将许多数据处理任务转换为简洁的数组表达式，它处理数据的速度要比内置的Python循环快了至少一个数量级。 8.1、将条件逻辑转为数组运算 NumPy的where()函数是三元表达式x if condition else y的矢量版本 从输出结果可以看到，where函数后返回了一个新的数组 8.2、数组统计运算 通过NumPy库中的相关方法，我们可以很方便地运用Python进行数组的统计汇总。 8.3、数组排序 如果希望对NumPy数组中的元素进行排序，可以通过sort()方法实现 也可以通过arr.sort(axis=)的形式指定轴 8.4、检索数组元素 all()函数用于判断整个数组中的元素的值是否全部满足条件，如果满足条件返回True，否则返回False。any()函数用于判断整个数组中的元素至少有一个满足条件就返回True，否则就返回False。 8.5、唯一化及其他集合逻辑 针对一维数组，NumPy提供了unique()函数来找出数组中的唯一值，并返回排序后的结果。 in1d()函数用于判断数组中的元素是否在另一个数组中存在，该函数返回的是-个布尔型的数组。 8.5.1、关于集合的其他函数 九、线性代数模块 线性代数模块是数学运算中的一个重要工具，它在图形信号处理，音频信号处理中起非常重要的作用。numpy.linalg模块中有一组 标准的矩阵分解运算以及诸如逆和行列式之类的东西。例如，矩阵相乘，如果我们通过“*”对两个数组相乘的话，得到的是一个元素级的积，而不是一个矩阵点积。 矩阵点积的条件是矩阵A的列数等于矩阵B的行数，假设A为 m x p的矩阵，B为p x n的矩阵,那么矩阵A与B的乘积就是一个m*n的矩阵C，其中矩阵C的第i行第j列的元素可以表示为: 除此之外，``linalg`模块中还提供了其他很多有用的函数。","categories":[{"name":"Python数据分析与应用","slug":"Python数据分析与应用","permalink":"http://www.studyz.club/categories/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%BA%94%E7%94%A8/"},{"name":"numpy","slug":"Python数据分析与应用/numpy","permalink":"http://www.studyz.club/categories/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%BA%94%E7%94%A8/numpy/"}],"tags":[{"name":"Python数据分析与应用","slug":"Python数据分析与应用","permalink":"http://www.studyz.club/tags/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%BA%94%E7%94%A8/"},{"name":"Python numpy","slug":"Python-numpy","permalink":"http://www.studyz.club/tags/Python-numpy/"}]},{"title":"Python数据分析与应用-概述","slug":"Python数据分析与应用1","date":"2019-11-13T11:39:38.253Z","updated":"2019-11-19T15:46:48.506Z","comments":true,"path":"posts/e99c4157/","link":"","permalink":"http://www.studyz.club/posts/e99c4157/","excerpt":"","text":"一、 什么使数据分析 二、熟悉Python数据分析的工具 一、 什么使数据分析 数据分析是用适当的方法对收集来的大量数据进行分析，帮助人们作出判断，以便采取适当行动。 `广义的数据分析包括狭义数据分析和数据挖掘。` 狭义的数据分析是指根据分析目的，采用对比分析、分组分析、交叉分析和回归分析等分析方法，对收集来的数据进行处理与分析，提取有价值的信息，发挥数据的作用，得到一个特征统计量结果的过程。 数据挖掘则是从大量的、不完全的、有噪声的、模糊的、随机的实际应用数据中，通过应用聚类、分类、回归和关联规则等技术，挖掘潜在价值的过程。 1. 典型的数据分析的流程 需求分析：数据分析中的需求分析也是数据分析环节的第一步和最重要的步骤之一，决定了后续的分析的方向、方法。 数据获取：数据是数据分析工作的基础，是指根据需求分析的结果提取，收集数据。 数据预处理：数据预处理是指对数据进行数据合并，数据清洗，数据变换和数据标准化，数据变换后使得整体数据变为干净整齐，可以直接用于分析建模这一过程的总称。 分析与建模：分析与建模是指通过对比分析、分组分析、交叉分析、回归分析等分析方法和聚类、分类、关联规则、智能推荐等模型与算法发现数据中的有价值信息，并得出结论的过程。 模型评价与优化：模型评价是指对已经建立的一个或多个模型，根据其模型的类别，使用不同的指标评价其性能优劣的过程。 部署：部署是指将通过了正式应用数据分析结果与结论应用至实际生产系统的过程。 2.用处 客户分析 营销分析 社交媒体分析 网络安全识别潜在攻击者 设备管理 交通物流分析 欺诈行为检测 二、熟悉Python数据分析的工具1、Python 数据分析主要包含以下 5 个方面优势 语法简单精练。对于初学者来说，比起其他编程语言，Python更容易上手。 有很强大的库。可以只使用Python这一种语言去构建以数据为中心的应用程序。 功能强大。Python是一个混合体，丰富的工具集使它介于传统的脚本语言和系统语言之间。Python不仅具备所有脚本语言简单和易用的特点，还提供了编译语言所具有的高级软件工程工具。 不仅适用于研究和原型构建，同时也适用于构建生产系统。研究人员和工程技术人员使用同一种编程工具，会给企业带来非常显著的组织效益，并降低企业的运营成本。 Python是一门胶水语言。Python程序能够以多种方式轻易地与其他语言的组件“粘接”在一起。 2、IPython——科学计算标准工具集的组成部分 是一个增强的Python shell，目的是提高编写、测试、调试Python代码的速度。 主要用于交互式数据并行处理，是分布式计算的基础架构。 提供了一个类似于Mathematica的HTML笔记本，一个基于Qt框架的GUI控制台，具有绘图、多行编辑以及语法高亮显示等功能。 3、NumPy(Numerical Python)—— Python 科学计算的基础包 快速高效的多维数组对象 ndarray。 对数组执行元素级的计算以及直接对数组执行数学运算的函数。 读写硬盘上基于数组的数据集的工具。 线性代数运算、傅里叶变换，以及随机数生成的功能。 将 C、C++、Fortran 代码集成到 Python 的工具。 4、SciPy——专门解决科学计算中各种标准问题域的模块的集合 scipy.integrate 数值积分例程和微分方程求解器 scipy.linalg 扩展了由 numpy.linalg 提供的线性代数例程和矩阵分解功能 scipy.optimize 函数优化器（最小化器）以及根查找算法 scipy.signal 信号处理工具 scipy.sparse 稀疏矩阵和稀疏线性系统求解器 scipy.special SPECFUN（这是一个实现了许多常用数学函数的 Fortran 库）的包装器 scipy.stats 检验连续和离散概率分布、各种统计检验方法，以及更好的描述统计法 scipy.weave 利用内联 C++代码加速数组计算的工具 5、Pandas——数据分析核心库 提供了一系列能够快速、便捷地处理结构化数据的数据结构和函数。 高性能的数组计算功能以及电子表格和关系型数据库（如 SQL）灵活的数据处理功能。 复杂精细的索引功能，以便便捷地完成重塑、切片和切块、聚合及选取数据子集等操作。 6、Matplotlib——绘制数据图表的 Python 库 Python的2D绘图库，非常适合创建出版物上用的图表。 操作比较容易，只需几行代码即可生成直方图、功率谱图、条形图、错误图和散点图等图形。 提供了pylab的模块，其中包括了NumPy和pyplot中许多常用的函数，方便用户快速进行计算和绘图。 交互式的数据绘图环境，绘制的图表也是交互式的。 7、scikit-learn——数据挖掘和数据分析工具 简单有效，可以供用户在各种环境下重复使用。 封装了一些常用的算法方法。 基本模块主要有数据预处理、模型选择、分类、聚类、数据降维和回归 6 个，在数据量不大的情况下，scikit-learn可以解决大部分问题。 8、Spyder——交互式 Python 语言开发环境 提供高级的代码编辑、交互测试和调试等特性。 包含数值计算环境。 可用于将调试控制台直接集成到图形用户界面的布局中。 模仿MATLAB的“工作空间”，可以很方便地观察和修改数组的值。 三、Anaconda1. Anaconda 预装了大量常用 Packages。 完全开源和免费。 额外的加速和优化是收费的，但对于学术用途，可以申请免费的 License。 对全平台和几乎所有Python版本支持。 2、Jupyter Notebook 的界面及其构成快捷键 “Esc”键：进入命令模式 “Y” 键：切换到代码单元 “M”键：切换到 Markdown 单元 “B”键：在本单元的下方增加一单元 “H”键：查看所有快捷命令 “Shift＋Enter”组合键：运行代码 可写md文档导出功能 Notebook 还有一个强大的特性，就是导出功能。可以将 Notebook 导出为多种格式，如HTML、Markdown、reST、PDF（通过 LaTeX）等格式。 导出功能可通过选择“File”→“Download as”级联菜单中的命令实现。","categories":[{"name":"Python数据分析与应用","slug":"Python数据分析与应用","permalink":"http://www.studyz.club/categories/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%BA%94%E7%94%A8/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://www.studyz.club/tags/Python/"},{"name":"Python数据分析与应用","slug":"Python数据分析与应用","permalink":"http://www.studyz.club/tags/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%BA%94%E7%94%A8/"}]},{"title":"数据挖据概念与技术-引论","slug":"数据挖掘概念与技术-引论","date":"2019-11-13T11:39:38.238Z","updated":"2019-11-19T15:46:48.506Z","comments":true,"path":"posts/d72bb692/","link":"","permalink":"http://www.studyz.club/posts/d72bb692/","excerpt":"","text":"一、为什么进行数据挖掘","categories":[{"name":"数据挖据概念与技术","slug":"数据挖据概念与技术","permalink":"http://www.studyz.club/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8D%AE%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%8A%80%E6%9C%AF/"}],"tags":[{"name":"-数据挖掘概念与技术  -引论","slug":"数据挖掘概念与技术-引论","permalink":"http://www.studyz.club/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%8A%80%E6%9C%AF-%E5%BC%95%E8%AE%BA/"}]},{"title":"Hadoop-Hbase-分布式安装及注意事项","slug":"Hadoop-Hbase-伪分布式安装及注意事项","date":"2019-11-11T14:26:27.711Z","updated":"2019-12-24T01:13:47.935Z","comments":true,"path":"posts/99fdfd17/","link":"","permalink":"http://www.studyz.club/posts/99fdfd17/","excerpt":"","text":"一、准备工作 二、Hbase安装 三、Hbase集群启动 四、注意事项及出现的问题 Hbase搭配高可用 一、准备工作配置前的环境 环境 hadoop2.7.3 jdk1.8.0_91 hbase1.3.5 hadoop,和jdk的版本有点老了，大数据的组件多，每次配置前都应先仔细查询各个版本的插件是否兼容。 我们的目标是在slave1,slave2,slave3中配置。配置是在master中配置再分发 192.168.43.159 master192.168.43.160 slave1192.168.43.161 slave2192.168.43.162 slave3 hbase下载地址二、Hbase安装(我们为master机也配置了zookeeper,将master作为Hmaster，slave1作为备份节点)1. 将下载好的hbase-1.3.5-bin.tar.gz上传到/opt目录中2.解压hbase-1.3.5-bin.tar.gz并放在/usr/local/目录1tar -zxvf hbase-1.3.5-bin.tar.gz -C &#x2F;usr&#x2F;local&#x2F; 3.进入到/usr/local/hbase1.3.5/conf/目录1vi hbase-env.sh ①配置JAVA_HOME路径12export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_91export HBASE_MANAGES_ZK&#x3D;false ②配置HBASE_MANAGES_ZK属性为false，我们这里不使用hbase自带的zookeeper1export HBASE_MANAGES_ZK&#x3D;false 4.修改hbase-site.xml1234567891011121314151617181920212223&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;&#x2F;name&gt; &lt;value&gt;hdfs:&#x2F;&#x2F;master:8020&#x2F;hbase&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;&#x2F;name&gt; &lt;value&gt;true&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;hbase.master.port&lt;&#x2F;name&gt; &lt;value&gt;16000&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;&#x2F;name&gt; &lt;value&gt;master:2181,slave1:2181,slave2:2181,slave3:2181&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;&#x2F;name&gt; &lt;value&gt;&#x2F;opt&#x2F;zookeeper&#x2F;data&lt;&#x2F;value&gt; &lt;&#x2F;property&gt;&lt;&#x2F;configuration&gt; 5.修改regionservers文件,删除原本内容后添加12345masterslave1slave2slave3 6.考虑到兼容性的问题，我们将hbase里的hadoop和zookeeper的jar包换成我们已安装的。①删除原有的jar包1rm -rf &#x2F;usr&#x2F;local&#x2F;hbase-1.3.5&#x2F;lib&#x2F;hadoop-*.jar 1rm -rf &#x2F;usr&#x2F;local&#x2F;hbase-1.3.5&#x2F;lib&#x2F;zookeeper-3.4.6.jar ②从hadoop的安装文件夹里拷贝新的jar包([hadoop开头的所有jar包，虽然有些hbase不需要，但为了方便全部拷贝)1find &#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F; -name hadoop-*.jar | xargs -i cp &#123;&#125; &#x2F;usr&#x2F;local&#x2F;hbase-1.3.5&#x2F;lib&#x2F; 在slave1中找到/opt/zookeeper/下的zookeeper-3.4.14.jar也可以直接在slave1中，这样 1scp -r &#x2F;opt&#x2F;zookeeper&#x2F;zookeeper-3.4.14.jar &#x2F;usr&#x2F;local&#x2F;hbase-1.3.5&#x2F;lib&#x2F; 7.将hadoop配置文件中的core-site.xml和hdfs-site.xml拷贝到hbase的配置文件中8.将hbase的目录分发到集群中的其他节点(这里最好是做软连接)12scp -r &#x2F;usr&#x2F;local&#x2F;hbase-1.3.5 slave1:&#x2F;usr&#x2F;local&#x2F;hbase-1.3.5&#x2F; 这里要特别注意一家结尾的/的问题，文件与文件夹不同 三、Hbase集群启动1.在启动hbase之前必须先启动zookeeper和hdfs在master机器 1start-dfs.sh 在slave1 1start-yarn.sh 在slave1,slave2,slave3 1zkServer.sh start 在master和slave1中 1hadoop-daemon.sh start zkfc 在slave1,slave2,slave3 1yarn-daemon.sh start resourcemanager 2.启动Hbase①单独启动1hbase-daemon.sh start master 1hbase-daemon.sh start regionserver ②启动脚本启动1start-hbase.sh 3.Hbase的WebUI1http:&#x2F;&#x2F;192.168.43.159:16010 四、注意事项及出现的问题1.即使防火墙没有开启，端口还是无法打开，因此我们要手动放行需要用到的端口打开端口的配置文件12vi &#x2F;etc&#x2F;sysconfig&#x2F;iptables 添加如下的端口12345678-A INPUT -p tcp -m state --state NEW -m tcp --dport 16010 -j ACCEPT-A INPUT -p tcp -m state --state NEW -m tcp --dport 2181 -j ACCEPT-A INPUT -p tcp -m state --state NEW -m tcp --dport 9000 -j ACCEPT-A INPUT -p tcp -m state --state NEW -m tcp --dport 60010 -j ACCEPT-A INPUT -p tcp -m state --state NEW -m tcp --dport 16030 -j ACCEPT-A INPUT -p tcp -m state --state NEW -m tcp --dport 16020 -j ACCEPT 如果网页打不开就要考虑一下端口的问题了 更新配置文件12systemctl restart iptables.service 2.本集群有两个NN，如果两个NN同时在线，由于master没有配置zookeeper，因此必须保持slave1节点是活跃状态，并且hmaster必须和活跃的namenode在一个节点上才能打开webui 3、为了解决这一问题就需要为Hbase也安装高可用① 先进入到hbase的配置文件夹下 1cd &#x2F;usr&#x2F;local&#x2F;hbase-1.3.5&#x2F;conf ② 用vi命令创建文件backup-masters 1vi backup-masters ③ 在文件中添加作为备份Hmaster的节点名称(之前设的master是Hmaster，现在slave1节点作为Hmaster的备份节点) 1slave1 这里需要注意：我们之前为hadoop配置了高可用，这里需要把1234&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;&#x2F;name&gt; &lt;value&gt;hdfs:&#x2F;&#x2F;master:8020&#x2F;hbase&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; 改成 1234&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;&#x2F;name&gt; &lt;value&gt;hdfs:&#x2F;&#x2F;mycluster:8020&#x2F;hbase&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; 并且在core-site.xml中将新加的zookeeper节点master加入 1234&lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;&#x2F;name&gt; &lt;value&gt;master:2181,slave1:2181,slave2:2181,slave3:2181&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; ④ 将修改后的配置文件分发到各个几点中 1scp -r backup-masters slave1:&#x2F;usr&#x2F;local&#x2F;hbase-1.3.5&#x2F;conf&#x2F; ⑤ 重启hbase⑥ 查验 在master机中输入jps命令发现有Hmaster 进入slave1的webUI里⑦ 至此Hbase的高可用配置完成 其他","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/categories/hadoop/"},{"name":"hbase","slug":"hadoop/hbase","permalink":"http://www.studyz.club/categories/hadoop/hbase/"}],"tags":[{"name":"hbase","slug":"hbase","permalink":"http://www.studyz.club/tags/hbase/"}]},{"title":"软件开发框架之MyBatis-缓存机制","slug":"软件开发框架之MyBatis-缓存机制","date":"2019-11-11T14:24:27.658Z","updated":"2019-11-13T00:40:52.875Z","comments":true,"path":"posts/a50b770b/","link":"","permalink":"http://www.studyz.club/posts/a50b770b/","excerpt":"","text":"一、缓存 二、MyBatis缓存机制 2.1 一级缓存 2.2 一级缓存失效的几种情况及原理分析 2.3 注意 三、二级缓存 3.1 二级缓存的使用配置 3.2 &lt;cache&gt;标签的相关属性 3.3 是先读一级缓存呢还是先读二级缓存？ 缓存原理图解 3.4 缓存相关设置 四、整合第三方缓存 4.1 整合ehcache 五、 测试实例源码 一、缓存 缓存在互联网系统中是非常重要的， 其主要作用是将数据保存到内存中， 当用户查询数据 时， 优先从缓存容器中获取数据，而不是频繁地从数据库中查询数据，从而提高查询性能。目 前流行的缓存服务器有MongoDB 、Redis 、Ehcache 等，不同的缓存服务器有不同的应用场景， 不存在孰优孰劣。 ​ MyBatis 提供一级缓存和二级缓存的机制。一级缓存是SqlSession 级别的缓存， 在操作数据 库时， 每个SqlSession 类的实体对象中都有一个HashMap 数据结构可以用来缓存数据，不同的 SqlSession 类的实例对象缓存的HashMap 数据结构互不影响。二级缓存是Mapper 级别的缓存， 多个SqlSession 类的实例对象操作同一个Mapper 配置文件中的SQL 语句，可以共用二级缓存， 二级缓存是跨SqlSession 的。这里需要注意的是，在没有配置的默认情况下， MyBatis 只开启一 级缓存。 缓存的作用 暂时的存储一些数据；加快系统的查询速度 例如：123456CPU: 主频: 4个2.7GHZ 内存: 4G-8G CPU: 一级缓存(4MB) ; 二级缓存(16MB);cpu读内存相当于读别人的东西，读缓存相当于读自己的大脑 CPU即将处理的东西放在一二级缓存，内存里的东西再放在缓存里 数据库交互；1Employee employee &#x3D; employeeDao.getEmpById(1); ① 先拿到链接与数据库建立链接 ② 将sql语句发给数据库，数据库执行完成后返回结果，然后封装返回employee对象 ③ 再一次查询一号员工又从①步骤开始 如果能将第一次查询的内容缓存起来，就能避免频繁的对数据库进行操作 二、MyBatis缓存机制 Map : 能保存查询的一些数据; MyBatis的缓存分为两级 一级缓存 : 线程级别的缓存；本地缓存；SqlSession级别的缓存 每次查询先看一级缓存中有没有，如果没有就去发送新的Sql 二级缓存 ： 全局范围的缓存；除过当前线程；SqlSession能用外其他也可以使用；namespace级别的缓存 2.1 一级缓存 MyBatis一级缓存又叫SqlSession级别的缓存;默认存在1. 只要之前查询过的数据，MyBatis就会保存在一个缓存中(Map);下次获取直接从缓存中拿;例如这个测试实例12345678910111213141516171819202122&#x2F;** * 体会一级缓存 *&#x2F; @Test public void test01() throws IOException &#123; &#x2F;&#x2F;initSqlSessionFactory();如果不用@BeforeEach标注每次测试前都要先调用一下这个方法 SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; TeacherDao mapper &#x3D; openSession.getMapper(TeacherDao.class); Teacher teacher &#x3D; mapper.geTeacherById(1); System.out.println(teacher); System.out.println(&quot;---------------------------------------&quot;); Teacher teacher2 &#x3D; mapper.geTeacherById(1); System.out.println(teacher2); System.out.println(teacher &#x3D;&#x3D; teacher2); &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125; 输出结果12345678DEBUG 11-09 19:25:21,721 &#x3D;&#x3D;&gt; Preparing: select * from t_teacher where id&#x3D;? (BaseJdbcLogger.java:143) DEBUG 11-09 19:25:21,754 &#x3D;&#x3D;&gt; Parameters: 1(Integer) (BaseJdbcLogger.java:143) DEBUG 11-09 19:25:21,776 &lt;&#x3D;&#x3D; Total: 1 (BaseJdbcLogger.java:143) Teacher [id&#x3D;1, name&#x3D;hh, course&#x3D;语文, address&#x3D;让胡路, birth&#x3D;Wed Jul 01 00:00:00 CST 1998]---------------------------------------Teacher [id&#x3D;1, name&#x3D;hh, course&#x3D;语文, address&#x3D;让胡路, birth&#x3D;Wed Jul 01 00:00:00 CST 1998]true 对于输出结果我们可以看到: ① 在第二次查询1号老师的时候并没有重新调用sql语句 ② teacher==teache2,说明是直接从缓存中获取的 2.2 一级缓存失效的几种情况及原理分析1、不同的SqlSession对应不同的一级缓存2、同一个SqlSession但是查询条件不同3、同一个SqlSession两次查询期间执行了任何一次增删改操作会清空缓存4、同一个SqlSession两次查询期间手动清空了缓存1. 不同的SqlSession对应不同的一级缓存1234567891011121314151617181920212223242526272829 &#x2F;** * 一级缓存失效的几种情况 * 一级缓存是SqlSession级别的缓存 * 1.不同的SqlSession对应不同的一级缓存; * 只用在同一个SqlSession期间查询到的数据会保存在这个SqlSession的缓存中； * 下次再使用这个SqlSession查询会从缓存中拿 *&#x2F; @Test public void test02() throws IOException &#123; &#x2F;&#x2F;initSqlSessionFactory();如果不用@BeforeEach标注每次测试前都要先调用一下这个方法 &#x2F;&#x2F;第一个会话 SqlSession openSession &#x3D; sqlSessionFactory.openSession(); TeacherDao teacherDao &#x3D; openSession.getMapper(TeacherDao.class); Teacher teacher &#x3D; teacherDao.geTeacherById(1); System.out.println(teacher); &#x2F;&#x2F;第二个会话 SqlSession openSession2 &#x3D; sqlSessionFactory.openSession(); TeacherDao teacherDao2 &#x3D; openSession.getMapper(TeacherDao.class); Teacher teacher2 &#x3D; teacherDao2.geTeacherById(1); System.out.println(teacher2); System.out.println(teacher&#x3D;&#x3D;teacher2); openSession.close(); openSession2.close(); &#125;&#125; 输出结果1234567891011DEBUG 11-09 20:35:31,544 &#x3D;&#x3D;&gt; Preparing: select * from t_teacher where id&#x3D;? (BaseJdbcLogger.java:143) DEBUG 11-09 20:35:31,577 &#x3D;&#x3D;&gt; Parameters: 1(Integer) (BaseJdbcLogger.java:143) DEBUG 11-09 20:35:31,599 &lt;&#x3D;&#x3D; Total: 1 (BaseJdbcLogger.java:143) Teacher [id&#x3D;1, name&#x3D;hh, course&#x3D;语文, address&#x3D;让胡路, birth&#x3D;Wed Jul 01 00:00:00 CST 1998]DEBUG 11-09 20:35:31,616 &#x3D;&#x3D;&gt; Preparing: select * from t_teacher where id&#x3D;? (BaseJdbcLogger.java:143) DEBUG 11-09 20:35:31,617 &#x3D;&#x3D;&gt; Parameters: 1(Integer) (BaseJdbcLogger.java:143) DEBUG 11-09 20:35:31,619 &lt;&#x3D;&#x3D; Total: 1 (BaseJdbcLogger.java:143) Teacher [id&#x3D;1, name&#x3D;hh, course&#x3D;语文, address&#x3D;让胡路, birth&#x3D;Wed Jul 01 00:00:00 CST 1998]false 2、同一个SqlSession但是查询条件不同1234567891011121314151617&#x2F;** * 一级缓存失效的几种情况 * 一级缓存是SqlSession级别的缓存 * 2、同一个,不同的参数，由于可能之前没查过，还是会发新的sql *&#x2F;@Testpublic void test03() throws IOException &#123; &#x2F;&#x2F;initSqlSessionFactory();如果不用@BeforeEach标注每次测试前都要先调用一下这个方法 SqlSession openSession &#x3D; sqlSessionFactory.openSession(); TeacherDao teacherDao &#x3D; openSession.getMapper(TeacherDao.class); Teacher teacher &#x3D; teacherDao.geTeacherById(1); System.out.println(teacher); Teacher teacher2 &#x3D; teacherDao.geTeacherById(2); System.out.println(teacher2); openSession.close();&#125; 测试结果12345678DEBUG 11-09 20:56:10,718 &#x3D;&#x3D;&gt; Preparing: select * from t_teacher where id&#x3D;? (BaseJdbcLogger.java:143) DEBUG 11-09 20:56:10,748 &#x3D;&#x3D;&gt; Parameters: 1(Integer) (BaseJdbcLogger.java:143) DEBUG 11-09 20:56:10,788 &lt;&#x3D;&#x3D; Total: 1 (BaseJdbcLogger.java:143) Teacher [id&#x3D;1, name&#x3D;hh, course&#x3D;语文, address&#x3D;让胡路, birth&#x3D;Wed Jul 01 00:00:00 CST 1998]DEBUG 11-09 20:56:10,790 &#x3D;&#x3D;&gt; Preparing: select * from t_teacher where id&#x3D;? (BaseJdbcLogger.java:143) DEBUG 11-09 20:56:10,791 &#x3D;&#x3D;&gt; Parameters: 2(Integer) (BaseJdbcLogger.java:143) DEBUG 11-09 20:56:10,793 &lt;&#x3D;&#x3D; Total: 1 (BaseJdbcLogger.java:143) Teacher [id&#x3D;2, name&#x3D;LaoCui, course&#x3D;数学, address&#x3D;萨尔图, birth&#x3D;Mon Jul 04 01:00:00 CDT 1988] 3、同一个SqlSession两次查询期间执行了任何一次增删改操作会清空缓存12345678910111213141516171819202122232425262728&#x2F;** * 一级缓存失效的几种情况 * 一级缓存是SqlSession级别的缓存 * 3、同一个SqlSession两次查询期间执行了任何一次增删改操作,增删改操作会清空缓存 *&#x2F;@Testpublic void test04() throws IOException &#123; &#x2F;&#x2F;initSqlSessionFactory();如果不用@BeforeEach标注每次测试前都要先调用一下这个方法 &#x2F;&#x2F;1、第一个会话 SqlSession openSession &#x3D; sqlSessionFactory.openSession(); TeacherDao teacherDao &#x3D; openSession.getMapper(TeacherDao.class); Teacher teacher &#x3D; teacherDao.geTeacherById(1); System.out.println(teacher); System.out.println(&quot;------------------------------------------------------&quot;); &#x2F;&#x2F;执行任何一个增删改 Teacher teacher2 &#x3D; new Teacher(); teacher2.setId(3); teacher2.setName(&quot;33333&quot;); teacherDao.updateTeacher(teacher2); System.out.println(&quot;------------------------------------------------------&quot;); Teacher teacher3 &#x3D; teacherDao.geTeacherById(2); System.out.println(teacher3); openSession.close();&#125; 测试结果123456789101112DEBUG 11-09 21:01:09,697 &#x3D;&#x3D;&gt; Preparing: select * from t_teacher where id&#x3D;? (BaseJdbcLogger.java:143) DEBUG 11-09 21:01:09,729 &#x3D;&#x3D;&gt; Parameters: 1(Integer) (BaseJdbcLogger.java:143) DEBUG 11-09 21:01:09,752 &lt;&#x3D;&#x3D; Total: 1 (BaseJdbcLogger.java:143) Teacher [id&#x3D;1, name&#x3D;hh, course&#x3D;语文, address&#x3D;让胡路, birth&#x3D;Wed Jul 01 00:00:00 CST 1998]------------------------------------------------------DEBUG 11-09 21:01:09,781 &#x3D;&#x3D;&gt; Preparing: UPDATE t_teacher SET teacherName&#x3D;? WHERE id&#x3D;? (BaseJdbcLogger.java:143) DEBUG 11-09 21:01:09,781 &#x3D;&#x3D;&gt; Parameters: 33333(String), 3(Integer) (BaseJdbcLogger.java:143) ------------------------------------------------------DEBUG 11-09 21:01:09,783 &#x3D;&#x3D;&gt; Preparing: select * from t_teacher where id&#x3D;? (BaseJdbcLogger.java:143) DEBUG 11-09 21:01:09,784 &#x3D;&#x3D;&gt; Parameters: 2(Integer) (BaseJdbcLogger.java:143) DEBUG 11-09 21:01:09,785 &lt;&#x3D;&#x3D; Total: 1 (BaseJdbcLogger.java:143) Teacher [id&#x3D;2, name&#x3D;LaoCui, course&#x3D;数学, address&#x3D;萨尔图, birth&#x3D;Mon Jul 04 01:00:00 CDT 1988] 4、同一个SqlSession两次查询期间手动清空了缓存123456789101112131415161718192021222324252627&#x2F;** * 一级缓存失效的几种情况 * 一级缓存是SqlSession级别的缓存 * 4、同一个SqlSession两次查询期间手动清空了缓存 * 每次查询先看一级缓存中有没有，如果没有就去发送新的Sql *&#x2F;@Testpublic void test05() throws IOException &#123; &#x2F;&#x2F;initSqlSessionFactory();如果不用@BeforeEach标注每次测试前都要先调用一下这个方法 &#x2F;&#x2F;1、第一个会话 SqlSession openSession &#x3D; sqlSessionFactory.openSession(); TeacherDao teacherDao &#x3D; openSession.getMapper(TeacherDao.class); Teacher teacher &#x3D; teacherDao.geTeacherById(1); System.out.println(teacher); System.out.println(&quot;------------------------------------------------------&quot;); System.out.println(&quot;手动清空缓存&quot;); &#x2F;&#x2F;清空当前SqlSession的一级缓存 openSession.clearCache(); Teacher teacher3 &#x3D; teacherDao.geTeacherById(1); System.out.println(teacher3); openSession.close();&#125; 测试结果12345678910DEBUG 11-09 21:09:18,502 &#x3D;&#x3D;&gt; Preparing: select * from t_teacher where id&#x3D;? (BaseJdbcLogger.java:143) DEBUG 11-09 21:09:18,532 &#x3D;&#x3D;&gt; Parameters: 1(Integer) (BaseJdbcLogger.java:143) DEBUG 11-09 21:09:18,555 &lt;&#x3D;&#x3D; Total: 1 (BaseJdbcLogger.java:143) Teacher [id&#x3D;1, name&#x3D;hh, course&#x3D;语文, address&#x3D;让胡路, birth&#x3D;Wed Jul 01 00:00:00 CST 1998]------------------------------------------------------手动清空缓存DEBUG 11-09 21:09:18,558 &#x3D;&#x3D;&gt; Preparing: select * from t_teacher where id&#x3D;? (BaseJdbcLogger.java:143) DEBUG 11-09 21:09:18,559 &#x3D;&#x3D;&gt; Parameters: 1(Integer) (BaseJdbcLogger.java:143) DEBUG 11-09 21:09:18,561 &lt;&#x3D;&#x3D; Total: 1 (BaseJdbcLogger.java:143) Teacher [id&#x3D;1, name&#x3D;hh, course&#x3D;语文, address&#x3D;让胡路, birth&#x3D;Wed Jul 01 00:00:00 CST 1998] 2.3 注意 实际开发中，MyBatis通常和Spring进行整合开发。Spring将事务放到Service中管理，对于每一个service中的sqlsession是不同的，这是通过mybatis-spring中的org.mybatis.spring.mapper.MapperScannerConfigurer创建sqlsession自动注入到service中的。 每次查询之后都要进行关闭sqlSession，关闭之后数据被清空。所以spring整合之后，如果没有事务，一级缓存是没有意义的。 MyBatis一级缓存的生命周期和SqlSession一致。 MyBatis一级缓存内部设计简单，只是一个没有容量限定的HashMap，在缓存的功能性上有所欠缺。 MyBatis的一级缓存最大范围是SqlSession内部，有多个SqlSession或者分布式的环境下，数据库写操作会引起脏数据。 mybatis和spring整合后进行mapper代理开发，不支持一级缓存。 2.4 MyBatis一级缓存实现原理 由于MyBatis使用SqlSession对象表示一次数据库的会话，那么，对于会话级别的一级缓存也应该是在SqlSession中控制的 三、二级缓存概述 二级缓存是Mapper 级别的缓存。使用二级缓存时，多个SqlSession 使用同一个Mapper ( namespace ）的SQL 语句操作数据库，得到的数据会存在二级缓存区域，二级缓存同样是使用 HashMap 进行数据存储。二级缓存比一级缓存作用域范围更大，多个SqISession 可以共用二级 缓存，二级缓存是跨SqISession 的。MyBatis 默认没有开启二级缓存，需要在配置中开启二级缓存。 二级缓存是namespace级别的缓存;意即哪个Dao需要缓存，在其相对应的xml文件中开启就行了* 二级缓存在SqlSession关闭或提交之后才会立即生效 * 一级缓存；SqlSession关闭或者提交以后，一级缓存的数据会放在二级缓存中；一级缓存中清除的才会进入二级缓存保存更长时间 mybatis默认没有使用的；配置； 3.1 二级缓存的使用配置二级缓存的使用：1、全局配置(mybatis-config.xml)开启二级缓存 1234&lt;settings&gt; &lt;!-- 开启全局缓存开关；对于长时间不变更的Dao使用二级缓存，经常变更的不用 --&gt; &lt;setting name&#x3D;&quot;cacheEnabled&quot; value&#x3D;&quot;true&quot;&#x2F;&gt;&lt;&#x2F;settings&gt; 2、配置某个dao.xml(TeacherDao.xml)文件，让其使用二级缓存 123&lt;!-- 使用二级缓存,在全局配置文件中打开配置文件后只需要在需要开启二级缓存的dao中加上&lt;cache&gt;标签即可 --&gt; &lt;cache&gt;&lt;&#x2F;cache&gt; 3、需要注意的是：在配置完二级缓存后，需要在bean中开启序列化不然会报如下错123456789org.apache.ibatis.cache.CacheException: Error serializing object. Cause: java.io.NotSerializableException: com.lizhi.bean.Teacher at org.apache.ibatis.cache.decorators.SerializedCache.serialize(SerializedCache.java:94) at org.apache.ibatis.cache.decorators.SerializedCache.putObject(SerializedCache.java:55) at org.apache.ibatis.cache.decorators.LoggingCache.putObject(LoggingCache.java:49) at org.apache.ibatis.cache.decorators.SynchronizedCache.putObject(SynchronizedCache.java:43) at org.apache.ibatis.cache.decorators.TransactionalCache.flushPendingEntries(TransactionalCache.java:116) at org.apache.ibatis.cache.decorators.TransactionalCache.commit(TransactionalCache.java:99) at org.apache.ibatis.cache.TransactionalCacheManager.commit(TransactionalCacheManager.java:44) 4、在接口开启序列化(Teacher.java)123public class Teacher implements Serializable&#123;......&#125; 5、测试实例 12345678910111213141516171819202122&#x2F;** * 测试二级缓存 * *&#x2F; @Test public void test06() &#123; SqlSession openSession &#x3D; sqlSessionFactory.openSession(); SqlSession openSession2 &#x3D; sqlSessionFactory.openSession(); TeacherDao teacherDao &#x3D; openSession.getMapper(TeacherDao.class); TeacherDao teacherDao2 &#x3D; openSession2.getMapper(TeacherDao.class); &#x2F;&#x2F;1.第一个Dao查询1号teacher Teacher teacher &#x3D; teacherDao.geTeacherById(1); System.out.println(teacher); openSession.close(); &#x2F;&#x2F;2.第二个Dao查询1号teacher Teacher teacher2 &#x3D; teacherDao2.geTeacherById(1); System.out.println(teacher2); openSession2.close(); &#125; 6、输出结果 1234567DEBUG 11-10 07:26:17,907 Cache Hit Ratio [com.lizhi.dao.TeacherDao]: 0.0 (LoggingCache.java:60) DEBUG 11-10 07:26:18,710 &#x3D;&#x3D;&gt; Preparing: select * from t_teacher where id&#x3D;? (BaseJdbcLogger.java:143) DEBUG 11-10 07:26:18,736 &#x3D;&#x3D;&gt; Parameters: 1(Integer) (BaseJdbcLogger.java:143) DEBUG 11-10 07:26:18,758 &lt;&#x3D;&#x3D; Total: 1 (BaseJdbcLogger.java:143) Teacher [id&#x3D;1, name&#x3D;hh, course&#x3D;语文, address&#x3D;让胡路, birth&#x3D;Wed Jul 01 00:00:00 CST 1998]DEBUG 11-10 07:26:18,770 Cache Hit Ratio [com.lizhi.dao.TeacherDao]: 0.5 (LoggingCache.java:60) Teacher [id&#x3D;1, name&#x3D;hh, course&#x3D;语文, address&#x3D;让胡路, birth&#x3D;Wed Jul 01 00:00:00 CST 1998] 对于输出结果我们可以看到：①第二次查询并没有重新发送sql,这与前面一级缓存失效的情况相反。②Cache Hit Ratio [com.lizhi.dao.TeacherDao]: 0.5 的意思是缓存命中率是0.5，第一次查询的时候去缓存里找，没有，第二次查询去缓存里找命中，查询两次命中一次所以缓存命中率为0.5 3.2 &lt;cache&gt;标签的相关属性 eviction eviction=”FIFO”:回收缓存策略 LRU:最近最少使用；移除最长时间不被使用的对象 FIFO : 先进先出；按对象进入缓存的顺序来移除他们 SOFT : 软引用；移除基于垃圾回收器状态和软引用规则的对象 WEAK ： 弱引用；更积极的移除基于垃圾回收器状态和弱引用规则的对象。 默认是LRU flushInterval:刷新时间间隔，单位毫秒 默认情况是不设置，也就是没有刷新间隔，缓存仅仅调用语句时刷新 size ：引用数目，正整数 代表缓存最多可以存储多少个对象，太大容易导致内存溢出 readOnly : 只读，true/false true: 只读缓存，会给所有调用者返回缓存对象的相同实例。因此这些对象不能被修改。这提供了很重要的性能优势。 false : 读写缓存；会返回缓存对象的拷贝（通过序列化）。这会慢一些，但是安全,因此默认是false。 3.3 是先读一级缓存呢还是先读二级缓存？123456789101112131415161718192021222324252627282930313233343536&#x2F;** * 测试先读一级缓存还是先读二级缓存 * 1. 不会出现一级缓存和二级缓存中有同一个数据的情况（一级缓存关闭了才会存到二级缓存） * 2.一级缓存；如果二级缓存没有数据就会去一级缓存中查询数据，如果一级缓存中没有数据， * 就会去数据库中查询，查询的结果放在一级缓存中。 * 3.任何时候都先查询二级缓存，再看一级缓存，如果都没有就去数据库查询 * * 先二 后一 最后库 * *&#x2F;@Testpublic void test07() &#123; SqlSession openSession &#x3D; sqlSessionFactory.openSession(); TeacherDao teacherDao &#x3D; openSession.getMapper(TeacherDao.class); Teacher teacher &#x3D; teacherDao.geTeacherById(1); System.out.println(teacher); openSession.close(); &#x2F;&#x2F;2.第二个Dao查询1号teacher SqlSession openSession2 &#x3D; sqlSessionFactory.openSession(); TeacherDao teacherDao2 &#x3D; openSession2.getMapper(TeacherDao.class); Teacher teacher2 &#x3D; teacherDao2.geTeacherById(1); System.out.println(teacher2); Teacher teacher3 &#x3D; teacherDao2.geTeacherById(1); System.out.println(teacher3); openSession2.close(); System.out.println(&quot;以下是查询二号老师，证明一二级缓存中没有会去数据库查询：&quot;); Teacher teacher4 &#x3D; teacherDao2.geTeacherById(2); System.out.println(teacher4); Teacher teacher5 &#x3D; teacherDao2.geTeacherById(2); System.out.println(teacher5); &#125; 运行结果12345678910111213141516171819DEBUG 11-12 07:39:04,337 Cache Hit Ratio [com.lizhi.dao.TeacherDao]: 0.0 (LoggingCache.java:60) Tue Nov 12 07:39:04 CST 2019 WARN: Establishing SSL connection without server&#39;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&#39;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &#39;false&#39;. You need either to explicitly disable SSL by setting useSSL&#x3D;false, or set useSSL&#x3D;true and provide truststore for server certificate verification.DEBUG 11-12 07:39:05,242 &#x3D;&#x3D;&gt; Preparing: select * from t_teacher where id&#x3D;? (BaseJdbcLogger.java:143) DEBUG 11-12 07:39:05,270 &#x3D;&#x3D;&gt; Parameters: 1(Integer) (BaseJdbcLogger.java:143) DEBUG 11-12 07:39:05,294 &lt;&#x3D;&#x3D; Total: 1 (BaseJdbcLogger.java:143) Teacher [id&#x3D;1, name&#x3D;hh, course&#x3D;语文, address&#x3D;让胡路, birth&#x3D;Wed Jul 01 00:00:00 CST 1998]DEBUG 11-12 07:39:05,308 Cache Hit Ratio [com.lizhi.dao.TeacherDao]: 0.5 (LoggingCache.java:60) Teacher [id&#x3D;1, name&#x3D;hh, course&#x3D;语文, address&#x3D;让胡路, birth&#x3D;Wed Jul 01 00:00:00 CST 1998]DEBUG 11-12 07:39:05,309 Cache Hit Ratio [com.lizhi.dao.TeacherDao]: 0.6666666666666666 (LoggingCache.java:60) Teacher [id&#x3D;1, name&#x3D;hh, course&#x3D;语文, address&#x3D;让胡路, birth&#x3D;Wed Jul 01 00:00:00 CST 1998]以下是查询二号老师，证明一二级缓存中没有会去数据库查询：DEBUG 11-12 07:39:05,309 Cache Hit Ratio [com.lizhi.dao.TeacherDao]: 0.5 (LoggingCache.java:60) DEBUG 11-12 07:39:05,310 &#x3D;&#x3D;&gt; Preparing: select * from t_teacher where id&#x3D;? (BaseJdbcLogger.java:143) DEBUG 11-12 07:39:05,311 &#x3D;&#x3D;&gt; Parameters: 2(Integer) (BaseJdbcLogger.java:143) DEBUG 11-12 07:39:05,312 &lt;&#x3D;&#x3D; Total: 1 (BaseJdbcLogger.java:143) Teacher [id&#x3D;2, name&#x3D;LaoCui, course&#x3D;数学, address&#x3D;萨尔图, birth&#x3D;Mon Jul 04 01:00:00 CDT 1988]DEBUG 11-12 07:39:05,313 Cache Hit Ratio [com.lizhi.dao.TeacherDao]: 0.4 (LoggingCache.java:60) Teacher [id&#x3D;2, name&#x3D;LaoCui, course&#x3D;数学, address&#x3D;萨尔图, birth&#x3D;Mon Jul 04 01:00:00 CDT 1988] 缓存原理图解 3.4 缓存相关设置1.全局setting的cacheEnable :* 配置二级缓存的开关。一级缓存是直打开的 2.select标签的useCache属性* 配置这个select是否使用二级缓存。一级缓存一直是使用的 123456789&lt;!-- public Teacher geTeacherById(Integer id); --&gt; &lt;!-- 抽取可重用的sql语句 --&gt; &lt;sql id&#x3D;&quot;selectSql&quot;&gt;select * from t_teacher&lt;&#x2F;sql&gt; &lt;!-- useCache&#x3D;&quot;true&quot;控制是否使用二级缓存 ，不影响一级缓存 --&gt;&lt;select id&#x3D;&quot;geTeacherById&quot; resultMap&#x3D;&quot;teacherMap&quot; useCache&#x3D;&quot;true&quot;&gt; &lt;include refid&#x3D;&quot;selectSql&quot;&gt;&lt;&#x2F;include&gt; where id&#x3D;#&#123;id&#125;&lt;&#x2F;select&gt; 3.sql标签的flushCache* 增删改默认flushCache=true。sql执行后，会同时清空一级二级缓存。查询默认flushCache=flase 4.SqlSession.clearCache();* 只是用来清除一级缓存 5.当在某一作用域(一级缓存Session/二级缓存Namespace)进行了C/U/D操作后，默认该作用域下所有select中的缓存将被clear。四、整合第三方缓存4.1 整合ehcache ehcache: ehcache非常专业的java进程缓存框架 1.导包12ehcache-core-2.6.8.jar(ehcache核心包)mybatis-ehcache-1.0.3.jar(mybatis与ehcache的整合包，如果不导就要自己写接口实现) 123这里日志包在下载的mybatis官方給的文件里有slf4j-api-1.7.26.jarslf4j-log4j12-1.7.26.jar 2.eache要有一个配置文件ehcache.xml放在类路径的根目录下3.在mapper.xml中配置自定义的缓存12345678&lt;mapper namespace&#x3D;&quot;com.lizhi.dao.TeacherDao&quot;&gt;&lt;!-- 使用二级缓存,在全局配置文件中打开配置文件后只需要在需要开启二级缓存的dao中加上&lt;cache&gt;标签即可 --&gt;&lt;!-- &lt;cache&gt;&lt;&#x2F;cache&gt; 是使用默认的二级缓存，加上type属性可以整合第三方的缓存工具 --&gt; &lt;!-- 在mybatis-ehcache-1.0.3.jar包里的EhcacheCache.class里 使用ehcache有个好处 ① 不用管序列化的问题；Teacher.java中的 implements Serializable可以删除 ② 缓存记录可以存在磁盘中，扩展了存储缓存的条数 --&gt;&lt;cache type&#x3D;&quot;org.mybatis.caches.ehcache.EhcacheCache&quot;&gt;&lt;&#x2F;cache&gt; &lt;cache-ref namespace=&quot;&quot;&gt;缓存引用：和别的dao公用一块缓存五、 测试实例源码 package com.lizhi.bean;class Teacher12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.lizhi.bean;import java.io.Serializable;import java.util.Date;public class Teacher implements Serializable&#123; private Integer id; private String name; private String course; private String address; private Date birth;&#x2F;&#x2F;Date是util包里的 public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id &#x3D; id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name &#x3D; name; &#125; public String getCourse() &#123; return course; &#125; public void setCourse(String course) &#123; this.course &#x3D; course; &#125; public String getAddress() &#123; return address; &#125; public void setAddress(String address) &#123; this.address &#x3D; address; &#125; public Date getBirth() &#123; return birth; &#125; public void setBirth(Date birth) &#123; this.birth &#x3D; birth; &#125; @Override public String toString() &#123; return &quot;Teacher [id&#x3D;&quot; + id + &quot;, name&#x3D;&quot; + name + &quot;, course&#x3D;&quot; + course + &quot;, address&#x3D;&quot; + address + &quot;, birth&#x3D;&quot; + birth + &quot;]&quot;; &#125; &#125; package com.lizhi.dao;class MyCache12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.lizhi.dao;import org.apache.ibatis.cache.Cache;&#x2F;** * 自定义缓存，利用第三方缓存 * @author Administrator * 打完implements Cache后鼠标放在MyCache上 *&#x2F;public class MyCache implements Cache&#123; @Override public void clear() &#123; &#x2F;&#x2F; TODO Auto-generated method stub &#125; @Override public String getId() &#123; &#x2F;&#x2F; TODO Auto-generated method stub return null; &#125; @Override public Object getObject(Object arg0) &#123; &#x2F;&#x2F; TODO Auto-generated method stub return null; &#125; @Override public int getSize() &#123; &#x2F;&#x2F; TODO Auto-generated method stub return 0; &#125; @Override public void putObject(Object arg0, Object arg1) &#123; &#x2F;&#x2F; TODO Auto-generated method stub &#125; @Override public Object removeObject(Object arg0) &#123; &#x2F;&#x2F; TODO Auto-generated method stub return null; &#125;&#125; interface TeacherDao1234567891011121314151617181920212223242526272829package com.lizhi.dao;import java.util.List;import org.apache.ibatis.annotations.Param;import com.lizhi.bean.Teacher;&#x2F;* * mapper和dao一样都是完成bean到数据库的映射 *&#x2F;public interface TeacherDao &#123; public Teacher geTeacherById(Integer id); &#x2F;&#x2F;按照一定的条件查询老师 public List&lt;Teacher&gt; geTeacherByCondition(Teacher teacher); &#x2F;&#x2F;查出所有的Teacher传入一个集合，然后查询出这个集合的所有老师,用来测试&lt;foreach&gt;标签 public List&lt;Teacher&gt; getTeachersByIdIn(@Param(&quot;ids&quot;)List&lt;Integer&gt; ids); &#x2F;&#x2F;测试&lt;choose&gt;标签分支选择 public List&lt;Teacher&gt; geTeacherByChoose(Teacher teacher); &#x2F;&#x2F;&lt;set&gt;标签 动态更新 public Integer updateTeacher(Teacher teacher); &#125; TeacherDao.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-&#x2F;&#x2F;mybatis.org&#x2F;&#x2F;DTD Mapper 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;mybatis.org&#x2F;dtd&#x2F;mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace&#x3D;&quot;com.lizhi.dao.TeacherDao&quot;&gt; &lt;!-- 使用二级缓存,在全局配置文件中打开配置文件后只需要在需要开启二级缓存的dao中加上&lt;cache&gt;标签即可 --&gt; &lt;!-- &lt;cache&gt;&lt;&#x2F;cache&gt; 是使用默认的二级缓存，加上type属性可以整合第三方的缓存工具 --&gt; &lt;!-- 在mybatis-ehcache-1.0.3.jar包里的EhcacheCache.class里 使用ehcache有个好处 ① 不用管序列化的问题；Teacher.java中的 implements Serializable可以删除 ② 缓存记录可以存在磁盘中，扩展了存储缓存的条数 --&gt; &lt;cache type&#x3D;&quot;org.mybatis.caches.ehcache.EhcacheCache&quot;&gt;&lt;&#x2F;cache&gt; &lt;!-- 自定义封装规则 --&gt; &lt;resultMap type&#x3D;&quot;com.lizhi.bean.Teacher&quot; id&#x3D;&quot;teacherMap&quot;&gt; &lt;id property&#x3D;&quot;id&quot; column&#x3D;&quot;id&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;address&quot; column&#x3D;&quot;address&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;birth&quot; column&#x3D;&quot;birth_date&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;course&quot; column&#x3D;&quot;class_name&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;name&quot; column&#x3D;&quot;teacherName&quot;&#x2F;&gt; &lt;&#x2F;resultMap&gt; &lt;!-- public Teacher geTeacherById(Integer id); --&gt; &lt;!-- 抽取可重用的sql语句 --&gt; &lt;sql id&#x3D;&quot;selectSql&quot;&gt;select * from t_teacher&lt;&#x2F;sql&gt; &lt;!-- useCache&#x3D;&quot;true&quot;控制是否使用二级缓存 ，不影响一级缓存 --&gt; &lt;select id&#x3D;&quot;geTeacherById&quot; resultMap&#x3D;&quot;teacherMap&quot; useCache&#x3D;&quot;true&quot;&gt; &lt;include refid&#x3D;&quot;selectSql&quot;&gt;&lt;&#x2F;include&gt; where id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; &lt;!-- &lt;if&gt;: 判断 --&gt; &lt;!-- public List&lt;Teacher&gt; geTeacherByCondition(Teacher teacher);&#x2F;&#x2F;按照一定的条件查询老师 --&gt; &lt;select id&#x3D;&quot;geTeacherByCondition&quot; resultMap&#x3D;&quot;teacherMap&quot;&gt; select * from t_teacher &lt;!-- test&#x3D;&quot;&quot; : 编写判断条件 id!&#x3D;null : 取出传入的JavaBean属性中的id值，判断其是否为空 --&gt; &lt;!-- 可以帮我们去掉前面的and --&gt; &lt;!-- trim : 截取字符串 prefix&#x3D;&quot;&quot; : 前缀，为下面的sql整体添加一个前缀 prefixOverrides&#x3D;&quot;&quot; : 取出整体字符串前面多余的字符串 suffix&#x3D;&quot;&quot; : 为整体添加一个后缀 suffixOverrides&#x3D;&quot;&quot; : 去掉多余的后缀 --&gt; &lt;trim prefix&#x3D;&quot;where&quot; prefixOverrides&#x3D;&quot;and&quot; suffixOverrides&#x3D;&quot;and&quot;&gt; &lt;if test&#x3D;&quot;id!&#x3D;null&quot;&gt; and id &gt; #&#123;id&#125; &lt;&#x2F;if&gt; &lt;!-- 判断空串&#39;&#39;，双引号的转义字符&quot; and : &amp;&amp; ，输入&amp;&amp;时会报错，需要用转义字符&amp;和html里一样 or : || if() : 传入非常强大的判断条件，之前java中怎么写判断这里都可以; 叫OGNL表达式； 转义字符：https:&#x2F;&#x2F;www.w3school.com.cn&#x2F;tags&#x2F;html_ref_entities.html 对于where标签这里有个问题；这里写了三个判断语句，但是如果最后的一个判断没过，只拼接到第二个， 结尾就会多个and解决办法就是把写的if判断写到&lt;where&gt;里让MyBatis自动补充 --&gt; &lt;!-- OGNL表达式； 对象导航图Person下有三个属性，address下又有三个属性，street下又有三个属性 Person &#x3D;&#x3D;&#x3D;lastName &#x3D;&#x3D;&#x3D;email &#x3D;&#x3D;&#x3D;Address &#x3D;&#x3D;&#x3D;city &#x3D;&#x3D;&#x3D;province &#x3D;&#x3D;&#x3D;street &#x3D;&#x3D;&#x3D;adminName &#x3D;&#x3D;&#x3D;&#x3D;info &#x3D;&#x3D;&#x3D;perCount 可用于方法，静态方法，构造器等等等 在mybatis中，传入的参数可以用来判断； 额外还有两个东西； _parmeter :代表传入来的参数(&lt;if test&#x3D;&quot;_parmeter&quot;&gt;&lt;&#x2F;if&gt;) ①，单个参数 : _parmeter就代表这个参数 ②，传入多个参数 : _parmeter就代表多个参数集合起来的map _databaseId : 代表当前环境(&lt;if test&#x3D;&quot;_databaseId&#x3D;&#x3D;&#39;mysql&#39;&quot;&gt;&lt;&#x2F;if&gt;)标识数据库 如果配置了databaseIdProvider : _databaseId就有值 --&gt; &lt;!-- 绑定一个表达式的值到一个变量 --&gt; &lt;bind name&#x3D;&quot;_name&quot; value&#x3D;&quot;&#39;%&#39;+name+&#39;%&#39;&quot;&#x2F;&gt; &lt;if test&#x3D;&quot;name!&#x3D;null &amp;&amp; !name.equals(&quot;&quot;)&quot;&gt; and teacherName like #&#123;name&#125; &lt;&#x2F;if&gt; &lt;if test&#x3D;&quot;birth!&#x3D;null&quot;&gt; and birth_date &lt; #&#123;birth&#125; &lt;&#x2F;if&gt; &lt;&#x2F;trim&gt; &lt;&#x2F;select&gt; &lt;!--public List&lt;Teacher&gt; getTeachersByIdIn(@Param(&quot;ids&quot;)List&lt;Integer&gt; ids); &#x2F;&#x2F;查出所有的Teacher传入一个集合，然后查询出这个集合的所有老师 --&gt; &lt;select id&#x3D;&quot;getTeachersByIdIn&quot; resultMap&#x3D;&quot;teacherMap&quot;&gt; select * from t_teacher where id in &lt;!-- 帮我们遍历集合的 collection&#x3D;&quot;&quot; : 指定要遍历的集合的key,这里用@Param(&quot;ids&quot;)起了个别名所以写ids就行 close&#x3D;&quot;&quot; : 指定以什么结束 index&#x3D;&quot;&quot; : 索引， 如果遍历出的是一个list, index:指定的变量保存了当前索引 item: 指定的元素保存了当前遍历的元素的值 如果遍历的是一个map,即(k,v) index,指定的变量就是保存了当前遍历的元素的key item,指定的变量保存当前遍历的元素的值 item&#x3D;&quot;&quot; : 每次遍历出的元素起一个变量名，相当于爪娃里for循环的i open&#x3D;&quot;&quot; : 指定以什么开始 separator&#x3D;&quot;&quot; : 每次遍历元素的分隔符 因为sql语句in后面跟的集合需要写成in (1,2,3,4,5,6),所以需要open和close --&gt; &lt;if test&#x3D;&quot;ids.size &gt; 0&quot;&gt; &lt;foreach collection&#x3D;&quot;ids&quot; item&#x3D;&quot;id_item&quot; separator&#x3D;&quot;,&quot; open&#x3D;&quot;(&quot; close&#x3D;&quot;)&quot; index&#x3D;&quot;&quot;&gt; #&#123;id_item&#125; &lt;&#x2F;foreach&gt; &lt;&#x2F;if&gt; &lt;&#x2F;select&gt; &lt;!-- public List&lt;Teacher&gt; geTeacherByChoose(Teacher teacher); &#x2F;&#x2F;需求: 带id用id查，不带id用其他属性查 --&gt; &lt;select id&#x3D;&quot;geTeacherByChoose&quot; resultMap&#x3D;&quot;teacherMap&quot;&gt; select * from t_teacher &lt;where&gt; &lt;choose&gt; &lt;when test&#x3D;&quot;id!&#x3D;null&quot;&gt; id&#x3D;#&#123;id&#125; &lt;&#x2F;when&gt; &lt;!-- &quot;&quot; 空串转义符 --&gt; &lt;when test&#x3D;&quot;name!&#x3D;null and !name.equals(&quot;&quot;)&quot;&gt; teacherName&#x3D;#&#123;name&#125; &lt;&#x2F;when&gt; &lt;when test&#x3D;&quot;birth!&#x3D;null&quot;&gt; birth_date&#x3D;#&#123;birth&#125; &lt;&#x2F;when&gt; &lt;!-- 其他情況 --&gt; &lt;otherwise&gt; 1&#x3D;1 &lt;&#x2F;otherwise&gt; &lt;&#x2F;choose&gt; &lt;&#x2F;where&gt; &lt;&#x2F;select&gt;&lt;!-- public Integer updateTeacher(Teacher teacher); &#x2F;&#x2F;&lt;set&gt;标签 动态更新 --&gt; &lt;select id&#x3D;&quot;updateTeacher&quot; resultMap&#x3D;&quot;teacherMap&quot;&gt; UPDATE t_teacher &lt;set&gt; &lt;if test&#x3D;&quot;name!&#x3D;null and !name.equals(&quot;&quot;)&quot;&gt; teacherName&#x3D;#&#123;name&#125;, &lt;&#x2F;if&gt; &lt;if test&#x3D;&quot;course!&#x3D;null and !course.equals(&quot;&quot;)&quot;&gt; class_name&#x3D;#&#123;course&#125;, &lt;&#x2F;if&gt; &lt;if test&#x3D;&quot;address!&#x3D;null and !address.equals(&quot;&quot;)&quot;&gt; address&#x3D;#&#123;address&#125;, &lt;&#x2F;if&gt; &lt;if test&#x3D;&quot;birth!&#x3D;null and !birth.equals(&quot;&quot;)&quot;&gt; birth_date&#x3D;#&#123;birth&#125; &lt;&#x2F;if&gt; &lt;&#x2F;set&gt; &lt;where&gt; id&#x3D;#&#123;id&#125; &lt;&#x2F;where&gt; &lt;&#x2F;select&gt; &lt;&#x2F;mapper&gt; ehcache.xml123456789101112131415161718192021222324252627282930313233343536&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;ehcache xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xsi:noNamespaceSchemaLocation&#x3D;&quot;..&#x2F;config&#x2F;ehcache.xsd&quot;&gt; &lt;!-- 磁盘保存路径 --&gt; &lt;diskStore path&#x3D;&quot;D:\\44\\ehcache&quot; &#x2F;&gt; &lt;defaultCache maxElementsInMemory&#x3D;&quot;1&quot; maxElementsOnDisk&#x3D;&quot;10000000&quot; eternal&#x3D;&quot;false&quot; overflowToDisk&#x3D;&quot;true&quot; timeToIdleSeconds&#x3D;&quot;120&quot; timeToLiveSeconds&#x3D;&quot;120&quot; diskExpiryThreadIntervalSeconds&#x3D;&quot;120&quot; memoryStoreEvictionPolicy&#x3D;&quot;LRU&quot;&gt; &lt;&#x2F;defaultCache&gt;&lt;&#x2F;ehcache&gt; &lt;!-- 属性说明：l diskStore：指定数据在磁盘中的存储位置。l defaultCache：当借助CacheManager.add(&quot;demoCache&quot;)创建Cache时，EhCache便会采用&lt;defalutCache&#x2F;&gt;指定的的管理策略 以下属性是必须的：l maxElementsInMemory - 在内存中缓存的element的最大数目 l maxElementsOnDisk - 在磁盘上缓存的element的最大数目，若是0表示无穷大l eternal - 设定缓存的elements是否永远不过期。如果为true，则缓存的数据始终有效，如果为false那么还要根据timeToIdleSeconds，timeToLiveSeconds判断l overflowToDisk - 设定当内存缓存溢出的时候是否将过期的element缓存到磁盘上 以下属性是可选的：l timeToIdleSeconds - 当缓存在EhCache中的数据前后两次访问的时间超过timeToIdleSeconds的属性取值时，这些数据便会删除，默认值是0,也就是可闲置时间无穷大l timeToLiveSeconds - 缓存element的有效生命期，默认是0.,也就是element存活时间无穷大 diskSpoolBufferSizeMB 这个参数设置DiskStore(磁盘缓存)的缓存区大小.默认是30MB.每个Cache都应该有自己的一个缓冲区.l diskPersistent - 在VM重启的时候是否启用磁盘保存EhCache中的数据，默认是false。l diskExpiryThreadIntervalSeconds - 磁盘缓存的清理线程运行间隔，默认是120秒。每个120s，相应的线程会进行一次EhCache中数据的清理工作l memoryStoreEvictionPolicy - 当内存缓存达到最大，有新的element加入的时候， 移除缓存中element的策略。默认是LRU（最近最少使用），可选的有LFU（最不常使用）和FIFO（先进先出） --&gt; log4j.properties1234567# Global logging configurationlog4j.rootLogger&#x3D;DEBUG, stdout# Console output...log4j.appender.stdout&#x3D;org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout&#x3D;org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern&#x3D;%5p [%t] - %m%n log4j.xml12345678910111213141516171819202122&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE log4j:configuration SYSTEM &quot;log4j.dtd&quot;&gt; &lt;log4j:configuration xmlns:log4j&#x3D;&quot;http:&#x2F;&#x2F;jakarta.apache.org&#x2F;log4j&#x2F;&quot;&gt; &lt;appender name&#x3D;&quot;STDOUT&quot; class&#x3D;&quot;org.apache.log4j.ConsoleAppender&quot;&gt; &lt;param name&#x3D;&quot;Encoding&quot; value&#x3D;&quot;UTF-8&quot; &#x2F;&gt; &lt;layout class&#x3D;&quot;org.apache.log4j.PatternLayout&quot;&gt; &lt;param name&#x3D;&quot;ConversionPattern&quot; value&#x3D;&quot;%-5p %d&#123;MM-dd HH:mm:ss,SSS&#125; %m (%F:%L) \\n&quot; &#x2F;&gt; &lt;&#x2F;layout&gt; &lt;&#x2F;appender&gt; &lt;logger name&#x3D;&quot;java.sql&quot;&gt; &lt;level value&#x3D;&quot;debug&quot; &#x2F;&gt; &lt;&#x2F;logger&gt; &lt;logger name&#x3D;&quot;org.apache.ibatis&quot;&gt; &lt;level value&#x3D;&quot;info&quot; &#x2F;&gt; &lt;&#x2F;logger&gt; &lt;root&gt; &lt;level value&#x3D;&quot;debug&quot; &#x2F;&gt; &lt;appender-ref ref&#x3D;&quot;STDOUT&quot; &#x2F;&gt; &lt;&#x2F;root&gt;&lt;&#x2F;log4j:configuration&gt; mybatis-config.xml1234567891011121314151617181920212223242526272829303132&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-&#x2F;&#x2F;mybatis.org&#x2F;&#x2F;DTD Config 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;mybatis.org&#x2F;dtd&#x2F;mybatis-3-config.dtd&quot;&gt;&lt;configuration&gt; &lt;settings&gt; &lt;!-- 开启延迟加载开关 --&gt; &lt;setting name&#x3D;&quot;lazyLoadingEnabled&quot; value&#x3D;&quot;true&quot;&#x2F;&gt; &lt;!-- 开启属性按需加载 --&gt; &lt;setting name&#x3D;&quot;aggressiveLazyLoading&quot; value&#x3D;&quot;false&quot;&#x2F;&gt; &lt;!-- 开启全局缓存开关；对于长时间不变更的Dao使用二级缓存，经常变更的不用 --&gt; &lt;setting name&#x3D;&quot;cacheEnabled&quot; value&#x3D;&quot;true&quot;&#x2F;&gt; &lt;&#x2F;settings&gt; &lt;environments default&#x3D;&quot;development&quot;&gt; &lt;environment id&#x3D;&quot;development&quot;&gt; &lt;transactionManager type&#x3D;&quot;JDBC&quot;&#x2F;&gt; &lt;!-- 配置连接池 POOLED: 连接池--&gt; &lt;dataSource type&#x3D;&quot;POOLED&quot;&gt; &lt;!-- $&#123;&#125;取出配置值 --&gt; &lt;property name&#x3D;&quot;driver&quot; value&#x3D;&quot;com.mysql.cj.jdbc.Driver&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;url&quot; value&#x3D;&quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;mybatis_0325?serverTimezone&#x3D;GMT%2B8&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;username&quot; value&#x3D;&quot;root&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;password&quot; value&#x3D;&quot;******&quot;&#x2F;&gt; &lt;&#x2F;dataSource&gt; &lt;&#x2F;environment&gt; &lt;&#x2F;environments&gt; &lt;mappers&gt; &lt;package name&#x3D;&quot;com.lizhi.dao&quot;&#x2F;&gt; &lt;&#x2F;mappers&gt;&lt;&#x2F;configuration&gt; MyBatisTest.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226package com.lizhi.test;import java.io.IOException;import java.io.InputStream;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.jupiter.api.BeforeEach;import org.junit.jupiter.api.Test;import com.lizhi.bean.Teacher;import com.lizhi.dao.TeacherDao;public class MyBatisTest &#123; &#x2F;&#x2F;工厂一个 SqlSessionFactory sqlSessionFactory; @BeforeEach public void initSqlSessionFactory() throws IOException &#123; &#x2F;&#x2F;1.跟据全局配置文件先得到SqlSessionFactory对象(官方文档复制)，快捷键Ctrl+shift+O导包 String resource &#x3D; &quot;mybatis-config.xml&quot;; InputStream inputStream &#x3D; Resources.getResourceAsStream(resource); sqlSessionFactory &#x3D; new SqlSessionFactoryBuilder().build(inputStream); &#125; &#x2F;** * 体会一级缓存 *&#x2F; @Test public void test01() throws IOException &#123; &#x2F;&#x2F;initSqlSessionFactory();如果不用@BeforeEach标注每次测试前都要先调用一下这个方法 SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; TeacherDao mapper &#x3D; openSession.getMapper(TeacherDao.class); Teacher teacher &#x3D; mapper.geTeacherById(1); System.out.println(teacher); System.out.println(&quot;---------------------------------------&quot;); Teacher teacher2 &#x3D; mapper.geTeacherById(1); System.out.println(teacher2); System.out.println(teacher &#x3D;&#x3D; teacher2); &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125; &#x2F;** * 一级缓存失效的几种情况 * 一级缓存是SqlSession级别的缓存 * 1.不同的SqlSession对应不同的一级缓存; * 只用在同一个SqlSession期间查询到的数据会保存在这个SqlSession的缓存中； * 下次再使用这个SqlSession查询会从缓存中拿 *&#x2F; @Test public void test02() throws IOException &#123; &#x2F;&#x2F;initSqlSessionFactory();如果不用@BeforeEach标注每次测试前都要先调用一下这个方法 &#x2F;&#x2F;第一个会话 SqlSession openSession &#x3D; sqlSessionFactory.openSession(); TeacherDao teacherDao &#x3D; openSession.getMapper(TeacherDao.class); Teacher teacher &#x3D; teacherDao.geTeacherById(1); System.out.println(teacher); &#x2F;&#x2F;第二个会话 SqlSession openSession2 &#x3D; sqlSessionFactory.openSession(); TeacherDao teacherDao2 &#x3D; openSession2.getMapper(TeacherDao.class); Teacher teacher2 &#x3D; teacherDao2.geTeacherById(1); System.out.println(teacher2); System.out.println(teacher&#x3D;&#x3D;teacher2); openSession.close(); openSession2.close(); &#125; &#x2F;** * 一级缓存失效的几种情况 * 一级缓存是SqlSession级别的缓存 * 2、同一个,不同的参数，由于可能之前没查过，还是会发新的sql *&#x2F; @Test public void test03() throws IOException &#123; &#x2F;&#x2F;initSqlSessionFactory();如果不用@BeforeEach标注每次测试前都要先调用一下这个方法 SqlSession openSession &#x3D; sqlSessionFactory.openSession(); TeacherDao teacherDao &#x3D; openSession.getMapper(TeacherDao.class); Teacher teacher &#x3D; teacherDao.geTeacherById(1); System.out.println(teacher); Teacher teacher2 &#x3D; teacherDao.geTeacherById(2); System.out.println(teacher2); openSession.close(); &#125; &#x2F;** * 一级缓存失效的几种情况 * 一级缓存是SqlSession级别的缓存 * 3、同一个SqlSession两次查询期间执行了任何一次增删改操作,增删改操作会清空缓存 *&#x2F; @Test public void test04() throws IOException &#123; &#x2F;&#x2F;initSqlSessionFactory();如果不用@BeforeEach标注每次测试前都要先调用一下这个方法 &#x2F;&#x2F;1、第一个会话 SqlSession openSession &#x3D; sqlSessionFactory.openSession(); TeacherDao teacherDao &#x3D; openSession.getMapper(TeacherDao.class); Teacher teacher &#x3D; teacherDao.geTeacherById(1); System.out.println(teacher); System.out.println(&quot;------------------------------------------------------&quot;); &#x2F;&#x2F;执行任何一个增删改 Teacher teacher2 &#x3D; new Teacher(); teacher2.setId(3); teacher2.setName(&quot;33333&quot;); teacherDao.updateTeacher(teacher2); System.out.println(&quot;------------------------------------------------------&quot;); Teacher teacher3 &#x3D; teacherDao.geTeacherById(2); System.out.println(teacher3); openSession.close(); &#125; &#x2F;** * 一级缓存失效的几种情况 * 一级缓存是SqlSession级别的缓存 * 4、同一个SqlSession两次查询期间手动清空了缓存 * 每次查询先看一级缓存中有没有，如果没有就去发送新的Sql *&#x2F; @Test public void test05() throws IOException &#123; &#x2F;&#x2F;initSqlSessionFactory();如果不用@BeforeEach标注每次测试前都要先调用一下这个方法 &#x2F;&#x2F;1、第一个会话 SqlSession openSession &#x3D; sqlSessionFactory.openSession(); TeacherDao teacherDao &#x3D; openSession.getMapper(TeacherDao.class); Teacher teacher &#x3D; teacherDao.geTeacherById(1); System.out.println(teacher); System.out.println(&quot;------------------------------------------------------&quot;); System.out.println(&quot;手动清空缓存&quot;); &#x2F;&#x2F;清空当前SqlSession的一级缓存 openSession.clearCache(); Teacher teacher3 &#x3D; teacherDao.geTeacherById(1); System.out.println(teacher3); openSession.close(); &#125; &#x2F;** * 测试二级缓存 * *&#x2F; @Test public void test06() &#123; SqlSession openSession &#x3D; sqlSessionFactory.openSession(); SqlSession openSession2 &#x3D; sqlSessionFactory.openSession(); TeacherDao teacherDao &#x3D; openSession.getMapper(TeacherDao.class); TeacherDao teacherDao2 &#x3D; openSession2.getMapper(TeacherDao.class); &#x2F;&#x2F;1.第一个Dao查询1号teacher Teacher teacher &#x3D; teacherDao.geTeacherById(1); System.out.println(teacher); openSession.close(); &#x2F;&#x2F;2.第二个Dao查询1号teacher Teacher teacher2 &#x3D; teacherDao2.geTeacherById(1); System.out.println(teacher2); openSession2.close(); &#125; &#x2F;** * 测试先读一级缓存还是先读二级缓存 * 1. 不会出现一级缓存和二级缓存中有同一个数据的情况（一级缓存关闭了才会存到二级缓存） * 2.一级缓存；如果二级缓存没有数据就会去一级缓存中查询数据，如果一级缓存中没有数据， * 就会去数据库中查询，查询的结果放在一级缓存中。 * 3.任何时候都先查询二级缓存，再看一级缓存，如果都没有就去数据库查询 * * 先二 后一 最后库 * *&#x2F; @Test public void test07() &#123; SqlSession openSession &#x3D; sqlSessionFactory.openSession(); TeacherDao teacherDao &#x3D; openSession.getMapper(TeacherDao.class); Teacher teacher &#x3D; teacherDao.geTeacherById(1); System.out.println(teacher); openSession.close(); &#x2F;&#x2F;2.第二个Dao查询1号teacher SqlSession openSession2 &#x3D; sqlSessionFactory.openSession(); TeacherDao teacherDao2 &#x3D; openSession2.getMapper(TeacherDao.class); Teacher teacher2 &#x3D; teacherDao2.geTeacherById(1); System.out.println(teacher2); Teacher teacher3 &#x3D; teacherDao2.geTeacherById(1); System.out.println(teacher3); System.out.println(&quot;以下是查询二号老师，证明一二级缓存中没有会去数据库查询：&quot;); Teacher teacher4 &#x3D; teacherDao2.geTeacherById(2); System.out.println(teacher4); Teacher teacher5 &#x3D; teacherDao2.geTeacherById(2); System.out.println(teacher5); &#125;&#125;","categories":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"-ssm -MyBatis - MyBatis缓存机制","slug":"ssm-MyBatis-MyBatis缓存机制","permalink":"http://www.studyz.club/tags/ssm-MyBatis-MyBatis%E7%BC%93%E5%AD%98%E6%9C%BA%E5%88%B6/"}]},{"title":"connect Network is unreachable解決方法","slug":"centos7-error","date":"2019-11-10T14:27:03.583Z","updated":"2019-11-20T01:57:26.123Z","comments":true,"path":"posts/9a8d8088/","link":"","permalink":"http://www.studyz.club/posts/9a8d8088/","excerpt":"","text":"用systemctl start network.service重启网络提示用systemctl stauts network.service查看详情发现报错Failed to start LSB: Bring up/down networking. 依次执行以下指令 123systemctl stop NetworkManagersystemctl disable NetworkManager 重新启动网络： 1systemctl start network.service","categories":[],"tags":[]},{"title":"分布式数据库Hbase简介","slug":"分布式数据库Hbase简介","date":"2019-11-10T05:43:35.541Z","updated":"2019-12-07T07:45:56.485Z","comments":true,"path":"posts/9ace6c8/","link":"","permalink":"http://www.studyz.club/posts/9ace6c8/","excerpt":"","text":"一、 认识Hbase 1.1 什么是Hbase 1.2 Hbase的特点 1.3 Hbase中的一些概念 1.4 Hbase的基本工作原理 1.5 Hbase的逻辑模型 1.6 Hbase的概念模型 1.7 HBase基本架构 二、 HBase的环境角色 2.1 HMaster 2.2 RegionServer 2.3 组件简述其他组件 三、 Hbase的架构 四、Hbase读写流程 4.1、写流程 4.2、MemStore Flush 4.3、读流程 4.4、StoreFile Compaction 4.5、Region Split 4.6、Hbase删除数据 一、 认识Hbase1.1 什么是Hbase 如果只用一句话介绍hbase。我会说，hbase是一款数据库。我们之前讲过mysql它也是数据库。但是我们称之为传统的关系型数据库。Hbase是适合大数据分析的数据库。我们考虑一下mysql是怎么存的？一条记录一行这种方式适合保存数据和提取数据。但是我们想象大数据情况下，我们要每条记录是没有意义的，我们往往都是找出某一列对所有记录进行分析，比如平均工资，我们不关心某个人的工资，但是我们需要整个列的工资。而hbase就是按列存储的数据库。hbase列式数据库 HBase的起源 HBase的原型是Google的BigTable论文，受到了该论文思想的启发，目前作为Hadoop的子项目来开发维护，用于支持结构化的数据存储。 Hbase的官方网站 大数据的三驾马车mapreducegfs–&gt; hdfsbigtable–&gt;hbase 1.2 Hbase的特点和传统数据库表相比hbase有如下不同： 巨大：一个表可以有数十亿行,上百万列,他不像传统数据库为了减少数据的冗余分很多表。 面向列：面向列（族）的存储和权限控制，列（族）独立检索； 稀疏：对于空（null）的列，并不占用存储空间，表可以设计的非常稀疏；(在Mysql假如一个行中有一个属性列是空的那也占空间，Hbase不占) 数据多版本：每个单元中的数据可以有多个版本，默认情况下版本号自动分配，是时间戳； (插入更新，旧数据与新数据同时存在，只是时间戳不同) 数据类型单一：Hbase中的数据都是字符串，没有类型。 1.3 Hbase中的一些概念 和mysql类似我们在介绍mysql的SQL语法前了解和数据库中表的基本概念。下面我们要介绍一下Hbase中的几个属性及基本概念。需要注意的是和传统数据有较大的差别。 表:为了方便理解我们讲Hbase中的表大概对应成mysql样的，实际有出入 列族(ColumnFamily) 有意思的是表中有列但是多个列上面还有一个概念叫列族。 列和列族的关系是一个列族内可以有多个列并且可以动态增减。首先，hbase认为我们一般不会有全列查询就是类似于select * 的操作。这样的话每个列族有独立的存储单元可以提高查询效率。通过将列分成不同的列族hbase可以更少的访问磁盘。所以千万不要把经常查询的两个列放在不同的列族里，这样会导致hbase要访问两块磁盘区域。 rowkey 行键，非常关键类似mysq中的主键唯一标识一行记录。我们知道要想做到高效速度快就离不开索引，hbase就是真的使用rowkey做索引。而且会根据row可以进行排序，保证类似或相同的rowkey都集中在一起，以提高效率。hbase中行健是唯一标示，有几个行建就有几行 数据类型 hbase中存储的都是二进制信息，所以没啥数据类型的概念，操作时以字符串格式为基准，大表么。 cell 存储数据的单元格。 Timestamp 时间戳 hbase根据时间戳来区别相同rowkey记录。 总述：Hbase中存储数据的最基本的单位是列，一列或多列单元格数据形成一行，并由row key决定。一个表有若干行。每列可能有多个版本。1.4 Hbase的基本工作原理 用户在表格中存储数据，每行都有一个可排序的主键和任意多的列。由于是稀松存储，所以同一张表里面的每行数据都可以有截然不同的列。 列名字的格式是“&lt;family&gt;:&lt;qualifier&gt;”（&lt;列族&gt;:&lt;限定符&gt;），都是由字符串组成的。每一张表有一个列族（family）集合，这个集合是固定不变的，只能通过改变表结构来改变。但是限定符（qualifier）的值相对于每一行来说都是可以改变的。 HBase把同一个列族里面的数据存储在同一个目录底下，并且HBase的写操作是锁行的，每一行来说都是一个原子元素，都可以加锁。 HBase所有数据的更新都有一个时间戳标记，每个更新都是一个新的版本，HBase会保留一定数量的版本，这个值是可以设定的。客户端可以选择获取距离某个时间点最近的版本单元的值，或者一次获取所有版本单元的值。1.5 Hbase的逻辑模型 我们可以将一个表想象成一个大的映射关系，通过行健、行健+时间戳或行健+列（列族：列修饰符），就可以定位特定数据。由于HBase是稀疏存储数据的，所以某些列可以空白的。下表给出了www.cnn.com网站的数据存放逻辑视图，表中仅有一行数据，行的唯一标识为“com.cnn.www”，对这行数据的每一次逻辑修改都有一个时间戳关联对应。表中共有四列：contents:html、anchor:cnnsi.com、anchor:my.look.ca、mime:type，每一行以前缀的方式给出其所属的列族。 数据存储逻辑图示 注意这是是逻辑图示并不是真实的物理存储情况 行健是数据行在表中的唯一标识，并作为检索记录的主键。在HBase中访问表中的行只有三种方式：通过当个行健访问；给定行健的范围访问；全表扫描。行健可以任意字符串（最大长度64KB）并按照字典序进行存储。对于那些经常一起读取的行，需要对key值精心设计，以便它们能放在一起存储。 1.6 Hbase的概念模型 HBase是按照列存储的稀疏行/列矩阵，物理模型实际上就是把概念模型中的一行进行切割，并按照列族存储，这点在进行数据设计和程序开发的时候必须牢记。 上面的逻辑视图在物理存储的时候应该表现成下面的样子。 从表中可以看出表中的空值是不被存储的，所以查询时间戳为t8的“contents:html”将返回null，同样查询时间戳为t9，“anchor:my.lock.ca”的项也返回null。如果没有指明时间戳，那么应该返回指定列的最新数据值，并且最新的值在表格里也是最先找到的，因为它们是按照时间排序的。所以，如果查询“contents:”而不指明时间戳，将返回t6时刻的数据；查询“anchor:”的“my.look.ca”而不指明时间戳，将返回t8时刻的数据。这种存储结构还有一个优势，可以随时向表中的任何一个列族添加新列，而不需要是事先说明。 1.7 HBase基本架构 架构角色： 1）Region ServerRegion Server 为 Region 的管理者，其实现类为 HRegionServer，主要作用如下:对于数据的操作：get, put, delete；对于 Region 的操作：splitRegion、compactRegion。 2）MasterMaster 是所有 Region Server 的管理者，其实现类为 HMaster，主要作用如下：对于表的操作：create, delete, alter对于 RegionServer的操作：分配 regions到每个RegionServer，监控每个 RegionServer的状态，负载均衡和故障转移。 3）ZookeeperHBase 通过 Zookeeper 来做 Master 的高可用、RegionServer 的监控、元数据的入口以及集群配置的维护等工作。 4）HDFSHDFS 为 HBase 提供最终的底层数据存储服务，同时为 HBase 提供高可用的支持。 二、 HBase的环境角色2.1 HMaster 功能简述 监控RegionServer 处理RegionServer故障转移 处理元数据的变更 处理region的分配或移除 在空闲时间进行数据的负载均衡 通过Zookeeper发布自己的位置给客户端 2.2 RegionServer 功能简述 负责存储HBase的实际数据 处理分配给它的Region 刷新缓存到HDFS 维护HLog 执行压缩 负责处理Region分片 2.3 组件简述其他组件 Write-Ahead logs HBase的修改记录，当对HBase读写数据的时候，数据不是直接写进磁盘，它会在内存中保留一段时间（时间以及数据量阈值可以设定）。如果机器突然挂了，把数据保存在内存中会引起数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入内存中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。 HFile 这是在磁盘上保存原始数据的实际的物理文件，是实际的存储文件。 Store HFile存储在Store中，一个Store对应HBase表中的一个列族 MemStore 顾名思义，就是内存存储，位于内存中，用来保存当前的数据操作，所以当数据保存在WAL中之后，RegsionServer会在内存中存储键值对。 Region Hbase表的分片，HBase表会根据RowKey值被切分成不同的region存储在RegionServer中，在一个RegionServer中可以有多个不同的region 三、 Hbase的架构参考架构图： HBase 一种是作为存储的分布式文件系统，另一种是作为数据处理模型的 MR 框架。因为日常开发人员比较熟练的是结构化的数据进行处理，但是在 HDFS 直接存储的文件往往不具有结构化，所以催生出了 HBase 在 HDFS 上的操作。如果需要查询数据，只需要通过键值便可以成功访问。 1、StoreFile保存实际数据的物理文件，StoreFile 以 HFile 的形式存储在 HDFS 上。每个 Store 会有一个或多个 StoreFile（HFile），数据在每个 StoreFile 中都是有序的。 2、MemStore写缓存，由于 HFile 中的数据要求是有序的，所以数据是先存储在 MemStore 中，排好序后，等到达刷写时机才会刷写到 HFile，每次刷写都会形成一个新的 HFile。 3、WAL (预写入日志)由于数据要经 MemStore 排序后才能刷写到 HFile，但把数据保存在内存中会有很高的概率导致数据丢失，为了解决这个问题，数据会先写在一个叫做 Write-Ahead logfile 的文件中，然后再写入 MemStore 中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。 架构图示： HBase内置有zookeeper，但一般我们会有其他的Zookeeper集群来监管master和regionserver，Zookeeper通过选举，保证任何时候，集群中只有一个活跃的HMaster，HMaster与HRegionServer 启动时会向ZooKeeper注册，存储所有HRegion的寻址入口，实时监控HRegionserver的上线和下线信息。并实时通知给HMaster，存储HBase的schema（模式）和table元数据，默认情况下，HBase 管理ZooKeeper 实例，Zookeeper的引入使得HMaster不再是单点故障。一般情况下会启动两个HMaster，非Active的HMaster会定期的和Active HMaster通信以获取其最新状态，从而保证它是实时更新的，因而如果启动了多个HMaster反而增加了Active HMaster的负担。 一个RegionServer可以包含多个HRegion，每个RegionServer维护一个HLog，和多个HFiles以及其对应的MemStore。RegionServer运行于DataNode上，数量可以与DatNode数量一致。 四、Hbase读写流程4.1、写流程 1、Client 先访问 zookeeper，获取 hbase:meta 表位于哪个 RegionServer。 (老版本有一个-root-表，存的是meta表存放位置的信息，因为怕meta表太大切分，但在实际生产环境中几乎没有大道可以让meta切分情况，所以-root-就删除了) 2、访问对应的 RegionServer，获取 hbase:meta 表，根据读请求的 namespace:table/rowkey，查询出目标数据位于哪个 Region Server 中的哪个 Region 中。并将该 table 的 region 信息以及 meta 表的位置信息缓存在客户端的 meta cache，方便下次访问。 3、与目标 Region Server 进行通讯； 4、将数据顺序写入（追加）到 WAL； 5、将数据写入对应的 MemStore，数据会在 MemStore 进行排序； 6、向客户端发送 ack； 7、等达到 MemStore 的刷写时机后，将数据刷写到 HFile。 4.2、MemStore Flush 将数据写到磁盘 MemStore 刷写时机： 1.当某个 memstroe 的大小达到了 hbase.hregion.memstore.flush.size（默认值 128M），其所在 region 的所有 memstore 都会刷写。 当 memstore 的大小达到了 hbase.hregion.memstore.flush.size（默认值 128M） *hbase.hregion.memstore.block.multiplier（默认值 4）时，会阻止继续往该 memstore 写数据。 2.当 region server 中 memstore 的总大小达到 java_heapsize *hbase.regionserver.global.memstore.size（默认值 0.4） *hbase.regionserver.global.memstore.size.lower.limit（默认值 0.95）， region 会按照其所有 memstore 的大小顺序（由大到小）依次进行刷写。直到 region server中所有 memstore 的总大小减小到上述值以下。(一个region Server中包含多个region) 当 region server 中 memstore 的总大小达到 java_heapsize*hbase.regionserver.global.memstore.size（默认值 0.4）时，会阻止继续往所有的 memstore 写数据。 到达自动刷写的时间，也会触发 memstore flush。自动刷新的时间间隔由该属性进行配置 hbase.regionserver.optionalcacheflushinterval（默认 1 小时）。(最后一次内存编辑一个小时候，将整个内存进行刷写) 4.当 WAL 文件的数量超过 hbase.regionserver.max.logs，region 会按照时间顺序依次进行刷写，直到 WAL 文件数量减小到 hbase.regionserver.max.log 以下（该属性名已经废弃， 现无需手动设置，最大值为 32）。 4.3、读流程 读流程 因为可能写入的数据还没刷写到磁盘中，所以读的时候是内存和磁盘一起读，为了加速读的数据添加了Block Cache,Block Cache只对磁盘数据生效，MemStore读的数据不会放入里面 1、Client 先访问 zookeeper，获取 hbase:meta 表位于哪个 Region Server。 2、访问对应的 Region Server，获取 hbase:meta 表，根据读请求的 namespace:table/rowkey，查询出目标数据位于哪个 Region Server 中的哪个 Region 中。并将该 table 的 region 信息以及 meta 表的位置信息缓存在客户端的 meta cache，方便下次访问。 3、与目标 Region Server 进行通讯； 4、分别在 Block Cache（读缓存），MemStore 和 Store File（HFile）中查询目标数据，并将查到的所有数据进行合并。此处所有数据是指同一条数据的不同版本（time stamp）或者不同的类型（Put/Delete）。 5、将从文件中查询到的数据块（Block，HFile 数据存储单元，默认大小为 64KB）缓存到Block Cache。 6、将合并后的最终结果返回给客户端。 4.4、StoreFile Compaction 由于memstore每次刷写都会生成一个新的HFile，且同一个字段的不同版本（timestamp）和不同类型（Put/Delete）有可能会分布在不同的 HFile 中，因此查询时需要遍历所有的 HFile。为了减少 HFile 的个数，以及清理掉过期和删除的数据，会进行 StoreFile Compaction。 Compaction 分为两种，分别是 Minor Compaction和Major Compaction。Minor Compaction会将临近的若干个较小的 HFile 合并成一个较大的 HFile，但不会清理过期和删除的数据。Major Compaction 会将一个 Store 下的所有的 HFile 合并成一个大 HFile，并且会清理掉过期和删除的数据`。 4.5、Region Split 默认情况下，每个 Table 起初只有一个 Region，随着数据的不断写入，Region 会自动进行拆分。刚拆分时，两个子 Region 都位于当前的 Region Server，但处于负载均衡的考虑，HMaster 有可能会将某个 Region 转移给其他的 Region Server。 Region Split 时机： 1.当1个region中的某个Store下所有StoreFile的总大小超过hbase.hregion.max.filesize，该 Region 就会进行拆分（0.94 版本之前）。 当 1 个 region 中 的 某 个 Store 下所有 StoreFile 的 总 大 小 超 过 Min(R^2 *&quot;hbase.hregion.memstore.flush.size&quot;,hbase.hregion.max.filesize&quot;)，该 Region 就会进行拆分，其中 R 为当前 Region Server 中属于该 Table 的个数（0.94 版本之后）。 4.6、Hbase删除数据 HBase 的删除操作并不会立即将数据从磁盘上删除，删除操作主要是对要被删除的数据打上标记。 当执行删除操作时，HBase 新插入一条相同的 KeyValue 数据，但是使 keytype=Delete，这便意味着数据被删除了，直到发生 Major compaction 操作时，数据才会被真正的从磁盘上删除，删除标记也会从StoreFile删除。 Time To Live (TTL)ColumnFamilies可以设置TTL长度（以秒为单位），HBase将在到期时间后自动删除行。这适用于行的所有版本，包括当前版本。当Minor compaction操作时，仅删除包含过期行的存储文件。设置hbase.store.delete.expired.storefile为false禁用此功能。将最小版本数设置为0以外也会禁用此功能。HBase还支持按每个单元格设置生存时间（TTL）。 Cell TTL处理和ColumnFamily TTL之间存在两个显着差异： Cell TTL以毫秒而不是秒为单位表示。 Cell TTL的TTL不能超过ColumnFamily 的TTL。 Keeping Deleted Cells可以选择保留已删除的单元格。删除key之后，数据是否还保留。 默认情况下，delete标记会涉及到时间的开始处（即delete操作会标记所有的版本）。因此，Get或Scan操作不会看到已删除的单元格(行或列)，即使Get或Scan操作设置了删除标记之前的时间范围。 ColumnFamilies可以选择保留已删除的单元格。在这种情况下，仍然可以检索已删除的单元格，只要这些操作指定一个时间范围，该时间范围在任何会影响单元格的删除的时间戳之前结束。允许在删除的情况下进行时间点查询。 被删除的单元格仍然受TTL限制，并且永远不会有超过“最大版本数量”的已删除单元格。 12使用HBase Shell 更改KEEP_DELETED_CELLS的值hbase&gt; hbase&gt; alter&#39;t1&#39;，NAME &#x3D;&gt;&#39;f1&#39;，KEEP_DELETED_CELLS &#x3D;&gt; true 12使用API 更改KEEP_DELETED_CELLS的值HColumnDescriptor.setKeepDeletedCells(true); KEEP_DELETED_CELLS是为了避免从HBase中删除单元格，因为删除它们的惟一原因是删除标记。因此，在启用了KEEP_DELETED_CELLS后，如果编写的版本多于配置的最大值，或者TTL和单元格超过了配置的超时时间，则删除单元格。 hbase权威指南http://www.hbasebook.comhttps://github.com/larsgeorge/hbase-book","categories":[{"name":"分布式数据库","slug":"分布式数据库","permalink":"http://www.studyz.club/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"-分布式数据库  -Hbase简介","slug":"分布式数据库-Hbase简介","permalink":"http://www.studyz.club/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93-Hbase%E7%AE%80%E4%BB%8B/"}]},{"title":"软件开发框架之MyBatis动态sql","slug":"软件开发框架之Spring-Mybatis动态sql","date":"2019-11-07T14:25:01.322Z","updated":"2019-11-09T08:12:26.052Z","comments":true,"path":"posts/a50b770b/","link":"","permalink":"http://www.studyz.club/posts/a50b770b/","excerpt":"","text":"一、 动态SQL中的元素介绍 二、&lt;if&gt;元素 2.1 &lt;where&gt;元素 2.2 重用sql&lt;sql&gt;和&lt;include&gt;元素 三、&lt;choose&gt;、&lt;when&gt;、&lt;otherwise&gt;元素 四、 &lt;trim&gt;元素 五、&lt;set&gt;元素 六、 &lt;foreach&gt;元素 &gt;七、 &lt;bind&gt;元素 八、 动态sql补充-OGNL表达式 九、 动态Sql测试实例源码 一、 动态SQL中的元素介绍动态SQL有什么作用？ 开发人员在使用JDBC或其他类似的框架进行数据库开发时，通常都要根据需求去手动拼装SQL，这是一个非常麻烦且痛苦的工作，而MyBatis提供的对SQL语句动态组装的功能，恰能很好的解决这一麻烦工作。 动态SQL是MyBatis的强大特性之一，MyBatis3采用了功能强大的基于OGNL的表达式来完成动态SQL。动态SQL主要元素如下表所示： 二、&lt;if&gt;元素 在MyBatis中，&lt;if&gt;元素是最常用的判断语句，它类似于Java中的if语句，主要用于实现某些简单的条件选择。其基本使用示例如下： 转义字符查询 1234567891011121314151617181920212223242526272829303132333435363738&lt;!-- 自定义封装规则 --&gt; &lt;resultMap type&#x3D;&quot;com.lizhi.bean.Teacher&quot; id&#x3D;&quot;teacherMap&quot;&gt; &lt;id property&#x3D;&quot;id&quot; column&#x3D;&quot;id&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;address&quot; column&#x3D;&quot;address&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;birth&quot; column&#x3D;&quot;birth_date&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;course&quot; column&#x3D;&quot;class_name&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;name&quot; column&#x3D;&quot;teacherName&quot;&#x2F;&gt; &lt;&#x2F;resultMap&gt; &lt;!-- public Teacher geTeacherById(Integer id); --&gt; &lt;select id&#x3D;&quot;geTeacherById&quot; resultMap&#x3D;&quot;teacherMap&quot;&gt; select * from t_teacher where id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; &lt;!-- &lt;if&gt;: 判断 --&gt; &lt;!-- public List&lt;Teacher&gt; geTeacherByCondition(Teacher teacher);&#x2F;&#x2F;按照一定的条件查询老师 --&gt; &lt;select id&#x3D;&quot;geTeacherByCondition&quot; resultMap&#x3D;&quot;teacherMap&quot;&gt; select * from t_teacher where &lt;!-- test&#x3D;&quot;&quot; : 编写判断条件 id!&#x3D;null : 取出传入的JavaBean属性中的id值，判断其是否为空 --&gt; &lt;if test&#x3D;&quot;id!&#x3D;null&quot;&gt; id &gt; #&#123;id&#125; and &lt;&#x2F;if&gt; &lt;!-- 判断空串&#39;&#39;，双引号的转义字符&quot; and : &amp;&amp; ，输入&amp;&amp;时会报错，需要用转义字符&amp;和html里一样 or : || if() : 传入非常强大的判断条件，之前java中怎么写判断这里都可以; 叫OGNL表达式； 转义字符：https:&#x2F;&#x2F;www.w3school.com.cn&#x2F;tags&#x2F;html_ref_entities.html --&gt; &lt;if test&#x3D;&quot;name!&#x3D;null &amp;&amp; !name.equals(&quot;&quot;) &quot;&gt; teacherName like #&#123;name&#125; and &lt;&#x2F;if&gt; &lt;if test&#x3D;&quot;birth!&#x3D;null&quot;&gt; birth_date &lt; #&#123;birth&#125; &lt;&#x2F;if&gt; &lt;&#x2F;select&gt; 测试12345678910111213141516171819202122&#x2F;** * 测试&lt;if&gt; *&#x2F;@Testpublic void test02() throws IOException &#123; SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; TeacherDao mapper &#x3D; openSession.getMapper(TeacherDao.class); Teacher teacher &#x3D; new Teacher(); teacher.setId(1); teacher.setName(&quot;%a%&quot;);&#x2F;&#x2F;表示名字字母带有a的名字 teacher.setBirth(new Date()); List&lt;Teacher&gt; list &#x3D; mapper.geTeacherByCondition(teacher); System.out.println(list); &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125; 这里有个问题；这里写了三个判断语句，但是如果最后的一个判断没过，只拼接到第二个，结尾就会多个and解决办法就是把写的if判断写到里让MyBatis自动补充and &lt;where&gt;元素&lt;where&gt;能自动帮我们去除位于前面的and同样的也能使用 &lt;trim&gt;元素进行截取字符串12345678910111213141516171819202122232425262728 &lt;!-- &lt;if&gt;: 判断 --&gt;&lt;!-- public List&lt;Teacher&gt; geTeacherByCondition(Teacher teacher);&#x2F;&#x2F;按照一定的条件查询老师 --&gt;&lt;select id&#x3D;&quot;geTeacherByCondition&quot; resultMap&#x3D;&quot;teacherMap&quot;&gt; select * from t_teacher &lt;!-- test&#x3D;&quot;&quot; : 编写判断条件 id!&#x3D;null : 取出传入的JavaBean属性中的id值，判断其是否为空 --&gt; &lt;!-- 可以帮我们去掉前面的and --&gt; &lt;where&gt; &lt;if test&#x3D;&quot;id!&#x3D;null&quot;&gt; and id &gt; #&#123;id&#125; &lt;&#x2F;if&gt; &lt;!-- 判断空串&#39;&#39;，双引号的转义字符&quot; and : &amp;&amp; ，输入&amp;&amp;时会报错，需要用转义字符&amp;和html里一样 or : || if() : 传入非常强大的判断条件，之前java中怎么写判断这里都可以; 叫OGNL表达式； 转义字符：https:&#x2F;&#x2F;www.w3school.com.cn&#x2F;tags&#x2F;html_ref_entities.html 这里有个问题；这里写了三个判断语句，但是如果最后的一个判断没过，只拼接到第二个，结尾就会多个and 解决办法就是把写的if判断写到&lt;where&gt;里让MyBatis自动补充 --&gt; &lt;if test&#x3D;&quot;name!&#x3D;null &amp;&amp; !name.equals(&quot;&quot;) &quot;&gt; and teacherName like #&#123;name&#125; &lt;&#x2F;if&gt; &lt;if test&#x3D;&quot;birth!&#x3D;null&quot;&gt; and birth_date &lt; #&#123;birth&#125; &lt;&#x2F;if&gt; &lt;&#x2F;where&gt; &lt;&#x2F;select&gt; 在实际应用中，我们可能会通过多个条件来精确的查询某个数据。例如，要查找某个客户的信息，可以通过姓名和职业来查找客户，也可以不填写职业直接通过姓名来查找客户，还可以都不填写而查询出所有客户，此时姓名和职业就是非必须条件。 2.2 重用sql&lt;sql&gt;和&lt;include&gt;元素1234567&lt;!-- public Teacher geTeacherById(Integer id); --&gt; &lt;!-- 抽取可重用的sql语句 --&gt; &lt;sql id&#x3D;&quot;selectSql&quot;&gt;select * from t_teacher&lt;&#x2F;sql&gt; &lt;select id&#x3D;&quot;geTeacherById&quot; resultMap&#x3D;&quot;teacherMap&quot;&gt; &lt;include refid&#x3D;&quot;selectSql&quot;&gt;&lt;&#x2F;include&gt; where id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; 三、&lt;choose&gt;、&lt;when&gt;、&lt;otherwise&gt;元素假设如下场景： “当客户名称不为空，则只根据客户名称进行客户筛选； 当客户名称为空，而客户职业不为空，则只根据客户职业进行客户筛选。 当客户名称和客户职业都为空，则要求查询出所有电话不为空的客户信息。” 这种情况下，使用&lt;if&gt;元素进行处理是非常不合适的。如果使用的是Java语言，这种情况显然更适合使用switch…case…default语句来处理，而在SQL中就可以使用&lt;choose&gt;、&lt;when&gt;、&lt;otherwise&gt;元素组合进行处理。其基本使用示例如代码所示： 12345678910111213141516171819202122232425&lt;!-- public List&lt;Teacher&gt; geTeacherByChoose(Teacher teacher); &#x2F;&#x2F;需求: 带id用id查，不带id用其他属性查 --&gt;&lt;select id&#x3D;&quot;geTeacherByChoose&quot; resultMap&#x3D;&quot;teacherMap&quot;&gt; select * from t_teacher &lt;where&gt; &lt;choose&gt; &lt;when test&#x3D;&quot;id!&#x3D;null&quot;&gt; id&#x3D;#&#123;id&#125; &lt;&#x2F;when&gt; &lt;!-- &quot;&quot; 空串转义符 --&gt; &lt;when test&#x3D;&quot;name!&#x3D;null and name.equals(&quot;&quot;)&quot;&gt; teacherName&#x3D;#&#123;name&#125; &lt;&#x2F;when&gt; &lt;when test&#x3D;&quot;birth!&#x3D;null&quot;&gt; birth_date&#x3D;#&#123;birth&#125; &lt;&#x2F;when&gt; &lt;!-- 其他情況 --&gt; &lt;otherwise&gt; 1&#x3D;1 &lt;&#x2F;otherwise&gt; &lt;&#x2F;choose&gt; &lt;&#x2F;where&gt;&lt;&#x2F;select&gt; 测试1234567891011121314151617181920&#x2F;** * 测试&lt;choose&gt;元素分支选择 *&#x2F; @Test public void test04() throws IOException &#123; SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; TeacherDao mapper &#x3D; openSession.getMapper(TeacherDao.class); Teacher teacher &#x3D; new Teacher(); teacher.setId(1); teacher.setName(&quot;LaoCui&quot;);&#x2F;&#x2F;表示名字字母带有a的名字 List&lt;Teacher&gt; list &#x3D; mapper.geTeacherByChoose(teacher); System.out.println(list); &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125; 结果，我们看到，当有id的时候先用id查12345DEBUG 11-08 20:27:44,118 &#x3D;&#x3D;&gt; Preparing: select * from t_teacher WHERE id&#x3D;? (BaseJdbcLogger.java:143) DEBUG 11-08 20:27:44,150 &#x3D;&#x3D;&gt; Parameters: 1(Integer) (BaseJdbcLogger.java:143) DEBUG 11-08 20:27:44,172 &lt;&#x3D;&#x3D; Total: 1 (BaseJdbcLogger.java:143) [Teacher [id&#x3D;1, name&#x3D;LaoHan, course&#x3D;语文, address&#x3D;让胡路, birth&#x3D;Wed Jul 01 00:00:00 CST 1998]] 四、 &lt;trim&gt;元素123456789101112131415161718192021222324252627282930313233343536&lt;!-- &lt;if&gt;: 判断 --&gt; &lt;!-- public List&lt;Teacher&gt; geTeacherByCondition(Teacher teacher);&#x2F;&#x2F;按照一定的条件查询老师 --&gt; &lt;select id&#x3D;&quot;geTeacherByCondition&quot; resultMap&#x3D;&quot;teacherMap&quot;&gt; select * from t_teacher &lt;!-- test&#x3D;&quot;&quot; : 编写判断条件 id!&#x3D;null : 取出传入的JavaBean属性中的id值，判断其是否为空 --&gt; &lt;!-- 可以帮我们去掉前面的and --&gt; &lt;!-- trim : 截取字符串 prefix&#x3D;&quot;&quot; : 前缀，为下面的sql整体添加一个前缀 prefixOverrides&#x3D;&quot;&quot; : 取出整体字符串前面多余的字符串 suffix&#x3D;&quot;&quot; : 为整体添加一个后缀 suffixOverrides&#x3D;&quot;&quot; : 去掉多余的后缀 --&gt; &lt;trim prefix&#x3D;&quot;where&quot; prefixOverrides&#x3D;&quot;and&quot; suffixOverrides&#x3D;&quot;and&quot;&gt; &lt;if test&#x3D;&quot;id!&#x3D;null&quot;&gt; and id &gt; #&#123;id&#125; &lt;&#x2F;if&gt; &lt;!-- 判断空串&#39;&#39;，双引号的转义字符&quot; and : &amp;&amp; ，输入&amp;&amp;时会报错，需要用转义字符&amp;和html里一样 or : || if() : 传入非常强大的判断条件，之前java中怎么写判断这里都可以; 叫OGNL表达式； 转义字符：https:&#x2F;&#x2F;www.w3school.com.cn&#x2F;tags&#x2F;html_ref_entities.html 对于where标签这里有个问题；这里写了三个判断语句，但是如果最后的一个判断没过，只拼接到第二个， 结尾就会多个and解决办法就是把写的if判断写到&lt;where&gt;里让MyBatis自动补充 --&gt; &lt;if test&#x3D;&quot;name!&#x3D;null &amp;&amp; !name.equals(&quot;&quot;) &quot;&gt; and teacherName like #&#123;name&#125; &lt;&#x2F;if&gt; &lt;if test&#x3D;&quot;birth!&#x3D;null&quot;&gt; and birth_date &lt; #&#123;birth&#125; &lt;&#x2F;if&gt; &lt;&#x2F;trim&gt; &lt;&#x2F;select&gt; 同样的道理，在MyBatis的SQL中就可以使用&lt;where&gt;元素 五、&lt;set&gt;元素 在Hibernate中，想要更新某个对象，就需要发送所有的字段给持久化对象，这种想更新的每一条数据都要将其所有的属性都更新一遍的方法，其执行效率是非常差的。为此，在MyBatis中可以使用动态SQL中的&lt;set&gt;元素进行处理：使用&lt;set&gt;和&lt;if&gt;元素对username和jobs进行更新判断，并动态组装SQL。这样就只需要传入想要更新的字段即可 123456789101112131415161718192021222324&lt;!-- public Integer updateTeacher(Teacher teacher); &#x2F;&#x2F;&lt;set&gt;标签 动态更新 --&gt; &lt;select id&#x3D;&quot;updateTeacher&quot; resultMap&#x3D;&quot;teacherMap&quot;&gt; UPDATE t_teacher &lt;set&gt; &lt;if test&#x3D;&quot;name!&#x3D;null and !name.equals(&quot;&quot;)&quot;&gt; teacherName&#x3D;#&#123;name&#125;, &lt;&#x2F;if&gt; &lt;if test&#x3D;&quot;course!&#x3D;null and !course.equals(&quot;&quot;)&quot;&gt; class_name&#x3D;#&#123;course&#125;, &lt;&#x2F;if&gt; &lt;if test&#x3D;&quot;address!&#x3D;null and !address.equals(&quot;&quot;)&quot;&gt; address&#x3D;#&#123;address&#125;, &lt;&#x2F;if&gt; &lt;if test&#x3D;&quot;birth!&#x3D;null and !birth.equals(&quot;&quot;)&quot;&gt; birth_date&#x3D;#&#123;birth&#125; &lt;&#x2F;if&gt; &lt;&#x2F;set&gt; &lt;where&gt; id&#x3D;#&#123;id&#125; &lt;&#x2F;where&gt; &lt;&#x2F;select&gt; 测试12345678910111213141516171819202122 &#x2F;** * 测试&lt;set&gt; *&#x2F;@Testpublic void test05() throws IOException &#123; SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; TeacherDao mapper &#x3D; openSession.getMapper(TeacherDao.class); Teacher teacher &#x3D; new Teacher(); teacher.setId(1); teacher.setName(&quot;hh&quot;); mapper.updateTeacher(teacher); &#x2F;&#x2F;提交 openSession.commit(); &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125; 六、 &lt;foreach&gt;元素假设如下需求：在一个客户表中有1000条数据，现在需要将id值小于100的客户信息全部查询出来，这要怎么做呢？ 针对上述需求，理想的解决方法就是使用MyBatis中动态SQL的&lt;foreach&gt;元素进行处理。其基本使用示例如下所示：1234567891011121314151617181920212223242526&lt;!--public List&lt;Teacher&gt; getTeachersByIdIn(@Param(&quot;ids&quot;)List&lt;Integer&gt; ids); &#x2F;&#x2F;查出所有的Teacher传入一个集合，然后查询出这个集合的所有老师 --&gt; &lt;select id&#x3D;&quot;getTeachersByIdIn&quot; resultMap&#x3D;&quot;teacherMap&quot;&gt; select * from t_teacher where id in &lt;!-- 帮我们遍历集合的 collection&#x3D;&quot;&quot; : 指定要遍历的集合的key,这里用@Param(&quot;ids&quot;)起了个别名所以写ids就行 close&#x3D;&quot;&quot; : 指定以什么结束 index&#x3D;&quot;&quot; : 索引， 如果遍历出的是一个list, index:指定的变量保存了当前索引 item: 指定的元素保存了当前遍历的元素的值 如果遍历的是一个map,即(k,v) index,指定的变量就是保存了当前遍历的元素的key item,指定的变量保存当前遍历的元素的值 item&#x3D;&quot;&quot; : 每次遍历出的元素起一个变量名，相当于爪娃里for循环的i open&#x3D;&quot;&quot; : 指定以什么开始 separator&#x3D;&quot;&quot; : 每次遍历元素的分隔符 因为sql语句in后面跟的集合需要写成in (1,2,3,4,5,6),所以需要open和close --&gt; &lt;foreach collection&#x3D;&quot;ids&quot; item&#x3D;&quot;id_item&quot; separator&#x3D;&quot;,&quot; open&#x3D;&quot;(&quot; close&#x3D;&quot;)&quot; index&#x3D;&quot;&quot;&gt; #&#123;id_item&#125; &lt;&#x2F;foreach&gt; &lt;&#x2F;select&gt; 在使用&lt;foreach&gt;时最关键也是最容易出错的就是collection属性，该属性是必须指定的，而且在不同情况下，该属性的值是不一样的。主要有以下3种情况： 如果传入的是单参数且参数类型是一个数组或者List的时候，collection属性值分别为array和list（或collection）。 如果传入的参数是多个的时候，就需要把它们封装成一个Map了，当然单参数也可以封装成Map集合，这时候collection属性值就为Map的key。 如果传入的参数是POJO包装类的时候，collection属性值就为该包装类中需要进行遍历的数组或集合的属性名。 七、 &lt;bind&gt;元素 &lt;bind&gt;元素 MyBatis的元素可以通过OGNL表达式来创建一个上下文变量，其使用方式如下： 12345678&lt;select id&#x3D;&quot;findCustomerByName&quot; parameterType&#x3D;&quot;com.dqsy.po.Customer&quot; resultType&#x3D;&quot;com.dqsy.po.Customer&quot;&gt; &lt;bind name&#x3D;&quot;pattern_username&quot; value&#x3D;&quot;&#39;%&#39;+_parameter.getUsername()+&#39;%&#39;&quot; &#x2F;&gt; select * from t_customer where username like #&#123;pattern_username&#125;&lt;&#x2F;select&gt; 八、 动态sql补充-OGNL表达式 OGNL(Object-Graph Navigation Language)中文名：对象图导航语言，通过它简单一致的表达式语法，可以存取对象的任意属性，调用对象的方法，遍历整个对象的结构图，实现字段类型转化等功能。它使用相同的表达式去存取对象的属性。这样可以更好的取得数据。主要是#、%和$这三个符号的使用。 123456789101112131415161718192021&lt;!-- OGNL表达式； 对象导航图Person下有三个属性，address下又有三个属性，street下又有三个属性 Person &#x3D;&#x3D;&#x3D;lastName &#x3D;&#x3D;&#x3D;email &#x3D;&#x3D;&#x3D;Address &#x3D;&#x3D;&#x3D;city &#x3D;&#x3D;&#x3D;province &#x3D;&#x3D;&#x3D;street &#x3D;&#x3D;&#x3D;adminName &#x3D;&#x3D;&#x3D;&#x3D;info &#x3D;&#x3D;&#x3D;perCount 可用于方法，静态方法，构造器等等等 在mybatis中，传入的参数可以用来判断； 额外还有两个东西； _parmeter :代表传入来的参数(&lt;if test&#x3D;&quot;_parmeter&quot;&gt;&lt;&#x2F;if&gt;) ①，单个参数 : _parmeter就代表这个参数 ②，传入多个参数 : _parmeter就代表多个参数集合起来的map _databaseId : 代表当前环境(&lt;if test&#x3D;&quot;_databaseId&#x3D;&#x3D;&#39;mysql&#39;&quot;&gt;&lt;&#x2F;if&gt;)标识数据库 如果配置了databaseIdProvider : _databaseId就有值 --&gt; 二、作用1.支持对象方法调用 objName.methodName() 2.支持类静态方法调用和值访问 @java.lang.String@format(‘name’,’mark’) 3.支持赋值操作和表达式串联 price=100,calculatePrice() 4.支持访问OGNL上下文(OGNL context)和ActionContext 5.支持操作集合对象 三、要素（表达式、根对象、上下文环境）OGNL的操作实际上就是围绕着OGNL结构的三个要素而进行的，分别是表达式(Expresssion)、根对象(Root Object)、上下文环境(Context) 1.表达式表达式是整个OGNL的核心，OGNL会根据表达式去对象中取值。所有OGNL操作都是针对表达式解析后进行的。它表明了此次OGNL操作要”做什么”。表达式就是一个带有语法含义的字符串，这个字符串规定了操作的类型和操作的内容。OGNL支持大量的表达式语法，不仅支持这种”链式”对象访问路径，还支持在表达式中进行简单的计算 2.根对象实际上OGNL的取值还需要一个上下文环境。设置了Root对象，OGNL可以对Root对象进行取值或写值等操作，Root对象所在环境就是OGNL的上下文环境（Context）。上下文环境规定了OGNL的操作”在哪里进行”。上下文环境Context是一个Map类型的对象，在表达式中访问Context中对象，需要使用”#”号加上对象名称，即”#对象名称”的形式 3.上下文环境实际上OGNL的取值还需要一个上下文环境，设置了Root对象，OGNL可以对Root对象进行取值或写值等操作，Root对象所在环境就是OGNL的上下文环境（Context）。上下文环境规定了OGNL的操作”在哪里进行”。上下文环境Context是一个Map类型的对象，在表达式中访问Context中的对象，需要使用”#”号加上对象名称，即”#对象名称”的形式 九、 动态Sql测试实例源码 t_teacher表 package com.lizhi.bean;class Teacher12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.lizhi.bean;import java.util.Date;public class Teacher &#123; private Integer id; private String name; private String course; private String address; private Date birth;&#x2F;&#x2F;Date是util包里的 public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id &#x3D; id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name &#x3D; name; &#125; public String getCourse() &#123; return course; &#125; public void setCourse(String course) &#123; this.course &#x3D; course; &#125; public String getAddress() &#123; return address; &#125; public void setAddress(String address) &#123; this.address &#x3D; address; &#125; public Date getBirth() &#123; return birth; &#125; public void setBirth(Date birth) &#123; this.birth &#x3D; birth; &#125; @Override public String toString() &#123; return &quot;Teacher [id&#x3D;&quot; + id + &quot;, name&#x3D;&quot; + name + &quot;, course&#x3D;&quot; + course + &quot;, address&#x3D;&quot; + address + &quot;, birth&#x3D;&quot; + birth + &quot;]&quot;; &#125; &#125; package com.lizhi.dao;interface TeacherDao1234567891011121314151617181920212223242526272829package com.lizhi.dao;import java.util.List;import org.apache.ibatis.annotations.Param;import com.lizhi.bean.Teacher;&#x2F;* * mapper和dao一样都是完成bean到数据库的映射 *&#x2F;public interface TeacherDao &#123; public Teacher geTeacherById(Integer id); &#x2F;&#x2F;按照一定的条件查询老师 public List&lt;Teacher&gt; geTeacherByCondition(Teacher teacher); &#x2F;&#x2F;查出所有的Teacher传入一个集合，然后查询出这个集合的所有老师,用来测试&lt;foreach&gt;标签 public List&lt;Teacher&gt; getTeachersByIdIn(@Param(&quot;ids&quot;)List&lt;Integer&gt; ids); &#x2F;&#x2F;测试&lt;choose&gt;标签分支选择 public List&lt;Teacher&gt; geTeacherByChoose(Teacher teacher); &#x2F;&#x2F;&lt;set&gt;标签 动态更新 public Integer updateTeacher(Teacher teacher); &#125; TeacherDao.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-&#x2F;&#x2F;mybatis.org&#x2F;&#x2F;DTD Mapper 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;mybatis.org&#x2F;dtd&#x2F;mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace&#x3D;&quot;com.lizhi.dao.TeacherDao&quot;&gt;&lt;!-- 自定义封装规则 --&gt; &lt;resultMap type&#x3D;&quot;com.lizhi.bean.Teacher&quot; id&#x3D;&quot;teacherMap&quot;&gt; &lt;id property&#x3D;&quot;id&quot; column&#x3D;&quot;id&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;address&quot; column&#x3D;&quot;address&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;birth&quot; column&#x3D;&quot;birth_date&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;course&quot; column&#x3D;&quot;class_name&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;name&quot; column&#x3D;&quot;teacherName&quot;&#x2F;&gt; &lt;&#x2F;resultMap&gt; &lt;!-- public Teacher geTeacherById(Integer id); --&gt; &lt;!-- 抽取可重用的sql语句 --&gt; &lt;sql id&#x3D;&quot;selectSql&quot;&gt;select * from t_teacher&lt;&#x2F;sql&gt; &lt;select id&#x3D;&quot;geTeacherById&quot; resultMap&#x3D;&quot;teacherMap&quot;&gt; &lt;include refid&#x3D;&quot;selectSql&quot;&gt;&lt;&#x2F;include&gt; where id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; &lt;!-- &lt;if&gt;: 判断 --&gt; &lt;!-- public List&lt;Teacher&gt; geTeacherByCondition(Teacher teacher);&#x2F;&#x2F;按照一定的条件查询老师 --&gt; &lt;select id&#x3D;&quot;geTeacherByCondition&quot; resultMap&#x3D;&quot;teacherMap&quot;&gt; select * from t_teacher &lt;!-- test&#x3D;&quot;&quot; : 编写判断条件 id!&#x3D;null : 取出传入的JavaBean属性中的id值，判断其是否为空 --&gt; &lt;!-- 可以帮我们去掉前面的and --&gt; &lt;!-- trim : 截取字符串 prefix&#x3D;&quot;&quot; : 前缀，为下面的sql整体添加一个前缀 prefixOverrides&#x3D;&quot;&quot; : 取出整体字符串前面多余的字符串 suffix&#x3D;&quot;&quot; : 为整体添加一个后缀 suffixOverrides&#x3D;&quot;&quot; : 去掉多余的后缀 --&gt; &lt;trim prefix&#x3D;&quot;where&quot; prefixOverrides&#x3D;&quot;and&quot; suffixOverrides&#x3D;&quot;and&quot;&gt; &lt;if test&#x3D;&quot;id!&#x3D;null&quot;&gt; and id &gt; #&#123;id&#125; &lt;&#x2F;if&gt; &lt;!-- 判断空串&#39;&#39;，双引号的转义字符&quot; and : &amp;&amp; ，输入&amp;&amp;时会报错，需要用转义字符&amp;和html里一样 or : || if() : 传入非常强大的判断条件，之前java中怎么写判断这里都可以; 叫OGNL表达式； 转义字符：https:&#x2F;&#x2F;www.w3school.com.cn&#x2F;tags&#x2F;html_ref_entities.html 对于where标签这里有个问题；这里写了三个判断语句，但是如果最后的一个判断没过，只拼接到第二个， 结尾就会多个and解决办法就是把写的if判断写到&lt;where&gt;里让MyBatis自动补充 --&gt; &lt;!-- OGNL表达式； 对象导航图Person下有三个属性，address下又有三个属性，street下又有三个属性 Person &#x3D;&#x3D;&#x3D;lastName &#x3D;&#x3D;&#x3D;email &#x3D;&#x3D;&#x3D;Address &#x3D;&#x3D;&#x3D;city &#x3D;&#x3D;&#x3D;province &#x3D;&#x3D;&#x3D;street &#x3D;&#x3D;&#x3D;adminName &#x3D;&#x3D;&#x3D;&#x3D;info &#x3D;&#x3D;&#x3D;perCount 可用于方法，静态方法，构造器等等等 在mybatis中，传入的参数可以用来判断； 额外还有两个东西； _parmeter :代表传入来的参数(&lt;if test&#x3D;&quot;_parmeter&quot;&gt;&lt;&#x2F;if&gt;) ①，单个参数 : _parmeter就代表这个参数 ②，传入多个参数 : _parmeter就代表多个参数集合起来的map _databaseId : 代表当前环境(&lt;if test&#x3D;&quot;_databaseId&#x3D;&#x3D;&#39;mysql&#39;&quot;&gt;&lt;&#x2F;if&gt;)标识数据库 如果配置了databaseIdProvider : _databaseId就有值 --&gt; &lt;!-- 绑定一个表达式的值到一个变量 --&gt; &lt;bind name&#x3D;&quot;_name&quot; value&#x3D;&quot;&#39;%&#39;+name+&#39;%&#39;&quot;&#x2F;&gt; &lt;if test&#x3D;&quot;name!&#x3D;null &amp;&amp; !name.equals(&quot;&quot;)&quot;&gt; and teacherName like #&#123;name&#125; &lt;&#x2F;if&gt; &lt;if test&#x3D;&quot;birth!&#x3D;null&quot;&gt; and birth_date &lt; #&#123;birth&#125; &lt;&#x2F;if&gt; &lt;&#x2F;trim&gt; &lt;&#x2F;select&gt; &lt;!--public List&lt;Teacher&gt; getTeachersByIdIn(@Param(&quot;ids&quot;)List&lt;Integer&gt; ids); &#x2F;&#x2F;查出所有的Teacher传入一个集合，然后查询出这个集合的所有老师 --&gt; &lt;select id&#x3D;&quot;getTeachersByIdIn&quot; resultMap&#x3D;&quot;teacherMap&quot;&gt; select * from t_teacher where id in &lt;!-- 帮我们遍历集合的 collection&#x3D;&quot;&quot; : 指定要遍历的集合的key,这里用@Param(&quot;ids&quot;)起了个别名所以写ids就行 close&#x3D;&quot;&quot; : 指定以什么结束 index&#x3D;&quot;&quot; : 索引， 如果遍历出的是一个list, index:指定的变量保存了当前索引 item: 指定的元素保存了当前遍历的元素的值 如果遍历的是一个map,即(k,v) index,指定的变量就是保存了当前遍历的元素的key item,指定的变量保存当前遍历的元素的值 item&#x3D;&quot;&quot; : 每次遍历出的元素起一个变量名，相当于爪娃里for循环的i open&#x3D;&quot;&quot; : 指定以什么开始 separator&#x3D;&quot;&quot; : 每次遍历元素的分隔符 因为sql语句in后面跟的集合需要写成in (1,2,3,4,5,6),所以需要open和close --&gt; &lt;if test&#x3D;&quot;ids.size &gt; 0&quot;&gt; &lt;foreach collection&#x3D;&quot;ids&quot; item&#x3D;&quot;id_item&quot; separator&#x3D;&quot;,&quot; open&#x3D;&quot;(&quot; close&#x3D;&quot;)&quot; index&#x3D;&quot;&quot;&gt; #&#123;id_item&#125; &lt;&#x2F;foreach&gt; &lt;&#x2F;if&gt; &lt;&#x2F;select&gt; &lt;!-- public List&lt;Teacher&gt; geTeacherByChoose(Teacher teacher); &#x2F;&#x2F;需求: 带id用id查，不带id用其他属性查 --&gt; &lt;select id&#x3D;&quot;geTeacherByChoose&quot; resultMap&#x3D;&quot;teacherMap&quot;&gt; select * from t_teacher &lt;where&gt; &lt;choose&gt; &lt;when test&#x3D;&quot;id!&#x3D;null&quot;&gt; id&#x3D;#&#123;id&#125; &lt;&#x2F;when&gt; &lt;!-- &quot;&quot; 空串转义符 --&gt; &lt;when test&#x3D;&quot;name!&#x3D;null and !name.equals(&quot;&quot;)&quot;&gt; teacherName&#x3D;#&#123;name&#125; &lt;&#x2F;when&gt; &lt;when test&#x3D;&quot;birth!&#x3D;null&quot;&gt; birth_date&#x3D;#&#123;birth&#125; &lt;&#x2F;when&gt; &lt;!-- 其他情況 --&gt; &lt;otherwise&gt; 1&#x3D;1 &lt;&#x2F;otherwise&gt; &lt;&#x2F;choose&gt; &lt;&#x2F;where&gt; &lt;&#x2F;select&gt;&lt;!-- public Integer updateTeacher(Teacher teacher); &#x2F;&#x2F;&lt;set&gt;标签 动态更新 --&gt; &lt;select id&#x3D;&quot;updateTeacher&quot; resultMap&#x3D;&quot;teacherMap&quot;&gt; UPDATE t_teacher &lt;set&gt; &lt;if test&#x3D;&quot;name!&#x3D;null and !name.equals(&quot;&quot;)&quot;&gt; teacherName&#x3D;#&#123;name&#125;, &lt;&#x2F;if&gt; &lt;if test&#x3D;&quot;course!&#x3D;null and !course.equals(&quot;&quot;)&quot;&gt; class_name&#x3D;#&#123;course&#125;, &lt;&#x2F;if&gt; &lt;if test&#x3D;&quot;address!&#x3D;null and !address.equals(&quot;&quot;)&quot;&gt; address&#x3D;#&#123;address&#125;, &lt;&#x2F;if&gt; &lt;if test&#x3D;&quot;birth!&#x3D;null and !birth.equals(&quot;&quot;)&quot;&gt; birth_date&#x3D;#&#123;birth&#125; &lt;&#x2F;if&gt; &lt;&#x2F;set&gt; &lt;where&gt; id&#x3D;#&#123;id&#125; &lt;&#x2F;where&gt; &lt;&#x2F;select&gt; &lt;&#x2F;mapper&gt; log4j.properties1234567# Global logging configurationlog4j.rootLogger&#x3D;DEBUG, stdout# Console output...log4j.appender.stdout&#x3D;org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout&#x3D;org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern&#x3D;%5p [%t] - %m%n log4j.xml12345678910111213141516171819202122&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE log4j:configuration SYSTEM &quot;log4j.dtd&quot;&gt; &lt;log4j:configuration xmlns:log4j&#x3D;&quot;http:&#x2F;&#x2F;jakarta.apache.org&#x2F;log4j&#x2F;&quot;&gt; &lt;appender name&#x3D;&quot;STDOUT&quot; class&#x3D;&quot;org.apache.log4j.ConsoleAppender&quot;&gt; &lt;param name&#x3D;&quot;Encoding&quot; value&#x3D;&quot;UTF-8&quot; &#x2F;&gt; &lt;layout class&#x3D;&quot;org.apache.log4j.PatternLayout&quot;&gt; &lt;param name&#x3D;&quot;ConversionPattern&quot; value&#x3D;&quot;%-5p %d&#123;MM-dd HH:mm:ss,SSS&#125; %m (%F:%L) \\n&quot; &#x2F;&gt; &lt;&#x2F;layout&gt; &lt;&#x2F;appender&gt; &lt;logger name&#x3D;&quot;java.sql&quot;&gt; &lt;level value&#x3D;&quot;debug&quot; &#x2F;&gt; &lt;&#x2F;logger&gt; &lt;logger name&#x3D;&quot;org.apache.ibatis&quot;&gt; &lt;level value&#x3D;&quot;info&quot; &#x2F;&gt; &lt;&#x2F;logger&gt; &lt;root&gt; &lt;level value&#x3D;&quot;debug&quot; &#x2F;&gt; &lt;appender-ref ref&#x3D;&quot;STDOUT&quot; &#x2F;&gt; &lt;&#x2F;root&gt;&lt;&#x2F;log4j:configuration&gt; mybatis-config.xml123456789101112131415161718192021222324252627282930&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-&#x2F;&#x2F;mybatis.org&#x2F;&#x2F;DTD Config 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;mybatis.org&#x2F;dtd&#x2F;mybatis-3-config.dtd&quot;&gt;&lt;configuration&gt; &lt;settings&gt; &lt;!-- 开启延迟加载开关 --&gt; &lt;setting name&#x3D;&quot;lazyLoadingEnabled&quot; value&#x3D;&quot;true&quot;&#x2F;&gt; &lt;!-- 开启属性按需加载 --&gt; &lt;setting name&#x3D;&quot;aggressiveLazyLoading&quot; value&#x3D;&quot;false&quot;&#x2F;&gt; &lt;&#x2F;settings&gt; &lt;environments default&#x3D;&quot;development&quot;&gt; &lt;environment id&#x3D;&quot;development&quot;&gt; &lt;transactionManager type&#x3D;&quot;JDBC&quot;&#x2F;&gt; &lt;!-- 配置连接池 POOLED: 连接池--&gt; &lt;dataSource type&#x3D;&quot;POOLED&quot;&gt; &lt;!-- $&#123;&#125;取出配置值 --&gt; &lt;property name&#x3D;&quot;driver&quot; value&#x3D;&quot;com.mysql.cj.jdbc.Driver&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;url&quot; value&#x3D;&quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;mybatis_0325?serverTimezone&#x3D;GMT%2B8&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;username&quot; value&#x3D;&quot;root&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;password&quot; value&#x3D;&quot;******&quot;&#x2F;&gt; &lt;&#x2F;dataSource&gt; &lt;&#x2F;environment&gt; &lt;&#x2F;environments&gt; &lt;mappers&gt; &lt;package name&#x3D;&quot;com.lizhi.dao&quot;&#x2F;&gt; &lt;&#x2F;mappers&gt;&lt;&#x2F;configuration&gt; 测试实例MyBatisTest.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144package com.lizhi.test;import java.io.IOException;import java.io.InputStream;import java.util.Arrays;import java.util.Date;import java.util.List;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.jupiter.api.BeforeEach;import org.junit.jupiter.api.Test;import com.lizhi.bean.Teacher;import com.lizhi.dao.TeacherDao;public class MyBatisTest &#123; &#x2F;&#x2F;工厂一个 SqlSessionFactory sqlSessionFactory; @BeforeEach public void initSqlSessionFactory() throws IOException &#123; &#x2F;&#x2F;1.跟据全局配置文件先得到SqlSessionFactory对象(官方文档复制)，快捷键Ctrl+shift+O导包 String resource &#x3D; &quot;mybatis-config.xml&quot;; InputStream inputStream &#x3D; Resources.getResourceAsStream(resource); sqlSessionFactory &#x3D; new SqlSessionFactoryBuilder().build(inputStream); &#125; &#x2F;** * 用id查询老师的信息 *&#x2F; @Test public void test01() throws IOException &#123; &#x2F;&#x2F;initSqlSessionFactory();如果不用@BeforeEach标注每次测试前都要先调用一下这个方法 SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; TeacherDao mapper &#x3D; openSession.getMapper(TeacherDao.class); Teacher teacher &#x3D; mapper.geTeacherById(1); System.out.println(teacher); &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125; &#x2F;** * 测试&lt;if&gt; *&#x2F; @Test public void test02() throws IOException &#123; SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; TeacherDao mapper &#x3D; openSession.getMapper(TeacherDao.class); Teacher teacher &#x3D; new Teacher(); teacher.setId(1); teacher.setName(&quot;%a%&quot;);&#x2F;&#x2F;表示名字字母带有a的名字 teacher.setBirth(new Date()); List&lt;Teacher&gt; list &#x3D; mapper.geTeacherByCondition(teacher); System.out.println(list); &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125; &#x2F;** * 测试&lt;foreach&gt; *&#x2F; @Test public void test03() throws IOException &#123; SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; TeacherDao mapper &#x3D; openSession.getMapper(TeacherDao.class); List&lt;Teacher&gt; list &#x3D; mapper.getTeachersByIdIn(Arrays.asList(1,2,3,4,5,6)); System.out.println(list); &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125; &#x2F;** * 测试&lt;choose&gt;元素分支选择 *&#x2F; @Test public void test04() throws IOException &#123; SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; TeacherDao mapper &#x3D; openSession.getMapper(TeacherDao.class); Teacher teacher &#x3D; new Teacher(); teacher.setId(1); teacher.setName(&quot;LaoCui&quot;);&#x2F;&#x2F;表示名字字母带有a的名字 List&lt;Teacher&gt; list &#x3D; mapper.geTeacherByChoose(teacher); System.out.println(list); &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125; &#x2F;** * 测试&lt;set&gt; *&#x2F; @Test public void test05() throws IOException &#123; SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; TeacherDao mapper &#x3D; openSession.getMapper(TeacherDao.class); Teacher teacher &#x3D; new Teacher(); teacher.setId(1); teacher.setName(&quot;hh&quot;); mapper.updateTeacher(teacher); &#x2F;&#x2F;提交 openSession.commit(); &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125;&#125;","categories":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"-ssm -MyBatis - MyBatis动态sql","slug":"ssm-MyBatis-MyBatis动态sql","permalink":"http://www.studyz.club/tags/ssm-MyBatis-MyBatis%E5%8A%A8%E6%80%81sql/"}]},{"title":"软件开发框架之MyBatis的关联映射","slug":"软件开发框架之Spring-MyBatis的关联映射","date":"2019-11-06T11:37:02.505Z","updated":"2019-11-09T08:13:28.654Z","comments":true,"path":"posts/3879ece7/","link":"","permalink":"http://www.studyz.club/posts/3879ece7/","excerpt":"","text":"一，关联关系概述 二， MyBatis中的一对一关联关系 2.1 级联属性的方式封装查询数据 2.1.1 级联属性的方式封装查询数据测试实例源码 2.2 &lt;association&gt;元素属性 2.3 MyBatis加载关联关系对象的两种方式 2.4 使用select属性指定分步查询 2.5 MyBatis按需加载和延迟加载 2.6 &lt;collection&gt;分布查询延迟加载 三， MyBatis中的一对多关联关系 3.1 一对多关系测试实例源码 四， MyBatis中的多对多关联关系 五， 本章所用测试实例的源码 一，关联关系概述 实际的开发中，对数据库的操作常常会涉及到多张表，这在面向对象中就涉及到了对象与对象之间的关联关系。针对多表之间的操作，MyBatis提供了关联映射，通过关联映射就可以很好的处理对象与对象之间的关联关系。本章中，将对MyBatis的关联关系映射进行详细的讲解。 在关系型数据库中，多表之间存在着三种关联关系，分别为一对一、一对多和多对多，如下图所示 在Java中，通过对象也可以进行关联关系描述，如图下图所示： 二， MyBatis中的一对一关联关系 在现实生活中，一对一关联关系是十分常见的。例如，一个人只能有一个身份证，同时一个身份证也只会对应一个人。 123456789101112&#x2F;&#x2F;1-1关联: 一个key开一把lock &#x2F;&#x2F;1-n关联: 从lock来看key &#x2F;&#x2F;n-1关联: 从key表看lock&#x2F;* * n-n关联: 学生与老师的关系 * t_student表 teacher表 * 1-n: 外键一定放在n的一端（让14亿人记住习大大的名字与让习大大记住14亿人的名字） * n-n: 中间表存储对应关系，存储学生id和老师id * * *&#x2F; 那么使用MyBatis是怎么处理图中的这种一对一关联关系的呢？ &lt;resultMap&gt;元素中，包含了一个&lt;association&gt;子元素，MyBatis就是通过该元素来处理一对一关联关系的。 2.1 级联属性的方式封装查询数据级联属性的方式封装查询数据测试实例源码 键两张表t_key t_lock 并在表t_key里设置外部键lockid 12345678910111213141516171819202122232425 &lt;!-- &#x2F;&#x2F;钥匙的id private Integer id; &#x2F;&#x2F;钥匙名 private String keyName; &#x2F;&#x2F;当前钥匙能开那把锁 private Lock lock; 查询出来的结果列名 id keyname lockid lid lockName 注意：这里的&#96;不是单引号而是ESC键下面的那个 --&gt; &lt;select id&#x3D;&quot;getKeyById&quot; resultMap&#x3D;&quot;mykey&quot;&gt; SELECT k.&#96;id&#96;, k.&#96;keyname&#96;, l.&#96;id&#96; lid, l.&#96;lockName&#96; FROM t_key k LEFT JOIN t_lock l ON k.&#96;lockid&#96; &#x3D; l.&#96;id&#96; WHERE k.&#96;id&#96; &#x3D;#&#123;id&#125; &lt;&#x2F;select&gt;&lt;!-- 自定义封装规则,使用级联属性封装查询出来的结果 --&gt; &lt;resultMap type&#x3D;&quot;com.lizhi.bean.Key&quot; id&#x3D;&quot;mykey&quot;&gt; &lt;id property&#x3D;&quot;id&quot; column&#x3D;&quot;id&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;keyName&quot; column&#x3D;&quot;keyname&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;lock.id&quot; column&#x3D;&quot;lid&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;lock.lockName&quot; column&#x3D;&quot;lockName&quot;&#x2F;&gt; &lt;&#x2F;resultMap&gt; 12345678910111213141516171819&#x2F;* * 联合查询情况下 * 1.使用级联属性封装联合查询后的所有结果 *&#x2F; @Test public void test01() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; KeyDao mapper &#x3D; openSession.getMapper(KeyDao.class); Key KeyById &#x3D; mapper.getKeyById(1); System.out.println(KeyById); &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125; 输出结果 12345DEBUG 11-07 07:53:08,387 &#x3D;&#x3D;&gt; Preparing: SELECT k.&#96;id&#96;, k.&#96;keyname&#96;, l.&#96;id&#96; lid, l.&#96;lockName&#96; FROM t_key k LEFT JOIN t_lock l ON k.&#96;lockid&#96; &#x3D; l.&#96;id&#96; WHERE k.&#96;id&#96; &#x3D;? (BaseJdbcLogger.java:143) DEBUG 11-07 07:53:08,415 &#x3D;&#x3D;&gt; Parameters: 2(Integer) (BaseJdbcLogger.java:143) DEBUG 11-07 07:53:08,435 &lt;&#x3D;&#x3D; Total: 1 (BaseJdbcLogger.java:143) Key [id&#x3D;2, keyName&#x3D;2号钥匙, lock&#x3D;Lock [id&#x3D;2, lockName&#x3D;二号锁]] 2.2 元素属性 property:指定映射到的实体类对象属性，与表字段一一对应 column:指定表中对应的字段 javaType:指定映射到实体对象属性的类型 select:指定引入嵌套查询的子SQL语句，该属性用于关联映射中的嵌套查询 fetchType:指定在关联查询时是否启用延迟加载。该属性有lazy和eager两个属性值，默认值为lazy（即默认关联映射延迟加载） KeyDao.xml12345678910111213141516171819202122232425262728293031323334&lt;!-- public Key getKeyById(Integer id); --&gt; &lt;!-- &#x2F;&#x2F;钥匙的id private Integer id; &#x2F;&#x2F;钥匙名 private String keyName; &#x2F;&#x2F;当前钥匙能开那把锁 private Lock lock; 查询出来的结果列名 id keyname lockid lid lockName 注意：这里的&#96;不是单引号而是ESC键下面的那个 --&gt; &lt;select id&#x3D;&quot;getKeyById&quot; resultMap&#x3D;&quot;mykey&quot;&gt; SELECT k.&#96;id&#96;, k.&#96;keyname&#96;, l.&#96;id&#96; lid, l.&#96;lockName&#96; FROM t_key k LEFT JOIN t_lock l ON k.&#96;lockid&#96; &#x3D; l.&#96;id&#96; WHERE k.&#96;id&#96; &#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; &lt;!-- 如果是一个复杂的类型关联，使用MyBatis推荐的association，嵌入结果映射 --&gt; &lt;resultMap type&#x3D;&quot;com.lizhi.bean.Key&quot; id&#x3D;&quot;mykey&quot;&gt; &lt;id property&#x3D;&quot;id&quot; column&#x3D;&quot;id&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;keyName&quot; column&#x3D;&quot;keyname&quot;&#x2F;&gt; &lt;!-- 接下来的属性是一个对象，自定义这个对象的封装规则，使用association：表示联合了一个对象 javaType指定属性的类名 --&gt; &lt;association property&#x3D;&quot;lock&quot; javaType&#x3D;&quot;com.lizhi.bean.Lock&quot;&gt; &lt;!-- 定义lock属性对应的Lock对象如何封装 --&gt; &lt;id property&#x3D;&quot;id&quot; column&#x3D;&quot;lid&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;lockName&quot; column&#x3D;&quot;lockName&quot;&#x2F;&gt; &lt;&#x2F;association&gt; &lt;&#x2F;resultMap&gt; 2.3 MyBatis加载关联关系对象的两种方式 MyBatis加载关联关系对象主要通过两种方式:嵌套查询和嵌套结果。 2.4 使用select属性指定分步查询目标: 在查询钥匙的时候顺便把锁查出来在LockDao.java中添加方法12&#x2F;&#x2F;按照lock的id找锁的简单方法,,测试分步查询 public Lock getLockByIdSimple(Integer id); 在KeyDao.java中添加方法12&#x2F;&#x2F;按照key的id找锁的简单方法,,测试分步查询 public Key getKeyByIdSimple(Integer id); 现在LockDao.xml中添加sql查询方法123456&lt;!-- &#x2F;&#x2F;按照lock的id找锁的简单方法,,测试分步查询 public Lock getLockByIdSimple(Integer id); --&gt; &lt;select id&#x3D;&quot;getLockByIdSimple&quot; resultType&#x3D;&quot;com.lizhi.bean.Lock&quot;&gt; SELECT * FROM t_lock WHERE id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; 再在KeyDao.xml中添加sql查询方法，并用select指定调用哪个sql语句查询锁12345678910111213141516171819202122232425&lt;!-- &#x2F;&#x2F;按照key的id找锁的简单方法,,测试分步查询 public Key getKeyByIdSimple(Integer id); --&gt; &lt;!-- 查询key的时候也可以带上锁子信息 --&gt; &lt;select id&#x3D;&quot;getKeyByIdSimple&quot; resultMap&#x3D;&quot;myKey02&quot;&gt; SELECT * FROM t_key WHERE id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; &lt;!-- 封装这些属性 private Integer id; &#x2F;&#x2F;钥匙的id private String keyName;&#x2F;&#x2F;钥匙名 private Lock lock; &#x2F;&#x2F;当前钥匙能开那把锁--&gt; &lt;resultMap type&#x3D;&quot;com.lizhi.bean.Key&quot; id&#x3D;&quot;myKey02&quot;&gt; &lt;id property&#x3D;&quot;id&quot; column&#x3D;&quot;id&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;keyName&quot; column&#x3D;&quot;keyName&quot;&#x2F;&gt; &lt;!-- 告诉MyBatis自己去调用一个查询查锁 select&#x3D;&quot;&quot; : 指定一个查询sql的唯一表示即id,MyBatis自动调用指定的sql将查出的lock封装起来，一般会加上sql所在配置文件的命名空间名称 public Lock getLockByIdSimple(Integer id);此时这个锁的方法需要传入一个参数id 告诉MyBatis把那一列的值传递过去 column&#x3D;&quot;&quot; : 指定将那一列的数据传递过去；；lockid是在t_key表里记录了钥匙与锁的对应关系 --&gt; &lt;association property&#x3D;&quot;lock&quot; select&#x3D;&quot;com.lizhi.dao.LockDao.getLockByIdSimple&quot; column&#x3D;&quot;lockid&quot;&gt; &lt;&#x2F;association&gt; &lt;&#x2F;resultMap&gt; 运行结果 有运行结果可以看到调用了两个sql语句，先查询了key的id，然后有根据t_key表中的lockid去调用查询锁的sql。 1234567DEBUG 11-07 14:46:47,874 &#x3D;&#x3D;&gt; Preparing: SELECT * FROM t_key WHERE id&#x3D;? (BaseJdbcLogger.java:143) DEBUG 11-07 14:46:47,904 &#x3D;&#x3D;&gt; Parameters: 1(Integer) (BaseJdbcLogger.java:143) DEBUG 11-07 14:46:47,926 &#x3D;&#x3D;&#x3D;&#x3D;&gt; Preparing: SELECT * FROM t_lock WHERE id&#x3D;? (BaseJdbcLogger.java:143) DEBUG 11-07 14:46:47,926 &#x3D;&#x3D;&#x3D;&#x3D;&gt; Parameters: 1(Integer) (BaseJdbcLogger.java:143) DEBUG 11-07 14:46:47,928 &lt;&#x3D;&#x3D;&#x3D;&#x3D; Total: 1 (BaseJdbcLogger.java:143) DEBUG 11-07 14:46:47,930 &lt;&#x3D;&#x3D; Total: 1 (BaseJdbcLogger.java:143) Key [id&#x3D;1, keyName&#x3D;1号钥匙, lock&#x3D;Lock [id&#x3D;1, lockName&#x3D;一号锁, keys&#x3D;null]] 虽然使用嵌套查询的方式比较简单，但是嵌套查询的方式要执行多条SQL语句，这对于大型数据集合和列表展示不是很好，因为这样可能会导致成百上千条关联的SQL语句被执行，从而极大的消耗数据库性能并且会降低查询效率。 解决办法2.5 MyBatis按需加载和延迟加载的配置 使用MyBatis的延迟加载在一定程度上可以降低运行消耗并提高查询效率。MyBatis默认没有开启延迟加载，需要在核心配置文件中的&lt;settings&gt;元素内进行配置，具体配置方式如下：123456&lt;settings&gt; &lt;!-- 开启延迟加载开关 --&gt; &lt;setting name&#x3D;&quot;lazyLoadingEnabled&quot; value&#x3D;&quot;true&quot;&#x2F;&gt; &lt;!-- 开启属性按需加载 --&gt; &lt;setting name&#x3D;&quot;aggressiveLazyLoading&quot; value&#x3D;&quot;false&quot;&#x2F;&gt; &lt;&#x2F;settings&gt; 就上述例子而言如果在MyBatis-config.xml中配置了按需加载和延迟加载。1234567891011121314151617181920212223242526272829@Testpublic void test03() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; KeyDao mapper &#x3D; openSession.getMapper(KeyDao.class); &#x2F;&#x2F; Key key &#x3D; mapper.getKeyByIdSimple(1); &#x2F;&#x2F;如果只是查钥匙名，但每次都会加载锁表的全部信息，造成了性能的浪费 &#x2F;&#x2F;开启按需加载和延迟加载后会发现只调用了一个sql语句加载用来获取钥匙名 &#x2F;&#x2F;按需加载：需要的时候再去查询；全局开启按需加载策略 &#x2F;&#x2F;延迟加载： 不着急加载（查询对象） System.out.println(key.getKeyName()); Thread.sleep(3000); String lockName &#x3D; key.getLock().getLockName(); System.out.println(lockName); &#125; catch (InterruptedException e) &#123; &#x2F;&#x2F; TODO Auto-generated catch block e.printStackTrace(); &#125; finally &#123; openSession.close(); &#125; &#125; 运行结果,我们会发现只调用了一个sql语句而没有加载嵌套的sql语句。加Tread是为了模拟两次加载属性123456789DEBUG 11-07 19:15:04,281 &#x3D;&#x3D;&gt; Preparing: SELECT * FROM t_key WHERE id&#x3D;? (BaseJdbcLogger.java:143) DEBUG 11-07 19:15:04,337 &#x3D;&#x3D;&gt; Parameters: 1(Integer) (BaseJdbcLogger.java:143) DEBUG 11-07 19:15:04,451 &lt;&#x3D;&#x3D; Total: 1 (BaseJdbcLogger.java:143) 1号钥匙DEBUG 11-07 19:15:07,455 &#x3D;&#x3D;&gt; Preparing: SELECT * FROM t_lock WHERE id&#x3D;? (BaseJdbcLogger.java:143) DEBUG 11-07 19:15:07,456 &#x3D;&#x3D;&gt; Parameters: 1(Integer) (BaseJdbcLogger.java:143) DEBUG 11-07 19:15:07,462 &lt;&#x3D;&#x3D; Total: 1 (BaseJdbcLogger.java:143) 一号锁 也可以在&lt;association&gt;中覆盖延迟加载和按需加载一次加载全部属性12345&lt;association property&#x3D;&quot;lock&quot; &lt;!-- 在开启延迟加载和按需加载后可以通过fetchType&#x3D;&quot;&quot;来配置是否延迟加载fetchType&#x3D;&quot;lazy&quot;是延迟加载 eager是不延迟加载--&gt; select&#x3D;&quot;com.lizhi.dao.LockDao.getLockByIdSimple&quot; column&#x3D;&quot;lockid&quot; fetchType&#x3D;&quot;eager&quot;&gt; &lt;&#x2F;association&gt; 在映射文件中，&lt;association&gt;元素和元素中都已默认配置了延迟加载属性，即默认属性fetchType=”lazy”（属性fetchType=”eager”表示立即加载），所以在配置文件中开启延迟加载后，无需在映射文件中再做配置。 使用元素进行一对一关联映射非常简单，只需要参考如下两种示例配置即可。 2.6 &lt;collection&gt;分布查询延迟加载多对一用锁去找钥匙 不常用，推荐使用连接查询的方式 在LockDao.java中添加方法12&#x2F;&#x2F;分步查询 public Lock getLockByIdStep(Integer id); 在KeyDao.java中添加方法12&#x2F;&#x2F;查出所有keypublic List&lt;Key&gt; getKeysByLockId(Integer id); 在KeyDao.xml中配置1234567 &lt;!-- &#x2F;&#x2F;按照锁的id查出所有key public List&lt;Key&gt; getKeysByLockId(Integer id); --&gt; &lt;!-- 测试collection分步查询的延迟加载 --&gt; &lt;select id&#x3D;&quot;getKeysByLockId&quot; resultType&#x3D;&quot;com.lizhi.bean.Key&quot;&gt; select * from t_key where lockid&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; 在LockDao.xml中配置1234567891011121314&lt;!-- 测试collection分步查询的延迟加载 --&gt; &lt;!-- public Lock getLockByIdStep(Integer id); --&gt; &lt;select id&#x3D;&quot;getLockByIdStep&quot; resultMap&#x3D;&quot;myLockStep&quot;&gt; SELECT * FROM t_lock WHERE id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; &lt;!-- collection分步查询 --&gt; &lt;resultMap type&#x3D;&quot;com.lizhi.bean.Lock&quot; id&#x3D;&quot;myLockStep&quot;&gt; &lt;id property&#x3D;&quot;id&quot; column&#x3D;&quot;id&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;lockName&quot; column&#x3D;&quot;lockName&quot;&#x2F;&gt; &lt;!-- collection : 指定集合类型的属性封装规则 --&gt; &lt;collection property&#x3D;&quot;keys&quot; select&#x3D;&quot;com.lizhi.dao.KeyDao.getKeysByLockId&quot; column&#x3D;&quot;id&quot;&gt;&lt;&#x2F;collection&gt; &lt;&#x2F;resultMap&gt; 测试12345678910111213141516171819202122232425262728&#x2F;** * 测试collection分步查询的延迟加载 * * 一般在工作的时候，写成两个方法 * public Key getKeyByIdSimple(Integer id);&#x2F;&#x2F;按照key的id找锁的简单方法,,测试分步查询 * *推荐都来写链接查询（left join） *public Key getKeyAssicate() *&#x2F;@Testpublic void test04() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; LockDao mapper &#x3D; openSession.getMapper(LockDao.class); Lock lock &#x3D; mapper.getLockByIdStep(3); System.out.println(lock.getLockName()); List&lt;Key&gt; keys &#x3D; lock.getKeys(); for (Key key : keys) &#123; System.out.println(key.getKeyName()); &#125; &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125; 运行结果12345678910DEBUG 11-07 20:39:39,989 &#x3D;&#x3D;&gt; Preparing: SELECT * FROM t_lock WHERE id&#x3D;? (BaseJdbcLogger.java:143) DEBUG 11-07 20:39:40,019 &#x3D;&#x3D;&gt; Parameters: 3(Integer) (BaseJdbcLogger.java:143) DEBUG 11-07 20:39:40,081 &lt;&#x3D;&#x3D; Total: 1 (BaseJdbcLogger.java:143) 226的锁DEBUG 11-07 20:39:40,083 &#x3D;&#x3D;&gt; Preparing: select * from t_key where lockid&#x3D;? (BaseJdbcLogger.java:143) DEBUG 11-07 20:39:40,084 &#x3D;&#x3D;&gt; Parameters: 3(Integer) (BaseJdbcLogger.java:143) DEBUG 11-07 20:39:40,114 &lt;&#x3D;&#x3D; Total: 3 (BaseJdbcLogger.java:143) 226钥匙1226钥匙2226钥匙3 三， MyBatis中的一对多关联关系一对多关系测试实例源码 开发人员接触更多的关联关系是一对多（或多对一）。例如，一个用户可以有多个订单，同时多个订单归一个用户所有。 那么使用MyBatis是怎么处理这种一对多关联关系的呢？ &lt;resultMap&gt;元素中，包含了一个&lt;collection&gt;子元素，MyBatis就是通过该元素来处理一对多关联关系的。 &lt;collection&gt;子元素的属性大部分与&lt;association&gt;元素相同，但其还包含一个特殊属性–ofType 。 ofType : ofType属性与javaType属性对应，它用于指定实体对象中集合类属性所包含的元素类型。 元素的使用也非常简单，同样可以参考如下两种示例进行配置，具体代码如下: 123456789101112131415161718192021222324252627282930313233 &lt;!-- public Lock getLockById(Integer id); --&gt;&lt;select id&#x3D;&quot;getLockById&quot; resultMap&#x3D;&quot;myLock&quot;&gt; SELECT l.* ,k.id kid, k.&#96;keyname&#96;,k.&#96;lockid&#96; FROM t_lock l LEFT JOIN t_key k ON k.&#96;lockid&#96; &#x3D; l.&#96;id&#96; WHERE l.&#96;id&#96; &#x3D;#&#123;id&#125;&lt;&#x2F;select&gt;&lt;!-- 自定义封装规则 &#x2F;&#x2F;锁的编号 private Integer id; &#x2F;&#x2F;锁的名字 private String lockName; &#x2F;&#x2F;查锁对应的所有钥匙 private List&lt;Key&gt; keys; --&gt;&lt;resultMap type&#x3D;&quot;com.lizhi.bean.Lock&quot; id&#x3D;&quot;myLock&quot;&gt; &lt;id property&#x3D;&quot;id&quot; column&#x3D;&quot;id&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;lockName&quot; column&#x3D;&quot;lockName&quot;&#x2F;&gt; &lt;!-- collection : 定义集合元素封装 property&#x3D;&quot;&quot; : 指定哪个属性是集合属性 javaType : 指定对象类型，只在association标签中使用 ofType&#x3D;&quot;&quot; : 指定集合里的数据类型 --&gt; &lt;collection property&#x3D;&quot;keys&quot; ofType&#x3D;&quot;com.lizhi.bean.Key&quot;&gt; &lt;!-- 在这个标签体中指定集合中这个元素的封装规则 --&gt; &lt;id property&#x3D;&quot;id&quot; column&#x3D;&quot;kid&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;keyName&quot; column&#x3D;&quot;keyname&quot;&#x2F;&gt; &lt;&#x2F;collection&gt;&lt;&#x2F;resultMap&gt; 测试实例1234567891011121314151617@Testpublic void test02() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; LockDao mapper &#x3D; openSession.getMapper(LockDao.class); Lock lock &#x3D; mapper.getLockById(3); System.out.println(lock); System.out.println(&quot;所有锁如下：&quot;); List&lt;Key&gt; keys &#x3D; lock.getKeys(); for (Key key : keys) &#123; System.out.println(key); &#125; &#125; finally &#123; openSession.close(); &#125; &#125; 运行结果123456Lock [id&#x3D;3, lockName&#x3D;226的锁, keys&#x3D;[Key [id&#x3D;3, keyName&#x3D;226钥匙1, lock&#x3D;null], Key [id&#x3D;4, keyName&#x3D;226钥匙2, lock&#x3D;null], Key [id&#x3D;5, keyName&#x3D;226钥匙3, lock&#x3D;null]]]所有锁如下：Key [id&#x3D;3, keyName&#x3D;226钥匙1, lock&#x3D;null]Key [id&#x3D;4, keyName&#x3D;226钥匙2, lock&#x3D;null]Key [id&#x3D;5, keyName&#x3D;226钥匙3, lock&#x3D;null] 四, MyBatis中的多对多关联关系 在实际项目开发中，多对多的关联关系也是非常常见的。以订单和商品为例，一个订单可以包含多种商品，而一种商品又可以属于多个订单。 在数据库中，多对多的关联关系通常使用一个中间表来维护，中间表中的订单id作为外键参照订单表的id，商品id作为外键参照商品表的id。 在MyBatis中，多对多的关联关系查询，同样可以使用前面介绍的&lt;collection&gt;元素进行处理（其用法和一对多关联关系查询语句用法基本相同）。 源码2.1.1 级联属性的方式封装查询数据测试实例源码 连接数据库的配置文件和日志文件在下面最后一个实例源码，可以将里面的按需加载和延迟加载删掉，没用到package com.lizhi.dao;interface KeyDao12345678910package com.lizhi.dao;import com.lizhi.bean.Key;public interface KeyDao &#123; &#x2F;&#x2F;将钥匙和锁的信息一起查出 public Key getKeyById(Integer id);&#125; package com.lizhi.bean;class Key12345678910111213141516171819202122232425262728293031323334353637383940package com.lizhi.bean;&#x2F;* * 钥匙 * *&#x2F;public class Key &#123; &#x2F;&#x2F;钥匙的id private Integer id; &#x2F;&#x2F;钥匙名 private String keyName; &#x2F;&#x2F;当前钥匙能开那把锁 private Lock lock; public Lock getLock() &#123; return lock; &#125; public void setLock(Lock lock) &#123; this.lock &#x3D; lock; &#125; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id &#x3D; id; &#125; public String getKeyName() &#123; return keyName; &#125; public void setKeyName(String keyName) &#123; this.keyName &#x3D; keyName; &#125; @Override public String toString() &#123; return &quot;Key [id&#x3D;&quot; + id + &quot;, keyName&#x3D;&quot; + keyName + &quot;, lock&#x3D;&quot; + lock + &quot;]&quot;; &#125; &#125; class Lock123456789101112131415161718192021222324252627282930package com.lizhi.bean;&#x2F;* * 锁子表 *&#x2F;public class Lock &#123; &#x2F;&#x2F;锁的编号 private Integer id; &#x2F;&#x2F;锁的名字 private String lockName; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id &#x3D; id; &#125; public String getLockName() &#123; return lockName; &#125; public void setLockName(String lockName) &#123; this.lockName &#x3D; lockName; &#125; @Override public String toString() &#123; return &quot;Lock [id&#x3D;&quot; + id + &quot;, lockName&#x3D;&quot; + lockName + &quot;]&quot;; &#125; &#125; KeyDao.xml12345678910111213141516171819202122232425262728293031323334&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-&#x2F;&#x2F;mybatis.org&#x2F;&#x2F;DTD Mapper 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;mybatis.org&#x2F;dtd&#x2F;mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace&#x3D;&quot;com.lizhi.dao.KeyDao&quot;&gt; &lt;!-- public Key getKeyById(Integer id); --&gt; &lt;!-- &#x2F;&#x2F;钥匙的id private Integer id; &#x2F;&#x2F;钥匙名 private String keyName; &#x2F;&#x2F;当前钥匙能开那把锁 private Lock lock; 查询出来的结果列名 id keyname lockid lid lockName 注意：这里的&#96;不是单引号而是ESC键下面的那个 --&gt; &lt;select id&#x3D;&quot;getKeyById&quot; resultMap&#x3D;&quot;mykey&quot;&gt; SELECT k.&#96;id&#96;, k.&#96;keyname&#96;, l.&#96;id&#96; lid, l.&#96;lockName&#96; FROM t_key k LEFT JOIN t_lock l ON k.&#96;lockid&#96; &#x3D; l.&#96;id&#96; WHERE k.&#96;id&#96; &#x3D;#&#123;id&#125; &lt;&#x2F;select&gt;&lt;!-- 自定义封装规则,使用级联属性封装查询出来的结果 --&gt; &lt;resultMap type&#x3D;&quot;com.lizhi.bean.Key&quot; id&#x3D;&quot;mykey&quot;&gt; &lt;id property&#x3D;&quot;id&quot; column&#x3D;&quot;id&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;keyName&quot; column&#x3D;&quot;keyname&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;lock.id&quot; column&#x3D;&quot;lid&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;lock.lockName&quot; column&#x3D;&quot;lockName&quot;&#x2F;&gt; &lt;&#x2F;resultMap&gt; &lt;&#x2F;mapper&gt; class MyBatis_Key_Test1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.lizhi.test;import java.io.IOException;import java.io.InputStream;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.Before;import org.junit.jupiter.api.Test;import com.lizhi.bean.Cat;import com.lizhi.bean.Key;import com.lizhi.dao.CatDao;import com.lizhi.dao.KeyDao;&#x2F;** * 联合查询测试 * @author Administrator * *&#x2F;public class MyBatis_Key_Test &#123; &#x2F;&#x2F;工厂一个 SqlSessionFactory sqlSessionFactory; @Before public void initSqlSessionFactory() throws IOException &#123; &#x2F;&#x2F;1.跟据全局配置文件先得到SqlSessionFactory对象(官方文档复制)，快捷键Ctrl+shift+O导包 String resource &#x3D; &quot;mybatis-config.xml&quot;; InputStream inputStream &#x3D; Resources.getResourceAsStream(resource); sqlSessionFactory &#x3D; new SqlSessionFactoryBuilder().build(inputStream); &#125; &#x2F;* * 联合查询情况下 * 1.使用级联属性封装联合查询后的所有结果 *&#x2F; @Test public void test01() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; KeyDao mapper &#x3D; openSession.getMapper(KeyDao.class); Key KeyById &#x3D; mapper.getKeyById(1); System.out.println(KeyById); &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125;&#125; 3.1 一对多关系测试实例源码对t_key表进行更改 对t_lock表进行更改 连接数据库的配置文件和日志文件在下面最后一个实例源码，可以将里面的按需加载和延迟加载删掉，没用到package com.lizhi.dao;interface KeyDaointerface LockDao1234567891011package com.lizhi.dao;import com.lizhi.bean.Lock;public interface LockDao &#123; &#x2F;&#x2F;查锁的时候将所有的钥匙也查询出来 public Lock getLockById(Integer id); &#125; package com.lizhi.bean;class Keyclass Lock123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.lizhi.bean;import java.util.List;&#x2F;* * 锁子表 *&#x2F;public class Lock &#123; &#x2F;&#x2F;锁的编号 private Integer id; &#x2F;&#x2F;锁的名字 private String lockName; &#x2F;&#x2F;查锁对应的所有钥匙 private List&lt;Key&gt; keys; &#x2F;&#x2F;1-1关联: 一个key开一把lock &#x2F;&#x2F;1-n关联: 从lock来看key &#x2F;&#x2F;n-1关联: 从key表看lock &#x2F;* * n-n关联: 学生与老师的关系 * t_student表 teacher表 * 1-n: 外键一定放在n的一端（让14亿人记住习大大的名字与让习大大记住14亿人的名字） * n-n: 中间表存储对应关系，存储学生id和老师id *&#x2F; public List&lt;Key&gt; getKeys() &#123; return keys; &#125; public void setKeys(List&lt;Key&gt; keys) &#123; this.keys &#x3D; keys; &#125; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id &#x3D; id; &#125; public String getLockName() &#123; return lockName; &#125; public void setLockName(String lockName) &#123; this.lockName &#x3D; lockName; &#125; @Override public String toString() &#123; return &quot;Lock [id&#x3D;&quot; + id + &quot;, lockName&#x3D;&quot; + lockName + &quot;, keys&#x3D;&quot; + keys + &quot;]&quot;; &#125; &#125; LockDao.xml1234567891011121314151617181920212223242526272829303132333435363738394041&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-&#x2F;&#x2F;mybatis.org&#x2F;&#x2F;DTD Mapper 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;mybatis.org&#x2F;dtd&#x2F;mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace&#x3D;&quot;com.lizhi.dao.LockDao&quot;&gt; &lt;!-- public Lock getLockById(Integer id); --&gt; &lt;select id&#x3D;&quot;getLockById&quot; resultMap&#x3D;&quot;myLock&quot;&gt; SELECT l.* ,k.id kid, k.&#96;keyname&#96;,k.&#96;lockid&#96; FROM t_lock l LEFT JOIN t_key k ON k.&#96;lockid&#96; &#x3D; l.&#96;id&#96; WHERE l.&#96;id&#96; &#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; &lt;!-- 自定义封装规则 &#x2F;&#x2F;锁的编号 private Integer id; &#x2F;&#x2F;锁的名字 private String lockName; &#x2F;&#x2F;查锁对应的所有钥匙 private List&lt;Key&gt; keys; --&gt; &lt;resultMap type&#x3D;&quot;com.lizhi.bean.Lock&quot; id&#x3D;&quot;myLock&quot;&gt; &lt;id property&#x3D;&quot;id&quot; column&#x3D;&quot;id&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;lockName&quot; column&#x3D;&quot;lockName&quot;&#x2F;&gt; &lt;!-- collection : 定义集合元素封装 property&#x3D;&quot;&quot; : 指定哪个属性是集合属性 javaType : 指定对象类型，只在association标签中使用 ofType&#x3D;&quot;&quot; : 指定集合里的数据类型 --&gt; &lt;collection property&#x3D;&quot;keys&quot; ofType&#x3D;&quot;com.lizhi.bean.Key&quot;&gt; &lt;!-- 在这个标签体中指定集合中这个元素的封装规则 --&gt; &lt;id property&#x3D;&quot;id&quot; column&#x3D;&quot;kid&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;keyName&quot; column&#x3D;&quot;keyname&quot;&#x2F;&gt; &lt;&#x2F;collection&gt; &lt;&#x2F;resultMap&gt; &lt;&#x2F;mapper&gt; MyBatis_Key_Test.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.lizhi.test;import java.io.IOException;import java.io.InputStream;import java.util.List;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.Before;import org.junit.jupiter.api.Test;import com.lizhi.bean.Cat;import com.lizhi.bean.Key;import com.lizhi.bean.Lock;import com.lizhi.dao.CatDao;import com.lizhi.dao.KeyDao;import com.lizhi.dao.LockDao;&#x2F;** * 联合查询测试 * @author Administrator * *&#x2F;public class MyBatis_Key_Test &#123; &#x2F;&#x2F;工厂一个 SqlSessionFactory sqlSessionFactory; @Before public void initSqlSessionFactory() throws IOException &#123; &#x2F;&#x2F;1.跟据全局配置文件先得到SqlSessionFactory对象(官方文档复制)，快捷键Ctrl+shift+O导包 String resource &#x3D; &quot;mybatis-config.xml&quot;; InputStream inputStream &#x3D; Resources.getResourceAsStream(resource); sqlSessionFactory &#x3D; new SqlSessionFactoryBuilder().build(inputStream); &#125; &#x2F;** * 1-n 查锁的时候将所有的钥匙也查询出来 * @throws IOException *&#x2F; @Test public void test02() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; LockDao mapper &#x3D; openSession.getMapper(LockDao.class); Lock lock &#x3D; mapper.getLockById(3); System.out.println(lock); System.out.println(&quot;所有锁如下：&quot;); List&lt;Key&gt; keys &#x3D; lock.getKeys(); for (Key key : keys) &#123; System.out.println(key); &#125; &#125; finally &#123; openSession.close(); &#125; &#125; &#125; 运行结果12345678DEBUG 11-07 13:48:44,901 &#x3D;&#x3D;&gt; Preparing: SELECT l.* ,k.id kid, k.&#96;keyname&#96;,k.&#96;lockid&#96; FROM t_lock l LEFT JOIN t_key k ON k.&#96;lockid&#96; &#x3D; l.&#96;id&#96; WHERE l.&#96;id&#96; &#x3D;? (BaseJdbcLogger.java:143) DEBUG 11-07 13:48:44,932 &#x3D;&#x3D;&gt; Parameters: 3(Integer) (BaseJdbcLogger.java:143) DEBUG 11-07 13:48:44,955 &lt;&#x3D;&#x3D; Total: 3 (BaseJdbcLogger.java:143) Lock [id&#x3D;3, lockName&#x3D;226的锁, keys&#x3D;[Key [id&#x3D;3, keyName&#x3D;226钥匙1, lock&#x3D;null], Key [id&#x3D;4, keyName&#x3D;226钥匙2, lock&#x3D;null], Key [id&#x3D;5, keyName&#x3D;226钥匙3, lock&#x3D;null]]]所有锁如下：Key [id&#x3D;3, keyName&#x3D;226钥匙1, lock&#x3D;null]Key [id&#x3D;4, keyName&#x3D;226钥匙2, lock&#x3D;null]Key [id&#x3D;5, keyName&#x3D;226钥匙3, lock&#x3D;null] 五， 本章所用测试实例的源码 package com.lizhi.bean;class Keyclass Lockpackage com.lizhi.dao;interface KeyDao123456789101112131415161718package com.lizhi.dao;import java.util.List;import com.lizhi.bean.Key;public interface KeyDao &#123; &#x2F;&#x2F;将钥匙和锁的信息一起查出 public Key getKeyById(Integer id); &#x2F;&#x2F;按照key的id找锁的简单方法,,测试分步查询 public Key getKeyByIdSimple(Integer id); &#x2F;&#x2F;查出所有key public List&lt;Key&gt; getKeysByLockId(Integer id);&#125; interface LockDao12345678910111213141516package com.lizhi.dao;import com.lizhi.bean.Lock;public interface LockDao &#123; &#x2F;&#x2F;查锁的时候将所有的钥匙也查询出来 public Lock getLockById(Integer id); &#x2F;&#x2F;按照lock的id找锁的简单方法,,测试分步查询 public Lock getLockByIdSimple(Integer id); &#x2F;&#x2F;分步查询 public Lock getLockByIdStep(Integer id);&#125; KeyDao.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-&#x2F;&#x2F;mybatis.org&#x2F;&#x2F;DTD Mapper 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;mybatis.org&#x2F;dtd&#x2F;mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace&#x3D;&quot;com.lizhi.dao.KeyDao&quot;&gt; &lt;!-- public Key getKeyById(Integer id); --&gt; &lt;!-- &#x2F;&#x2F;钥匙的id private Integer id; &#x2F;&#x2F;钥匙名 private String keyName; &#x2F;&#x2F;当前钥匙能开那把锁 private Lock lock; 查询出来的结果列名 id keyname lockid lid lockName 注意：这里的&#96;不是单引号而是ESC键下面的那个 --&gt; &lt;select id&#x3D;&quot;getKeyById&quot; resultMap&#x3D;&quot;mykey&quot;&gt; SELECT k.&#96;id&#96;, k.&#96;keyname&#96;, l.&#96;id&#96; lid, l.&#96;lockName&#96; FROM t_key k LEFT JOIN t_lock l ON k.&#96;lockid&#96; &#x3D; l.&#96;id&#96; WHERE k.&#96;id&#96; &#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; &lt;!-- 自定义封装规则,使用级联属性封装查询出来的结果 --&gt;&lt;!-- 这是级联属性的可以注释测试association &lt;resultMap type&#x3D;&quot;com.lizhi.bean.Key&quot; id&#x3D;&quot;mykey&quot;&gt; &lt;id property&#x3D;&quot;id&quot; column&#x3D;&quot;id&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;keyName&quot; column&#x3D;&quot;keyname&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;lock.id&quot; column&#x3D;&quot;lid&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;lock.lockName&quot; column&#x3D;&quot;lockName&quot;&#x2F;&gt; &lt;&#x2F;resultMap&gt; --&gt; &lt;!-- 如果是一个复杂的类型关联，使用MyBatis推荐的association，嵌入结果映射 --&gt; &lt;resultMap type&#x3D;&quot;com.lizhi.bean.Key&quot; id&#x3D;&quot;mykey&quot;&gt; &lt;id property&#x3D;&quot;id&quot; column&#x3D;&quot;id&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;keyName&quot; column&#x3D;&quot;keyname&quot;&#x2F;&gt; &lt;!-- 接下来的属性是一个对象，自定义这个对象的封装规则，使用association：表示联合了一个对象 javaType指定属性的类名 --&gt; &lt;association property&#x3D;&quot;lock&quot; javaType&#x3D;&quot;com.lizhi.bean.Lock&quot;&gt; &lt;!-- 定义lock属性对应的Lock对象如何封装 --&gt; &lt;id property&#x3D;&quot;id&quot; column&#x3D;&quot;lid&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;lockName&quot; column&#x3D;&quot;lockName&quot;&#x2F;&gt; &lt;&#x2F;association&gt; &lt;&#x2F;resultMap&gt; &lt;!-- &#x2F;&#x2F;按照key的id找锁的简单方法,,测试分步查询 public Key getKeyByIdSimple(Integer id); --&gt; &lt;!-- 查询key的时候也可以带上锁子信息 --&gt; &lt;select id&#x3D;&quot;getKeyByIdSimple&quot; resultMap&#x3D;&quot;myKey02&quot;&gt; SELECT * FROM t_key WHERE id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; &lt;!-- 封装这些属性 private Integer id; &#x2F;&#x2F;钥匙的id private String keyName;&#x2F;&#x2F;钥匙名 private Lock lock; &#x2F;&#x2F;当前钥匙能开那把锁--&gt; &lt;resultMap type&#x3D;&quot;com.lizhi.bean.Key&quot; id&#x3D;&quot;myKey02&quot;&gt; &lt;id property&#x3D;&quot;id&quot; column&#x3D;&quot;id&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;keyName&quot; column&#x3D;&quot;keyName&quot;&#x2F;&gt; &lt;!-- 告诉MyBatis自己去调用一个查询查锁 select&#x3D;&quot;&quot; : 指定一个查询sql的唯一表示即id,MyBatis自动调用指定的sql将查出的lock封装起来 一般会加上sql所在配置文件的命名空间名称 public Lock getLockByIdSimple(Integer id);此时这个锁的方法需要传入一个参数id 告诉MyBatis把那一列的值传递过去 column&#x3D;&quot;&quot; : 指定将那一列的数据传递过去；；lockid是在t_key表里记录了钥匙与锁的对应关系 --&gt; &lt;association property&#x3D;&quot;lock&quot; select&#x3D;&quot;com.lizhi.dao.LockDao.getLockByIdSimple&quot; column&#x3D;&quot;lockid&quot; &gt; &lt;&#x2F;association&gt; &lt;&#x2F;resultMap&gt; &lt;!-- &#x2F;&#x2F;按照锁的id查出所有key public List&lt;Key&gt; getKeysByLockId(Integer id); --&gt; &lt;!-- 测试collection分步查询的延迟加载 --&gt; &lt;select id&#x3D;&quot;getKeysByLockId&quot; resultType&#x3D;&quot;com.lizhi.bean.Key&quot;&gt; select * from t_key where lockid&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; &lt;&#x2F;mapper&gt; LockDao.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-&#x2F;&#x2F;mybatis.org&#x2F;&#x2F;DTD Mapper 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;mybatis.org&#x2F;dtd&#x2F;mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace&#x3D;&quot;com.lizhi.dao.LockDao&quot;&gt; &lt;!-- public Lock getLockById(Integer id); --&gt; &lt;select id&#x3D;&quot;getLockById&quot; resultMap&#x3D;&quot;myLock&quot;&gt; SELECT l.* ,k.id kid, k.&#96;keyname&#96;,k.&#96;lockid&#96; FROM t_lock l LEFT JOIN t_key k ON k.&#96;lockid&#96; &#x3D; l.&#96;id&#96; WHERE l.&#96;id&#96; &#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; &lt;!-- 自定义封装规则 &#x2F;&#x2F;锁的编号 private Integer id; &#x2F;&#x2F;锁的名字 private String lockName; &#x2F;&#x2F;查锁对应的所有钥匙 private List&lt;Key&gt; keys; --&gt; &lt;resultMap type&#x3D;&quot;com.lizhi.bean.Lock&quot; id&#x3D;&quot;myLock&quot;&gt; &lt;id property&#x3D;&quot;id&quot; column&#x3D;&quot;id&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;lockName&quot; column&#x3D;&quot;lockName&quot;&#x2F;&gt; &lt;!-- collection : 定义集合元素封装 property&#x3D;&quot;&quot; : 指定哪个属性是集合属性 javaType : 指定对象类型，只在association标签中使用 ofType&#x3D;&quot;&quot; : 指定集合里的数据类型 --&gt; &lt;collection property&#x3D;&quot;keys&quot; ofType&#x3D;&quot;com.lizhi.bean.Key&quot;&gt; &lt;!-- 在这个标签体中指定集合中这个元素的封装规则 --&gt; &lt;id property&#x3D;&quot;id&quot; column&#x3D;&quot;kid&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;keyName&quot; column&#x3D;&quot;keyname&quot;&#x2F;&gt; &lt;&#x2F;collection&gt; &lt;&#x2F;resultMap&gt; &lt;!-- &#x2F;&#x2F;按照lock的id找锁的简单方法,,测试分步查询 public Lock getLockByIdSimple(Integer id); --&gt; &lt;select id&#x3D;&quot;getLockByIdSimple&quot; resultType&#x3D;&quot;com.lizhi.bean.Lock&quot;&gt; SELECT * FROM t_lock WHERE id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt;&lt;!-- 测试collection分步查询的延迟加载 --&gt; &lt;!-- public Lock getLockByIdStep(Integer id); --&gt; &lt;select id&#x3D;&quot;getLockByIdStep&quot; resultMap&#x3D;&quot;myLockStep&quot;&gt; SELECT * FROM t_lock WHERE id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; &lt;!-- collection分步查询 --&gt; &lt;resultMap type&#x3D;&quot;com.lizhi.bean.Lock&quot; id&#x3D;&quot;myLockStep&quot;&gt; &lt;id property&#x3D;&quot;id&quot; column&#x3D;&quot;id&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;lockName&quot; column&#x3D;&quot;lockName&quot;&#x2F;&gt; &lt;!-- collection : 指定集合类型的属性封装规则 --&gt; &lt;collection property&#x3D;&quot;keys&quot; select&#x3D;&quot;com.lizhi.dao.KeyDao.getKeysByLockId&quot; column&#x3D;&quot;id&quot;&gt;&lt;&#x2F;collection&gt; &lt;&#x2F;resultMap&gt; &lt;&#x2F;mapper&gt; log4j.properties1234567# Global logging configurationlog4j.rootLogger&#x3D;DEBUG, stdout# Console output...log4j.appender.stdout&#x3D;org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout&#x3D;org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern&#x3D;%5p [%t] - %m%n log4j.xml12345678910111213141516171819202122&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE log4j:configuration SYSTEM &quot;log4j.dtd&quot;&gt; &lt;log4j:configuration xmlns:log4j&#x3D;&quot;http:&#x2F;&#x2F;jakarta.apache.org&#x2F;log4j&#x2F;&quot;&gt; &lt;appender name&#x3D;&quot;STDOUT&quot; class&#x3D;&quot;org.apache.log4j.ConsoleAppender&quot;&gt; &lt;param name&#x3D;&quot;Encoding&quot; value&#x3D;&quot;UTF-8&quot; &#x2F;&gt; &lt;layout class&#x3D;&quot;org.apache.log4j.PatternLayout&quot;&gt; &lt;param name&#x3D;&quot;ConversionPattern&quot; value&#x3D;&quot;%-5p %d&#123;MM-dd HH:mm:ss,SSS&#125; %m (%F:%L) \\n&quot; &#x2F;&gt; &lt;&#x2F;layout&gt; &lt;&#x2F;appender&gt; &lt;logger name&#x3D;&quot;java.sql&quot;&gt; &lt;level value&#x3D;&quot;debug&quot; &#x2F;&gt; &lt;&#x2F;logger&gt; &lt;logger name&#x3D;&quot;org.apache.ibatis&quot;&gt; &lt;level value&#x3D;&quot;info&quot; &#x2F;&gt; &lt;&#x2F;logger&gt; &lt;root&gt; &lt;level value&#x3D;&quot;debug&quot; &#x2F;&gt; &lt;appender-ref ref&#x3D;&quot;STDOUT&quot; &#x2F;&gt; &lt;&#x2F;root&gt;&lt;&#x2F;log4j:configuration&gt; mybatis-config.xml123456789101112131415161718192021222324252627282930&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-&#x2F;&#x2F;mybatis.org&#x2F;&#x2F;DTD Config 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;mybatis.org&#x2F;dtd&#x2F;mybatis-3-config.dtd&quot;&gt;&lt;configuration&gt; &lt;settings&gt; &lt;!-- 开启延迟加载开关 --&gt; &lt;setting name&#x3D;&quot;lazyLoadingEnabled&quot; value&#x3D;&quot;true&quot;&#x2F;&gt; &lt;!-- 开启属性按需加载 --&gt; &lt;setting name&#x3D;&quot;aggressiveLazyLoading&quot; value&#x3D;&quot;false&quot;&#x2F;&gt; &lt;&#x2F;settings&gt; &lt;environments default&#x3D;&quot;development&quot;&gt; &lt;environment id&#x3D;&quot;development&quot;&gt; &lt;transactionManager type&#x3D;&quot;JDBC&quot;&#x2F;&gt; &lt;!-- 配置连接池 POOLED: 连接池--&gt; &lt;dataSource type&#x3D;&quot;POOLED&quot;&gt; &lt;!-- $&#123;&#125;取出配置值 --&gt; &lt;property name&#x3D;&quot;driver&quot; value&#x3D;&quot;com.mysql.cj.jdbc.Driver&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;url&quot; value&#x3D;&quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;mybatis_0325?serverTimezone&#x3D;GMT%2B8&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;username&quot; value&#x3D;&quot;root&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;password&quot; value&#x3D;&quot;******&quot;&#x2F;&gt; &lt;&#x2F;dataSource&gt; &lt;&#x2F;environment&gt; &lt;&#x2F;environments&gt; &lt;mappers&gt; &lt;package name&#x3D;&quot;com.lizhi.dao&quot;&#x2F;&gt; &lt;&#x2F;mappers&gt;&lt;&#x2F;configuration&gt; MyBatis_Key_Test.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149package com.lizhi.test;import java.io.IOException;import java.io.InputStream;import java.util.List;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.Before;import org.junit.jupiter.api.Test;import com.lizhi.bean.Cat;import com.lizhi.bean.Key;import com.lizhi.bean.Lock;import com.lizhi.dao.CatDao;import com.lizhi.dao.KeyDao;import com.lizhi.dao.LockDao;&#x2F;** * 联合查询测试 * @author Administrator * *&#x2F;public class MyBatis_Key_Test &#123; &#x2F;&#x2F;工厂一个 SqlSessionFactory sqlSessionFactory; @Before public void initSqlSessionFactory() throws IOException &#123; &#x2F;&#x2F;1.跟据全局配置文件先得到SqlSessionFactory对象(官方文档复制)，快捷键Ctrl+shift+O导包 String resource &#x3D; &quot;mybatis-config.xml&quot;; InputStream inputStream &#x3D; Resources.getResourceAsStream(resource); sqlSessionFactory &#x3D; new SqlSessionFactoryBuilder().build(inputStream); &#125; &#x2F;* * 联合查询情况下 * 1.使用级联属性封装联合查询后的所有结果 *&#x2F; @Test public void test01() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; KeyDao mapper &#x3D; openSession.getMapper(KeyDao.class); Key KeyById &#x3D; mapper.getKeyById(1); System.out.println(KeyById); &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125; &#x2F;** * 1-n 查锁的时候将所有的钥匙也查询出来 * @throws IOException *&#x2F; @Test public void test02() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; LockDao mapper &#x3D; openSession.getMapper(LockDao.class); Lock lock &#x3D; mapper.getLockById(3); System.out.println(lock); System.out.println(&quot;所有锁如下：&quot;); List&lt;Key&gt; keys &#x3D; lock.getKeys(); for (Key key : keys) &#123; System.out.println(key); &#125; &#125; finally &#123; openSession.close(); &#125; &#125; &#x2F;** * 分步查询 * 0.查询钥匙的时候顺便查出锁 * 1.Key key &#x3D; KeyDao.getKeyById() * 2.Lock lock &#x3D; lockDao.getLockById(1) * * 测试按需加载和延迟加载 * @throws IOException *&#x2F; @Test public void test03() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; KeyDao mapper &#x3D; openSession.getMapper(KeyDao.class); &#x2F;&#x2F; Key key &#x3D; mapper.getKeyByIdSimple(1); &#x2F;&#x2F;如果只是查钥匙名，但每次都会加载锁表的全部信息，造成了性能的浪费 &#x2F;&#x2F;开启按需加载和延迟加载后会发现只调用了一个sql语句加载用来获取钥匙名 &#x2F;&#x2F;按需加载：需要的时候再去查询；全局开启按需加载策略 &#x2F;&#x2F;延迟加载： 不着急加载（查询对象） System.out.println(key.getKeyName()); Thread.sleep(3000); String lockName &#x3D; key.getLock().getLockName(); System.out.println(lockName); &#125; catch (InterruptedException e) &#123; &#x2F;&#x2F; TODO Auto-generated catch block e.printStackTrace(); &#125; finally &#123; openSession.close(); &#125; &#125; &#x2F;** * 测试collection分步查询的延迟加载 * * 一般在工作的时候，写成两个方法 * public Key getKeyByIdSimple(Integer id);&#x2F;&#x2F;按照key的id找锁的简单方法,,测试分步查询 * *推荐都来写链接查询（left join） *public Key getKeyAssicate() *&#x2F; @Test public void test04() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; LockDao mapper &#x3D; openSession.getMapper(LockDao.class); Lock lock &#x3D; mapper.getLockByIdStep(3); System.out.println(lock.getLockName()); List&lt;Key&gt; keys &#x3D; lock.getKeys(); for (Key key : keys) &#123; System.out.println(key.getKeyName()); &#125; &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125;&#125;","categories":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"-ssm -Spring MyBatis的关联映射","slug":"ssm-Spring-MyBatis的关联映射","permalink":"http://www.studyz.club/tags/ssm-Spring-MyBatis%E7%9A%84%E5%85%B3%E8%81%94%E6%98%A0%E5%B0%84/"}]},{"title":"软件开发框架之MyBatis","slug":"软件开发框架之Spring-MyBatis","date":"2019-10-31T13:19:00.438Z","updated":"2019-11-09T08:13:22.823Z","comments":true,"path":"posts/a7d5b03b/","link":"","permalink":"http://www.studyz.club/posts/a7d5b03b/","excerpt":"","text":"1. 什么是MyBatis 1.2 Hibernate-数据库交互的框架（ORM框架） 1.3 MyBatis是持久化层框架（SQL映射框架）-操作数据库 1.3.1 环境搭建 1.3.2 如何写xml有提示 1.3.3 Hello测试实例完整代码 1.3.4 利用MyBatis实现对数据库的增删改查 1.4 全局配置文件 1.4.1 配置文件的主要元素介绍 1.4.2 映射文件的主要元素介绍 1.4.2.1 传参问题 MyBatis官方中文文档 1.4.3 配置文件详解测试实例源码 1. 什么是MyBatis MyBatis框架也被称之为ORM（Object/Relation Mapping，即对象关系映射）框架。所谓的ORM就是一种为了解决面向对象与关系型数据库中数据类型不匹配的技术，它通过描述Java对象与数据库表之间的映射关系，自动将Java应用程序中的对象持久化到关系型数据库的表中。 MyBatis（前身是iBatis）是一个支持普通SQL查询、存储过程以及高级映射的持久层框架。 MyBatis官方中文文档MyBatis–GitHub地址，下载地址ORM框架的工作原理 1.2 Hibernate-数据库交互的框架（ORM框架）（使用各种框架，全自动的框架） 需求 最好有一个框架能支持定制化sql，而且还功能强大；sql也不要硬编码在java文件中（导致维护修改起来比较麻烦）； jdbc是纯手工， Hibernate是全自动。就像洗衣机纯手工麻烦，全自动洗衣机不能完成对有特殊洗涤要求的衣服进行定点清洗。而MyBatis将SQL文件单独放出来其他步骤自动执行，便于维护。 1）、MyBatis将重要的步骤抽取出来可以人工定制，其他步骤自动化； 2）、重要步骤都是写在配置文件中（好维护）； 3）、完全解决数据库的优化问题； 4）、MyBatis底层就是对原生JDBC的一个简单封装； 5）、既将java编码与sql抽取了出来，还不会失去自动化功能；半自动的持久化层框架； 6）、mybatis是一个轻量级的框架； Hibernate与MyBatis有什么区别 MyBatis工作原理 1.3 MyBatis是持久化层框架（SQL映射框架）-操作数据库1.3.1 环境搭建 使用MyBatis框架非常简单，只需在应用程序中引入MyBatis的核心包和lib目录中的依赖包即可。 如果底层采用的是MySQL数据库，那么还需要将MySQL数据库的驱动JAR包添加到应用程序的类路径中；如果采用其他类型的数据库，则同样需要将对应类型的数据库驱动包添加到应用程序的类路径中。 123456789101112131415161718192021221）、环境搭建 1）、创建一个java工程； 2）、创建测试库，测试表，以及封装数据的javaBean，和操作数据库的dao接口 创建表：自己用工具创建 创建javaBean：Employee（封装表的数据） 创建一个Dao接口，用来操作数据库； 3）、用MyBatis操作数据库？ 1）、导包 &#x2F;&#x2F;mysql-connector-java-5.1.37-bin.jar &#x2F;&#x2F;mybatis-3.4.1.jar &#x2F;&#x2F;log4j-1.2.17.jar &#x2F;&#x2F;建议导入日志包；这样的化在mybatis关键的环节就会有日志打印； &#x2F;&#x2F;log4j（日志框架）;依赖类路径下一个log4j.xml配置文件； 2）、写配置（两个，全局配置文件（指导mybatis运行的），dao接口的实现文件（描述dao中每个方法怎么工作）） 1）、第一个配置文件；（称为mybatis的全局配置文件，指导mybatis如何正确运行，比如连接向哪个数据库）(mybatis-config.xml) 2）、第二个配置文件：（编写每一个方法都如何向数据库发送sql语句，如何执行。。。。相当于接口的实现类）(EmployeeDao.xml) 1)、将mapper的namespace属性改为接口的全类名 2)、配置细节 第二个配置文件配置细节 3）、我们写的dao接口的实现文件，mybatis默认是不知道的，需要在全局配置文件中注册； 123）、测试 1）、根据全局配置文件先创建一个 12）、sqlSessionFactory中获取sqlSession对象操作数据库即可 Hello测试实例完整代码1.3.2 如何写xml有提示123451）、只要eclipse找到了这个文件的dtd约束文件的位置即可2）、绑定约束文件的位置 1、复制dtd的引用网址： http:&#x2F;&#x2F;mybatis.org&#x2F;dtd&#x2F;mybatis-3-config.dtd(这个是mybatis-config.xml需要的，哪个配置文件需要什么dtd文件在配置文件头可以看到，别的配置文件想要有提示也需要按此方法配置) 2.windows--&gt;Preferences--&gt;XML--&gt;XML Catalog--&gt;Add 将mybatis的jar包解压缩就可得到这俩dtd文件 两个文件：1234567 1）、全局配置文件：mybatis-config.xml；指导mybatis正确运行的一些全局设置； 2）、SQL映射文件：EmployeeDao.xml；相当于是对Dao接口的一个实现描述细节： 1）、获取到的是接口的代理对象；mybatis自动创建的； 2）、SqlSessionFactory和SqlSession； SqlSessionFactory创建SqlSession对象，Factory只new一次就行 SqlSession：相当于connection和数据库进行交互的，和数据库的一次会话，就应该创建一个新的sqlSession； 1.4 配置文件元素SqlSessionFactory SqlSessionFactory:是MyBatis框架中十分重要的对象，它是单个数据库映射关系经过编译后的内存镜像，其主要作用是创建SqlSession。 SqlSessionFactory对象的实例可以通过SqlSessionFactoryBuilder对象来构建，而SqlSessionFactoryBuilder则可以通过XML配置文件或一个预先定义好的Configuration实例构建出SqlSessionFactory的实例。 通过XML配置文件构建出的SqlSessionFactory实例现代码如下： 123InputStream inputStream &#x3D; Resources.getResourceAsStream(&quot;配置文件位置&quot;);SqlSessionFactory sqlSessionFactory &#x3D; new SqlSessionFactoryBuilder().build(inputStream); SqlSessionFactory对象是线程安全的，它一旦被创建，在整个应用执行期间都会存在。如果我们多次的创建同一个数据库的SqlSessionFactory，那么此数据库的资源将很容易被耗尽。为此，通常每一个数据库都会只对应一个SqlSessionFactory，所以在构建SqlSessionFactory实例时，建议使用单列模式。 SqlSession SqlSession是MyBatis框架中另一个重要的对象，它是应用程序与持久层之间执行交互操作的一个单线程对象，其主要作用是执行持久化操作。 使用完SqlSession对象后要及时关闭，通常可以将其放在finally块中关闭。 123456SqlSession sqlSession &#x3D; sqlSessionFactory.openSession();try &#123; &#x2F;&#x2F; 此处执行持久化操作&#125; finally &#123; sqlSession.close();&#125; 使用工具类创建SqlSession 为了简化开发，通常在实际项目中都会使用工具类来创建SqlSession 123456789101112131415public class MybatisUtils &#123; private static SqlSessionFactory sqlSessionFactory &#x3D; null; static &#123; try &#123; Reader reader &#x3D; Resources.getResourceAsReader(&quot;mybatis-config.xml&quot;); sqlSessionFactory &#x3D; new SqlSessionFactoryBuilder().build(reader); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public static SqlSession getSession() &#123; return sqlSessionFactory.openSession(); &#125; &#125; 1.4.1 配置文件的主要元素介绍 实例中的mybatis-config.xml mybatis-config.xml 在MyBatis框架的核心配置文件中，&lt;configuration&gt;元素是配置文件的根元素，其他元素都要在&lt;configuration&gt;元素内配置。 1.&lt;properties&gt;元素 &lt;properties&gt;是一个配置属性的元素，该元素通常用来将内部的配置外在化，即通过外部的配置来动态的替换内部定义的属性。例如，数据库的连接等属性，就可以通过典型的Java属性文件中的配置来替换，具体方式如下： 1.编写db.properties 12345username&#x3D;rootpassword&#x3D;******driverclass&#x3D;com.mysql.cj.jdbc.Driverjdbcurl&#x3D;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;mybatis_0325?serverTimezone&#x3D;GMT%2B8&amp;useSSL&#x3D;false 2.配置&lt;properties… /&gt;属性 1234567&lt;!-- 1. 和Spring的context,property-placeholder;引用外部配置文件 --&gt; &lt;!-- resource : 从类路径下开始引用 url: 引用磁盘路径文件或者网络路径的资源 --&gt; &lt;properties resource&#x3D;&quot;dbconfig.properties&quot;&gt;&lt;&#x2F;properties&gt; 3.修改配置文件中数据库连接的信息 12345678&lt;dataSource type&#x3D;&quot;POOLED&quot;&gt; &lt;!-- $&#123;&#125;取出配置值 --&gt; &lt;property name&#x3D;&quot;driver&quot; value&#x3D;&quot;$&#123;driverclass&#125;&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;url&quot; value&#x3D;&quot;$&#123;jdbcurl&#125;&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;username&quot; value&#x3D;&quot;$&#123;username&#125;&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;password&quot; value&#x3D;&quot;$&#123;password&#125;&quot;&#x2F;&gt; &lt;&#x2F;dataSource&gt; 2.&lt;settings&gt;元素 &lt;settings&gt;元素主要用于改变MyBatis运行时的行为，例如开启二级缓存、开启延迟加载等。 123456789101112&lt;!-- 2. settings这是 MyBatis 中极为重要的调整设置，它们会改变 MyBatis 的运行时行为。 详见文档 --&gt; &lt;!-- mapUnderscoreToCamelCase: 是否开启自动驼峰命名规则（camel case）映射，即从经典数据库列名 A_COLUMN 到经典 Java 属性名 aColumn 的类似映射。 即在接口Employee中，用驼峰命名可以和数据库中用下划线命名的列名相对应,javaBean 可以自动对应 --&gt; &lt;settings&gt; &lt;!-- name: 配置项的key value:配置项的值 --&gt; &lt;!-- 数据库不区分大小写 loginAcount login_account --&gt; &lt;setting name&#x3D;&quot;mapUnderscoreToCamelCase&quot; value&#x3D;&quot;true&quot;&#x2F;&gt; &lt;&#x2F;settings&gt; 上述示例配置通常不需要开发人员去配置，了解即可。 3.&lt;typeAliases&gt;元素 &lt;typeAliases&gt;元素用于为配置文件中的Java类型设置一个简短的名字，即设置别名。别名的设置与XML配置相关，其使用的意义在于减少全限定类名的冗余。 3.1 使用元素配置别名的方法如下： 12345678910111213&lt;!-- 3.类型别名，为常用的类型（JavaBean）起别名 --&gt; &lt;typeAliases&gt; &lt;!-- typeAlias: 就是为一个javaBean起别名；别名默认就是类名（不区分大小写），配置文件中就可以用别名了 --&gt; &lt;!-- 在EmployeeDao.xml中&lt;select id&#x3D;&quot;getEmpById&quot; resultType&#x3D;&quot;Emp&quot; &gt;标签就可以直接用Employee代替简化开发 alias&#x3D;&quot;&quot;指定一个别名如果不指定就是默认Employee--&gt; &lt;!-- &lt;typeAlias type&#x3D;&quot;com.lizhi.bean.Employee&quot; alias&#x3D;&quot;emp&quot;&#x2F;&gt;--&gt; &lt;!-- 批量起别名 ，name&#x3D;&quot;&quot;指定包名，为这个包下的所有类起别名,默认别名就是类名 也可以在指定的类上面添加@Alias(&quot;&quot;)注解的方式指定别名 --&gt; &lt;!-- &lt;package name&#x3D;&quot;com.lizhi.bean&quot;&#x2F;&gt;--&gt; &lt;!-- 推荐使用全类名 --&gt; &lt;&#x2F;typeAliases&gt; 1234&lt;typeAliases&gt; &lt;typeAlias alias&#x3D;&quot;user&quot; type&#x3D;&quot;com.dqsy.po.User&quot;&#x2F;&gt; &lt;&#x2F;typeAliases&gt; 3.2 当POJO类过多时，可以通过自动扫描包的形式自定义别名，具体如下： 1234&lt;typeAliases&gt; &lt;package name&#x3D;&quot;com.dqsy.po&quot;&#x2F;&gt; &lt;&#x2F;typeAliases&gt; 如果在程序中使用了注解，则别名为其注解的值。 MyBatis框架默认为许多常见的Java类型提供了相应的类型别名，如下表所示。 4.&lt;typeHandler&gt;元素 &lt;typeHandler&gt;的作用就是将预处理语句中传入的参数从javaType（Java类型）转换为jdbcType（JDBC类型），或者从数据库取出结果时将jdbcType转换为javaType。 &lt;typeHandler&gt;元素可以在配置文件中注册自定义的类型处理器，它的使用方式有两种。 123456789原生jdbc执行sql；或者封装结果集 Connection connection &#x3D; dataSource.getConnection(); String sql &#x3D; &quot;insert into t_employee(empname,email,gender) value(?,?,?)&quot;; PreparedStatement ps &#x3D; connection.prepareStatement(sql); ps.setString(0,&quot;admin&quot;); ps.setString(1,&quot;aaa@qq.com&quot;); ps.setInt(2,12); int i &#x3D; ps.executeUpdate() &#x2F;&#x2F;ps.close();connection.close(); 1.注册一个类的类型处理器 1234&lt;typeHandlers&gt; &lt;typeHandler handler&#x3D;&quot;com.dqsy.type.CustomtypeHandler&quot; &#x2F;&gt; &lt;&#x2F;typeHandlers&gt; 2.注册一个包中所有的类型处理器 1234&lt;typeHandlers&gt; &lt;package name&#x3D;&quot;com.dqsy.type&quot; &#x2F;&gt; &lt;&#x2F;typeHandlers&gt; 5.&lt;objectFactory&gt;元素 MyBatis中默认的ObjectFactory的作用是实例化目标类，它既可以通过默认构造方法实例化，也可以在参数映射存在的时候通过参数构造方法来实例化。通常使用默认的ObjectFactory即可。 大部分场景下都不用配置和修改默认的ObjectFactory ，如果想覆盖ObjectFactory的默认行为，可以通过自定义ObjectFactory来实现，具体如下： 1.自定义一个对象工厂 1234567891011121314151617public class MyObjectFactory extends DefaultObjectFactory &#123; private static final long serialVersionUID &#x3D; -4114845625429965832L; public &lt;T&gt; T create(Class&lt;T&gt; type) &#123; return super.create(type); &#125; public &lt;T&gt; T create(Class&lt;T&gt; type, List&lt;Class&lt;?&gt;&gt; constructorArgTypes, List&lt;Object&gt; constructorArgs) &#123; return super.create(type, constructorArgTypes, constructorArgs); &#125; public void setProperties(Properties properties) &#123; super.setProperties(properties); &#125; public &lt;T&gt; boolean isCollection(Class&lt;T&gt; type) &#123; return Collection.class.isAssignableFrom(type); &#125; &#125; 2.在配置文件中使用&lt;objectFactory&gt;元素配置自定义的ObjectFactory 1234&lt;objectFactory type&#x3D;&quot;com.dqsy.factory.MyObjectFactory&quot;&gt; &lt;property name&#x3D;&quot;name&quot; value&#x3D;&quot;MyObjectFactory&quot;&#x2F;&gt;&lt;&#x2F;objectFactory&gt; 由于自定义ObjectFactory在实际开发时不经常使用，了解即可。 6.&lt;plugins&gt;元素 MyBatis允许在已映射语句执行过程中的某一点进行拦截调用，这种拦截调用是通过插件来实现的。&lt;plugins&gt;元素的作用就是配置用户所开发的插件。 如果用户想要进行插件开发，必须要先了解其内部运行原理，因为在试图修改或重写已有方法的行为时，很可能会破坏MyBatis原有的核心模块。关于插件的使用，只需了解&lt;plugins&gt;元素的作用即可，有兴趣的读者可以查找官方文档等资料自行学习。 7. &lt;environments&gt;元素 &lt;environments&gt;元素用于对环境进行配置。MyBatis的环境配置实际上就是数据源的配置，我们可以通过&lt;environments&gt;元素配置多种数据源，即配置多种数据库。 使用元素进行环境配置的示例如下： 12345678910111213141516171819&lt;!-- 6.environments配置环境 environment: 配置一个具体的环境。都需要一个事务管理器和一个数据源。 可以配置多个environments，id是当前环境的唯一标示 default&#x3D;&quot;development&quot; : 默认使用哪个环境 后来数据源和事务管理都是Spring来做 --&gt; &lt;environments default&#x3D;&quot;development&quot;&gt; &lt;environment id&#x3D;&quot;development&quot;&gt; &lt;transactionManager type&#x3D;&quot;JDBC&quot;&#x2F;&gt; &lt;!-- 配置连接池 POOLED: 连接池--&gt; &lt;dataSource type&#x3D;&quot;POOLED&quot;&gt; &lt;!-- $&#123;&#125;取出配置值 --&gt; &lt;property name&#x3D;&quot;driver&quot; value&#x3D;&quot;$&#123;driverclass&#125;&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;url&quot; value&#x3D;&quot;$&#123;jdbcurl&#125;&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;username&quot; value&#x3D;&quot;$&#123;username&#125;&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;password&quot; value&#x3D;&quot;$&#123;password&#125;&quot;&#x2F;&gt; &lt;&#x2F;dataSource&gt; &lt;&#x2F;environment&gt; &lt;&#x2F;environments&gt; 在MyBatis中，可以配置两种类型的事务管理器，分别是JDBC和MANAGED。关于这两个事务管理器的描述如下： JDBC：此配置直接使用了JDBC的提交和回滚设置，它依赖于从数据源得到的连接来管理事务的作用域。 MANAGED：此配置从来不提交或回滚一个连接，而是让容器来管理事务的整个生命周期。默认情况下，它会关闭连接，但一些容器并不希望这样，为此可以将closeConnection属性设置为false来阻止它默认的关闭行为。 注意：如果项目中使用的是Spring+ MyBatis，则没有必要在MyBatis中配置事务管理器，因为实际开发中，会使用Spring自带的管理器来实现事务管理 数据源的配置 1.UNPOOLED 配置此数据源类型后，在每次被请求时会打开和关闭连接。它对没有性能要求的简单应用程序是一个很好的选择。在使用时，需要配置5种属性。 3.JNDI 可以在EJB或应用服务器等容器中使用。容器可以集中或在外部配置数据源，然后放置一个JNDI上下文的引用。在使用时，需要配置2个属性。 8.&lt;databaseIdProvider&gt;元素12345678910111213&lt;!-- 7.mybatis用来考虑数据库移植性的 ,例如ODBC和JDBC的语法不同，当切换数据库的时候会报错 type&#x3D;&quot;DB_VENDOR&quot;是固定的值--&gt; &lt;databaseIdProvider type&#x3D;&quot;DB_VENDOR&quot;&gt; &lt;!-- name&#x3D;&quot;&quot;:数据库厂商标识，value&#x3D;&quot;&quot;:给这个标识起一个好用的名字 MySQL的标识：MySQL Oracle的标识：Oracle SQL Server的标识：SQL Server --&gt; &lt;property name&#x3D;&quot;MySQL&quot; value&#x3D;&quot;mysql&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;Oracle&quot; value&#x3D;&quot;oracle&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;SQL Server&quot; value&#x3D;&quot;sqlserver&quot;&#x2F;&gt; &lt;&#x2F;databaseIdProvider&gt; EmployeeDao.xml写sql实例语句时加上databaseId 1234567891011121314151617 &lt;select id&#x3D;&quot;getEmpById&quot; resultType&#x3D;&quot;com.lizhi.bean.Employee&quot; &gt; &lt;!-- sql语句不要写分号 --&gt; select * from t_employee where id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt;&lt;!-- 如果能精确匹配，不能就用模糊的 &lt;select id&#x3D;&quot;getEmpById&quot; resultType&#x3D;&quot;com.lizhi.bean.Employee&quot; databaseId&#x3D;&quot;mysql&quot;&gt; select * from t_employee where id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; &lt;select id&#x3D;&quot;getEmpById&quot; resultType&#x3D;&quot;com.lizhi.bean.Employee&quot; databaseId&#x3D;&quot;oracle&quot;&gt; select * from t_employee where id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; &lt;select id&#x3D;&quot;getEmpById&quot; resultType&#x3D;&quot;com.lizhi.bean.Employee&quot; databaseId&#x3D;&quot;sqlserver&quot;&gt; select * from t_employee where id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt;--&gt; 一般开发中都会提前确定好数据库的厂商，很少有改动的需求 9.&lt;mappers&gt;元素123456789101112131415161718192021&lt;!-- 8.写好的Sql映射文件需要使用mappers注册进来 --&gt; &lt;mappers&gt; &lt;!-- url: 可以从磁盘或者网络路径引用 resource: 在类路径下找sql映射文件 class: 直接引用接口的全类名，可以将接口的实例化配置文件放在和接口同一个包下，且文件名和接口名一致 class的另一种方法； --&gt; &lt;!-- &lt;mapper resource&#x3D;&quot;mybatis&#x2F;EmployeeDao.xml&quot;&#x2F;&gt; --&gt; &lt;!-- &lt;mapper class&#x3D;&quot;com.lizhi.dao.EmployeeDao&quot;&#x2F;&gt; --&gt; &lt;!-- &lt;mapper class&#x3D;&quot;com.lizhi.dao.EmployeeDaoAnnotation&quot;&#x2F;&gt; --&gt; &lt;!-- 常规的配置方法和注解方法配合使用 重要的Dao可以写配置，简单的Dao就直接写注解 --&gt; &lt;!-- 批量注册name&#x3D;&quot;&quot;,Dao所在包名 这里有个问题；批量扫描只能扫描指定的包名，而不能扫描位于conf&#x2F;mybatis&#x2F;EmployeeDao.xml路径下的 EmployeeDao文件，这时将包名mybatis改成com.lizhi.dao虽然看起来是两个包，但是从类路径来看是一个包 因此都可以扫描 --&gt; &lt;package name&#x3D;&quot;com.lizhi.dao&quot;&#x2F;&gt; &lt;&#x2F;mappers&gt; 1.使用类路径引入 1234&lt;mappers&gt; &lt;mapper resource&#x3D;&quot;com&#x2F;dqsy&#x2F;mapper&#x2F;UserMapper.xml&quot;&#x2F;&gt;&lt;&#x2F;mappers&gt; 2.使用本地文件路径引入 1234&lt;mappers&gt; &lt;mapper url&#x3D;&quot;file:&#x2F;&#x2F;&#x2F;D:&#x2F;com&#x2F;dqsy&#x2F;mapper&#x2F;UserMapper.xml&quot;&#x2F;&gt;&lt;&#x2F;mappers&gt; 3.使用接口类引入可以将接口的实例化配置文件放在和接口同一个包下，且文件名和接口名一致 1234&lt;mappers&gt; &lt;mapper class&#x3D;&quot;com.dqsy.mapper.UserMapper&quot;&#x2F;&gt;&lt;&#x2F;mappers&gt; 4.使用包名引入 1234&lt;mappers&gt; &lt;package name&#x3D;&quot;com.dqsy.mapper&quot;&#x2F;&gt;&lt;&#x2F;mappers&gt; 1.4.2 映射文件的主要元素介绍 实例中对应的EmployeeDao.xml 在映射文件中，&lt;mapper&gt;元素是映射文件的根元素，其他元素都是它的子元素。 1.&lt;select&gt;元素 &lt;select&gt;元素用来映射查询语句，它可以帮助我们从数据库中读取出数据，并组装数据给业务开发人员。 使用&lt;select&gt;元素执行查询操作非常简单，其示例如下： 12345&lt;!-- public Employee getEmpById(Integer id); --&gt; &lt;select id&#x3D;&quot;getEmpById&quot; resultType&#x3D;&quot;com.lizhi.bean.Employee&quot;&gt; SELECT * FROM t_employee WHERE id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; 传参问题EmployeeDao.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980 &lt;!-- 传参到底能传哪些 --&gt; &lt;!-- public Employee getEmpById(Integer id); --&gt; &lt;select id&#x3D;&quot;getEmpById&quot; resultType&#x3D;&quot;com.lizhi.bean.Employee&quot;&gt; SELECT * FROM t_employee WHERE id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt;&lt;!--如果采用没有采用@param的方式 id&#x3D;#&#123;id&#125; and empname&#x3D;#&#123;empname&#125;的方式来传入参数会报如下异常 org.apache.ibatis.exceptions.PersistenceException: ### Error querying database. Cause: org.apache.ibatis.binding.BindingException: Parameter &#39;id&#39; not found. Available parameters are [arg1, arg0, param1, param2]而改成id&#x3D;#&#123;param1&#125; and empname&#x3D;#&#123;param2&#125;或者改成arg1, arg0则可运行（param1, param2，param3, param4） --&gt;&lt;!-- public Employee getEmpByIdAndEmpname(Integer id,String empname); --&gt; &lt;select id&#x3D;&quot;getEmpByIdAndEmpname&quot; resultType&#x3D;&quot;com.lizhi.bean.Employee&quot;&gt; select * from t_employee where id&#x3D;#&#123;id&#125; and empname&#x3D;#&#123;empname&#125; &lt;&#x2F;select&gt; &lt;!-- public Employee getEmployeeAndEmpname(Map&lt;String, Override&gt;map); --&gt; &lt;select id&#x3D;&quot;getEmployeeAndEmpname&quot; resultType&#x3D;&quot;com.lizhi.bean.Employee&quot;&gt; select * from $&#123;tablename&#125; where id&#x3D;#&#123;id&#125; and empname&#x3D;#&#123;empname&#125; &lt;&#x2F;select&gt; &lt;!-- 现象： 1.传入单个参数： ①基本类型： 取值：#&#123;随便写&#125; ②传入pojo: 2.多个参数： public Employee getEmpByIdAndEmpname(Integer id,String empname); 取值：#&#123;参数名&#125;的方式无效 可以用id&#x3D;#&#123;param1&#125; and empname&#x3D;#&#123;param2&#125;或者改成arg1, arg0的方式 原因：只要传入了多个参数MyBatis会自动的将这些参数封装在map中，封装是使用的key就是参数的索引和参数的第几个表示 Map&lt;String,Object&gt; map &#x3D; new HashMap&lt;&gt;(); map.put(&quot;arg1&quot;,传入的值);map.put(&quot;arg1&quot;,&quot;传入的值&quot;)； #&#123;Key&#125;就是从map中取值 3.@param:为参数指定key;命名参数；推荐使用此方法 我们可以告诉MyBatis，封装参数map的时候别乱来，使用我们指定的Key public Employee getEmpByIdAndEmpname(@Param(&quot;id&quot;)Integer id,@Param(&quot;empname&quot;)String empname); 4.传入pojo(javaBean) 取值：#&#123;pojo的属性名&#125; 5.传入map:将多个要使用的参数封装起来 取值：#&#123;key&#125; 扩展：多个参数：自动封装map;多种方式混合 method01(@param(&quot;id&quot;)Integer id,String empname,Employee employee); Integwe id -&gt;#&#123;id&#125; String empname -&gt; #&#123;param2&#125; Employee employee (取出这个里面的email) -&gt;#&#123;param3.email&#125; 无论传入什么参数都要能正确的取出值； #&#123;key&#x2F;属性名&#125; 1. #&#123;key&#125;取值的时候可以设置一些规则 id&#x3D;#&#123;id,jdbcType&#x3D;INTEFER&#125;都是自动配置的所以可以不写 javaType,jdbcType,mode,numericScale,resultMap.typeHandler,jdbcTypeName,expression 只有jdbcType才可能是需要被指定的，默认不指定jdbcType，mysql没问题，oracle没问题， 万一传入的数据是null,mysql插入null没问题，【oracle不指定null到底是什么类型】 实际上在MyBatis中，有两种取值方式： #&#123;属性名&#125;: 是参数预编译的方式，参数的位置都是用？替代，参数后来都是预编译设置进去的；安全，不会有sql注入的问题 $&#123;属性名&#125;: 不是参数预编译，而是直接和sql语句进行拼串；不安全，就是将参数直接放在指定的位置运行 &#x2F;&#x2F;id&#x3D; &#39;传入的字符串&#39; and empname&#x3D; 如果传入&#39;1 or 1&#x3D;1 or&#39;，则不管and后面有没有东西or 后面的 1&#x3D;1永远是true，就会通过验证访问数据库的内容，这就是sql注入 $&#123;属性名&#125;用处：sql语句只有参数位置是支持预编译的,表名等不支持预编译；假设有一个数据库里面有两张日志表 log_2019_11,log_2019_12 select * from log_2019_11 where id&#x3D;? and empname&#x3D;? 需求，id,empname固定查询不同日志表里的日志 解决办法：在getEmployeeAndEmpname方法里测试 采取这种方法可行select * from $&#123;tablename&#125; where id&#x3D;#&#123;id&#125; and empname&#x3D;#&#123;empname&#125; 在getEmployeeAndEmpname方法里测试 id&#x3D;$&#123;id&#125; and empname&#x3D;#&#123;empname&#125; 打印的日志 select * from t_employee where id&#x3D;1 and empname&#x3D;? id&#x3D;#&#123;id&#125; and empname&#x3D;#&#123;empname&#125; 打印的日志 select * from t_employee where id&#x3D;? and empname&#x3D;? 一般使用#&#123;&#125;因为安全，在不支持参数预编译的位置要进行取值就使用$&#123;&#125;;--&gt; 1234567891011121314151617&lt;!-- 查询所有员工public List&lt;Employee&gt; getAllEmps(); --&gt; &lt;!-- resultType&#x3D;&quot;&quot;如果返回的是个集合，写的是集合里面的元素的类型 --&gt; &lt;select id&#x3D;&quot;getAllEmps&quot; resultType&#x3D;&quot;com.lizhi.bean.Employee&quot;&gt; select * from t_employee &lt;&#x2F;select&gt;&lt;!-- 查询返回map --&gt; &lt;!-- public Map&lt;String, Object&gt; getEmpByIdReturnMap(Integer id); --&gt; &lt;select id&#x3D;&quot;getEmpByIdReturnMap&quot; resultType&#x3D;&quot;map&quot;&gt; select * from t_employee where id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; &lt;!-- 查询多个返回一个map ，查询多个情况下，集合里面写元素类型 public Map&lt;Integer, Employee&gt; getEmpsReturnMap(); --&gt; &lt;select id&#x3D;&quot;getEmpsReturnMap&quot; resultType&#x3D;&quot;com.lizhi.bean.Employee&quot;&gt; select * from t_employee &lt;&#x2F;select&gt; 数据库列名与Bean的属性名不对应时的查询方式详见&lt;resultMap&gt;元素&lt;select&gt;元素的常用属性 增删改查常用属性 2. &lt;insert&gt;元素 &lt;insert&gt;元素用于映射插入语句，在执行完元素中定义的SQL语句后，会返回一个表示插入记录数的整数。 元素的配置示例如下：12345678910&lt;insert id&#x3D;&quot;addCustomer&quot; parameterType&#x3D;&quot;com.dqsy.po.Customer&quot; flushCache&#x3D;&quot;true&quot; statementType&#x3D;&quot;PREPARED&quot; keyProperty&#x3D;&quot;&quot; keyColumn&#x3D;&quot;&quot; useGeneratedKeys&#x3D;&quot;&quot; timeout&#x3D;&quot;20&quot;&gt; 元素的属性 &lt;insert&gt;元素的属性与&lt;select&gt;元素的属性大部分相同，但还包含了3个特有属性，这3个属性的描述如下所示。 执行插入操作后，很多时候需要返回插入成功的数据生成的主键值，此时就可以通过上面讲解的3个属性来实现。 2.1 对于支持主键自助增长的数据库（如MySQL），可以通过如下配置实现：12345678910&lt;!-- public int insertEmployee(Employee employee); --&gt; &lt;!-- 让Mybatis自动的将自增的id赋值给传入的employee对象的id的属性 useGeneratedKeys&#x3D;”true”：原生jdbc获取自增主键的方法 keyProperty&#x3D;&quot;&quot;:将刚才自增的id赋值给那个属性 --&gt; &lt;insert id&#x3D;&quot;insertEmployee&quot; useGeneratedKeys&#x3D;&quot;true&quot; keyProperty&#x3D;&quot;id&quot;&gt; INSERT INTO t_employee(empname,gender,email) VALUES(#&#123;empname&#125;,#&#123;gender&#125;,#&#123;email&#125;) &lt;&#x2F;insert&gt; 2.2对于不支持主键自助增长的数据库(如Oracle)，可以通过如下配置实现：12345678910111213&lt;!-- public int insertEmployee2(Employee employee); --&gt; &lt;insert id&#x3D;&quot;insertEmployee2&quot;&gt; &lt;!-- 对于不支持自增主键的数据库采用查询主键的方式 order&#x3D;&quot;BEFORE&quot;:在插入sql语句执行之前查询id 在核心sql语句之前先运行一个查询个查询sql查到id,将查到的id赋值给JavaBean的哪个属性； 获取最大的id值再+1赋值给id达到自增的目的 --&gt; &lt;selectKey order&#x3D;&quot;BEFORE&quot; resultType&#x3D;&quot;integer&quot; keyProperty&#x3D;&quot;id&quot;&gt; select max(id)+1 from t_employee &lt;&#x2F;selectKey&gt; INSERT INTO t_employee(id,empname,gender,email) VALUES(#&#123;id&#125;,#&#123;empname&#125;,#&#123;gender&#125;,#&#123;email&#125;) &lt;&#x2F;insert&gt; 3.&lt;update&gt;元素和&lt;delete&gt;元素 &lt;update&gt;和&lt;delete&gt;元素的使用比较简单，它们的属性配置也基本相同。 &lt;update&gt;和&lt;delete&gt;元素的常用属性如下：12345678910111213&lt;update id&#x3D;&quot;updateCustomer&quot; parameterType&#x3D;&quot;com.dqsy.po.Customer&quot; flushCache&#x3D;&quot;true&quot; statementType&#x3D;&quot;PREPARED&quot; timeout&#x3D;&quot;20&quot;&gt;&lt;delete id&#x3D;&quot;deleteCustomer&quot; parameterType&#x3D;&quot;com.dqsy.po.Customer&quot; flushCache&#x3D;&quot;true&quot; statementType&#x3D;&quot;PREPARED&quot; timeout&#x3D;&quot;20&quot;&gt; &lt;update&gt;和&lt;delete&gt;元素的使用示例如下：12345678910&lt;update id&#x3D;&quot;updateCustomer&quot; parameterType&#x3D;&quot;com.dqsy.po.Customer&quot;&gt; update t_customer set username&#x3D;#&#123;username&#125;,jobs&#x3D;#&#123;jobs&#125;,phone&#x3D;#&#123;phone&#125; where id&#x3D;#&#123;id&#125;&lt;&#x2F;update&gt;&lt;delete id&#x3D;&quot;deleteCustomer&quot; parameterType&#x3D;&quot;Integer&quot;&gt; delete from t_customer where id&#x3D;#&#123;id&#125;&lt;&#x2F;delete&gt; &lt;sql&gt;元素 在一个映射文件中，通常需要定义多条SQL语句，这些SQL语句的组成可能有一部分是相同的（如多条select语句中都查询相同的id、username、jobs字段），如果每一个SQL语句都重写一遍相同的部分，势必会增加代码量，导致映射文件过于臃肿。那么有没有什么办法将这些SQL语句中相同的组成部分抽取出来，然后在需要的地方引用呢？ 元素的作用就是定义可重用的SQL代码片段，然后在其他语句中引用这一代码片段。 定义一个包含id、username、jobs和phone字段的代码片段如下： 12&lt;sql id&#x3D;&quot;customerColumns&quot;&gt;id,username,jobs,phone&lt;&#x2F;sql&gt; 上述代码片段可以包含在其他语句中使用，具体如下：1234567&lt;select id&#x3D;&quot;findCustomerById&quot; parameterType&#x3D;&quot;Integer&quot; resultType&#x3D;&quot;com.dqsy.po.Customer&quot;&gt; select &lt;include refid&#x3D;&quot;customerColumns&quot;&#x2F;&gt; from t_customer where id &#x3D; #&#123;id&#125;&lt;&#x2F;select&gt; 定义sql片段 4.&lt;resultMap&gt;元素 &lt;resultMap&gt;元素表示结果映射集，是MyBatis中最重要也是最强大的元素。它的主要作用是定义映射规则、级联的更新以及定义类型转化器等。CatDao.xml123456789101112131415161718192021222324252627282930&lt;mapper namespace&#x3D;&quot;com.lizhi.dao.CatDao&quot;&gt;&lt;!-- public cat getCatById(Integer id); --&gt; &lt;!-- resultType&#x3D;&quot;com.lizhi.bean.Cat&quot;:使用默认规则，属性名列名一一对应 resultMap&#x3D;&quot;myCat&quot;:查出数据封装结果的时候，使用myCat自定义的结果集， --&gt; &lt;select id&#x3D;&quot;getCatById&quot; resultMap&#x3D;&quot;myCat&quot;&gt; &lt;!-- 这样可以用起别名的方法配合resultType&#x3D;&quot;com.lizhi.bean.Cat&quot;:使用默认规则，属性名列名一一对应 select id,cname name,cAge age,cgender gender from t_cat where id&#x3D;#&#123;id&#125; --&gt; &lt;!-- 这样不可以用 --&gt; select * from t_cat where id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; &lt;!-- 自定义结果集(resultMap)，自己定义每一列数据和JavaBean的映射规则 type&#x3D;&quot;&quot; :制定要为哪个JavaBean自定义封装规则,全类名 id&#x3D;&quot;&quot;:唯一标识 --&gt; &lt;resultMap type&#x3D;&quot;com.lizhi.bean.Cat&quot; id&#x3D;&quot;myCat&quot;&gt; &lt;!-- 指定主键列的对应规则 column&#x3D;&quot;id&quot;:指定那一列是主键列 property&#x3D;&quot;&quot;:指定Cat.java文件里的哪个属性封装id这一列数据 --&gt; &lt;id property&#x3D;&quot;id&quot; column&#x3D;&quot;id&quot;&#x2F;&gt; &lt;!-- 普通列 --&gt; &lt;result property&#x3D;&quot;name&quot; column&#x3D;&quot;cName&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;age&quot; column&#x3D;&quot;cAge&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;gender&quot; column&#x3D;&quot;cgender&quot;&#x2F;&gt; &lt;association property&#x3D;&quot;&quot;&gt;&lt;&#x2F;association&gt; &lt;&#x2F;resultMap&gt; &lt;resultMap&gt;元素中包含了一些子元素，它的元素结构如下所示：1234567891011121314&lt;resultMap type&#x3D;&quot;&quot; id&#x3D;&quot;&quot;&gt; &lt;constructor&gt; &lt;!-- 类在实例化时,用来注入结果到构造方法中--&gt; &lt;idArg&#x2F;&gt; &lt;!-- ID参数;标记结果作为ID--&gt; &lt;arg&#x2F;&gt; &lt;!-- 注入到构造方法的一个普通结果--&gt; &lt;&#x2F;constructor&gt; &lt;id&#x2F;&gt; &lt;!-- 用于表示哪个列是主键--&gt; &lt;result&#x2F;&gt; &lt;!-- 注入到字段或JavaBean属性的普通结果--&gt; &lt;association property&#x3D;&quot;&quot; &#x2F;&gt; &lt;!-- 用于一对一关联 --&gt; &lt;collection property&#x3D;&quot;&quot; &#x2F;&gt; &lt;!-- 用于一对多关联 --&gt; &lt;discriminator javaType&#x3D;&quot;&quot;&gt; &lt;!-- 使用结果值来决定使用哪个结果映射--&gt; &lt;case value&#x3D;&quot;&quot; &#x2F;&gt; &lt;!-- 基于某些值的结果映射 --&gt; &lt;&#x2F;discriminator&gt; &lt;&#x2F;resultMap&gt; 源码1.3.3 Hello测试实例完整代码 包com.lizhi.beanEmployee.java12345678910111213141516171819202122232425262728293031323334353637383940package com.lizhi.bean;public class Employee &#123; private Integer id; private String empname; private String email; private Integer gender; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id &#x3D; id; &#125; public String getEmpname() &#123; return empname; &#125; public void setEmpname(String empname) &#123; this.empname &#x3D; empname; &#125; public String getEmail() &#123; return email; &#125; public void setEmail(String email) &#123; this.email &#x3D; email; &#125; public Integer getGender() &#123; return gender; &#125; public void setGender(Integer gender) &#123; this.gender &#x3D; gender; &#125; @Override public String toString() &#123; return &quot;Employee [id&#x3D;&quot; + id + &quot;, empname&#x3D;&quot; + empname + &quot;, email&#x3D;&quot; + email + &quot;, gender&#x3D;&quot; + gender + &quot;]&quot;; &#125;&#125; 包com.lizhi.bean.Dao接口EmployeeDao.java1234567891011package com.lizhi.bean.Dao;import com.lizhi.bean.Employee;public interface EmployeeDao &#123; &#x2F;&#x2F;按照员工的ID查询 public Employee getEmpById(Integer id);&#125; 配置文件EmployeeDao.xml12345678910111213141516171819&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-&#x2F;&#x2F;mybatis.org&#x2F;&#x2F;DTD Mapper 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;mybatis.org&#x2F;dtd&#x2F;mybatis-3-mapper.dtd&quot;&gt; &lt;!-- namespace : 名称空间，写接口的全类名，相当于告诉MyBatis这个配置文件是实现哪个接口的 --&gt;&lt;mapper namespace&#x3D;&quot;com.lizhi.bean.Dao.EmployeeDao&quot;&gt;&lt;!-- public Employee getEmpById(Integer id); --&gt; &lt;!-- select，用来定义一个查询操作 , id:是方法名，相当于这个配置是对于某个方法的实现类 resultType: 指定方法运行后的返回值类型；（查询操作必须指定） #&#123;属性名&#125;: 代表取出传递过来的某个参数的的值 --&gt; &lt;select id&#x3D;&quot;getEmpById&quot; resultType&#x3D;&quot;com.lizhi.bean.Employee&quot;&gt; select * from t_employee where id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt;&lt;&#x2F;mapper&gt; mybatis-config.xml123456789101112131415161718192021222324&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-&#x2F;&#x2F;mybatis.org&#x2F;&#x2F;DTD Config 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;mybatis.org&#x2F;dtd&#x2F;mybatis-3-config.dtd&quot;&gt;&lt;configuration&gt; &lt;environments default&#x3D;&quot;development&quot;&gt; &lt;environment id&#x3D;&quot;development&quot;&gt; &lt;transactionManager type&#x3D;&quot;JDBC&quot;&#x2F;&gt; &lt;!-- 配置连接池 --&gt; &lt;dataSource type&#x3D;&quot;POOLED&quot;&gt; &lt;property name&#x3D;&quot;driver&quot; value&#x3D;&quot;com.mysql.cj.jdbc.Driver&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;url&quot; value&#x3D;&quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;mybatis_0325?serverTimezone&#x3D;GMT%2B8&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;username&quot; value&#x3D;&quot;root&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;password&quot; value&#x3D;&quot;******&quot;&#x2F;&gt; &lt;&#x2F;dataSource&gt; &lt;&#x2F;environment&gt; &lt;&#x2F;environments&gt; &lt;!-- 引入我们自己编写的每一个接口的实现文件 --&gt; &lt;mappers&gt; &lt;!-- resource: 表示从类路径下找资源 --&gt; &lt;mapper resource&#x3D;&quot;EmployeeDao.xml&quot;&#x2F;&gt; &lt;&#x2F;mappers&gt;&lt;&#x2F;configuration&gt; log4j.xml12345678910111213141516171819202122&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE log4j:configuration SYSTEM &quot;log4j.dtd&quot;&gt;&lt;log4j:configuration xmlns:log4j&#x3D;&quot;http:&#x2F;&#x2F;jakarta.apache.org&#x2F;log4j&#x2F;&quot;&gt; &lt;appender name&#x3D;&quot;STDOUT&quot; class&#x3D;&quot;org.apache.log4j.ConsoleAppender&quot;&gt; &lt;param name&#x3D;&quot;Encoding&quot; value&#x3D;&quot;UTF-8&quot; &#x2F;&gt; &lt;layout class&#x3D;&quot;org.apache.log4j.PatternLayout&quot;&gt; &lt;param name&#x3D;&quot;ConversionPattern&quot; value&#x3D;&quot;%-5p %d&#123;MM-dd HH:mm:ss,SSS&#125; %m (%F:%L) \\n&quot; &#x2F;&gt; &lt;&#x2F;layout&gt; &lt;&#x2F;appender&gt; &lt;logger name&#x3D;&quot;java.sql&quot;&gt; &lt;level value&#x3D;&quot;debug&quot; &#x2F;&gt; &lt;&#x2F;logger&gt; &lt;logger name&#x3D;&quot;org.apache.ibatis&quot;&gt; &lt;level value&#x3D;&quot;info&quot; &#x2F;&gt; &lt;&#x2F;logger&gt; &lt;root&gt; &lt;level value&#x3D;&quot;debug&quot; &#x2F;&gt; &lt;appender-ref ref&#x3D;&quot;STDOUT&quot; &#x2F;&gt; &lt;&#x2F;root&gt;&lt;&#x2F;log4j:configuration&gt; log4j.properties1234567# Global logging configurationlog4j.rootLogger&#x3D;DEBUG, stdout# Console output...log4j.appender.stdout&#x3D;org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout&#x3D;org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern&#x3D;%5p [%t] - %m%n 测试类MyBatisTest123456789101112131415161718192021222324252627282930313233343536373839package com.lizhi.bean.Dao;import java.io.IOException;import java.io.InputStream;import javax.imageio.IIOException;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.jupiter.api.Test;import com.lizhi.bean.Employee;public class MyBatisTest &#123; @Test public void test() throws IOException &#123; &#x2F;&#x2F;1.根据全局配置文件创建出一个SqlSessionFactory &#x2F;&#x2F;SqlSessionFactory,是SqlSession工厂，负责创建SqlSession对象 &#x2F;&#x2F;SqlSession: Sql回话（代表和数据库的一次会话）; String resource &#x3D; &quot;mybatis-config.xml&quot;; InputStream inputStream &#x3D; Resources.getResourceAsStream(resource); SqlSessionFactory sqlSessionFactory &#x3D; new SqlSessionFactoryBuilder().build(inputStream); &#x2F;&#x2F;2.获取和数据库的一次会话,就相当于getConnection(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(); &#x2F;&#x2F;3.使用SqlSession操作数据库,先获取带Dao接口的实现 EmployeeDao employeeDao &#x3D; openSession.getMapper(EmployeeDao.class); &#x2F;&#x2F;4.调用之前的方法 Employee employee &#x3D; employeeDao.getEmpById(1); System.out.println(employee); &#125;&#125; 测试结果123456DEBUG [main] - Created connection 627318073.DEBUG [main] - Setting autocommit to false on JDBC Connection [com.mysql.cj.jdbc.ConnectionImpl@25641d39]DEBUG [main] - &#x3D;&#x3D;&gt; Preparing: select * from t_employee where id&#x3D;?DEBUG [main] - &#x3D;&#x3D;&gt; Parameters: 1(Integer)DEBUG [main] - &lt;&#x3D;&#x3D; Total: 1Employee [id&#x3D;1, empname&#x3D;admin, email&#x3D;123456789@qq.com, gender&#x3D;0] 1.3.4 利用MyBatis实现对数据库的增删改查 包com.lizhi.dao接口EmployeeDao.java1234567891011121314151617package com.lizhi.dao;import com.lizhi.bean.Employee;public interface EmployeeDao &#123; &#x2F;&#x2F;查询 public Employee getEmpById(Integer id); &#x2F;&#x2F;更新 public int updateEmployee(Employee employee); &#x2F;&#x2F;删除 public boolean deleteEmployee(Integer id); &#x2F;&#x2F;插入 public int insertEmployee(Employee employee);&#125; 包com.lizhi.beanEmployee.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package com.lizhi.bean;public class Employee &#123; private Integer id; private String empname; private String email; private Integer gender; &#x2F;&#x2F;写完无参构造器后必须再写个有参的构造器 public Employee() &#123; super(); &#125; public Employee(Integer id, String empname, String email, Integer gender) &#123; super(); this.id &#x3D; id; this.empname &#x3D; empname; this.email &#x3D; email; this.gender &#x3D; gender; &#125; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id &#x3D; id; &#125; public String getEmpname() &#123; return empname; &#125; public void setEmpname(String empname) &#123; this.empname &#x3D; empname; &#125; public String getEmail() &#123; return email; &#125; public void setEmail(String email) &#123; this.email &#x3D; email; &#125; public Integer getGender() &#123; return gender; &#125; public void setGender(Integer gender) &#123; this.gender &#x3D; gender; &#125; @Override public String toString() &#123; return &quot;Employee [id&#x3D;&quot; + id + &quot;, empname&#x3D;&quot; + empname + &quot;, email&#x3D;&quot; + email + &quot;, gender&#x3D;&quot; + gender + &quot;]&quot;; &#125;&#125; EmployeeDao.xml(EmployeeDao的具体实现方法)1234567891011121314151617181920212223242526272829&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-&#x2F;&#x2F;mybatis.org&#x2F;&#x2F;DTD Mapper 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;mybatis.org&#x2F;dtd&#x2F;mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace&#x3D;&quot;com.lizhi.dao.EmployeeDao&quot;&gt; &lt;!-- public Employee getEmpById(Integer id); 方法的实现;参数类型不用写--&gt; &lt;select id&#x3D;&quot;getEmpById&quot; resultType&#x3D;&quot;com.lizhi.bean.Employee&quot; &gt; &lt;!-- sql语句不要写分号 --&gt; select * from t_employee where id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; &lt;!-- public int updateEmployee(Employee employee);方法的实现 ;;增删改不用写返回值类型，增删改是返回影响多少行 mybatis自动判断，如果是数字(int,long),如果是boolean(影响0行自动封装false,否则true)。#&#123;属性名&#125;，从传入参数对象中取出对应属性值--&gt; &lt;update id&#x3D;&quot;updateEmployee&quot; &gt; update t_employee set empname&#x3D;#&#123;empname&#125;, gender&#x3D;#&#123;gender&#125; ,email&#x3D;#&#123;email&#125; where id&#x3D;#&#123;id&#125; &lt;&#x2F;update&gt;&lt;!-- public int deleteEmployee(Integer id); --&gt; &lt;delete id&#x3D;&quot;deleteEmployee&quot;&gt; DELETE FROM t_employee WHERE id&#x3D;#&#123;id&#125; &lt;&#x2F;delete&gt;&lt;!-- public int insertEmployee(Employee employee); --&gt; &lt;insert id&#x3D;&quot;insertEmployee&quot;&gt; INSERT INTO t_employee(empname,gender,email) VALUES(#&#123;empname&#125;,#&#123;gender&#125;,#&#123;email&#125;) &lt;&#x2F;insert&gt; &lt;&#x2F;mapper&gt; log4j.propertieslog4j.properties log4j.xmllog4j.xml mybatis-config.xml123456789101112131415161718192021222324&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-&#x2F;&#x2F;mybatis.org&#x2F;&#x2F;DTD Config 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;mybatis.org&#x2F;dtd&#x2F;mybatis-3-config.dtd&quot;&gt;&lt;configuration&gt; &lt;environments default&#x3D;&quot;development&quot;&gt; &lt;environment id&#x3D;&quot;development&quot;&gt; &lt;transactionManager type&#x3D;&quot;JDBC&quot;&#x2F;&gt; &lt;!-- 配置连接池 --&gt; &lt;dataSource type&#x3D;&quot;POOLED&quot;&gt; &lt;property name&#x3D;&quot;driver&quot; value&#x3D;&quot;com.mysql.cj.jdbc.Driver&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;url&quot; value&#x3D;&quot;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;mybatis_0325?serverTimezone&#x3D;GMT%2B8&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;username&quot; value&#x3D;&quot;root&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;password&quot; value&#x3D;&quot;******&quot;&#x2F;&gt; &lt;&#x2F;dataSource&gt; &lt;&#x2F;environment&gt; &lt;&#x2F;environments&gt; &lt;!-- 引入我们自己编写的每一个接口的实现文件 --&gt; &lt;mappers&gt; &lt;!-- resource: 表示从类路径下找资源 --&gt; &lt;mapper resource&#x3D;&quot;mybatis&#x2F;EmployeeDao.xml&quot;&#x2F;&gt; &lt;&#x2F;mappers&gt;&lt;&#x2F;configuration&gt; 包com.lizhi.testMybatisCRUDTest.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125package com.lizhi.test;import java.io.IOException;import java.io.InputStream;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.jupiter.api.Test;import com.lizhi.bean.Employee;import com.lizhi.dao.EmployeeDao;public class MybatisCRUDTest &#123; &#x2F;&#x2F;工厂一个 SqlSessionFactory sqlSessionFactory; public void initSqlSessionFactory() throws IOException &#123; &#x2F;&#x2F;1.跟据全局配置文件先得到SqlSessionFactory对象(官方文档复制)，快捷键Ctrl+shift+O导包 String resource &#x3D; &quot;mybatis-config.xml&quot;; InputStream inputStream &#x3D; Resources.getResourceAsStream(resource); sqlSessionFactory &#x3D; new SqlSessionFactoryBuilder().build(inputStream); &#125; &#x2F;** * 更新 * @throws IOException *&#x2F; @Test public void testupdateEmployee() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(true); try &#123; EmployeeDao employeeDao &#x3D; openSession.getMapper(EmployeeDao.class); int rows &#x3D; employeeDao.updateEmployee(new Employee(4 , &quot;lizhi&quot; ,&quot;lizhi@qq.com&quot;, 0 )); if (rows &gt; 0) &#123; System.out.println(&quot;您成功修改了&quot;+rows+&quot;条数据！&quot;); &#125; else &#123; System.out.println(&quot;修改失败！&quot;); &#125; &#125; finally &#123; openSession.close(); &#125; &#125; &#x2F;** * 删除 * @throws IOException *&#x2F; @Test public void testdeleteEmployee() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(true); try &#123; EmployeeDao employee &#x3D; openSession.getMapper(EmployeeDao.class); employee.deleteEmployee(3);&#x2F;&#x2F;id &#125; finally &#123; openSession.close(); &#125; &#125; &#x2F;** * 测试插入 * @throws IOException *&#x2F; @Test public void testInsert() throws IOException &#123; initSqlSessionFactory(); &#x2F;&#x2F;1.获取和数据库的一次会话 &#x2F;&#x2F;true是代表自动提交，不用手动提交 SqlSession openSession &#x3D; sqlSessionFactory.openSession(true); try &#123; &#x2F;&#x2F;2.获取到接口的映射器 EmployeeDao mapper &#x3D; openSession.getMapper(EmployeeDao.class); &#x2F;&#x2F;3.测试 int i &#x3D; mapper.insertEmployee(new Employee(null , &quot;tomcat&quot; ,&quot;tomcat@qq.com&quot;, 0 )); if (i&gt;0) &#123; System.out.println(&quot;您成功插入了&quot;+i+&quot;条数据！&quot;); &#125; else &#123; System.out.println(&quot;插入失败！&quot;); &#125; &#125; finally &#123; &#x2F;&#x2F;手动提交 &#x2F;&#x2F;openSession.commit(); &#x2F;&#x2F;关闭连接 openSession.close(); &#125; &#125; &#x2F;** * 测试查询 * @throws IOException *&#x2F; @Test public void test() throws IOException &#123; &#x2F;&#x2F;1.跟据全局配置文件先得到SqlSessionFactory initSqlSessionFactory(); &#x2F;&#x2F;2.得到SqlSession对象 SqlSession openSession &#x3D; sqlSessionFactory.openSession(); &#x2F;&#x2F;3.获取到dao接口的实现类(映射器) EmployeeDao dao &#x3D; openSession.getMapper(EmployeeDao.class); try &#123; Employee employee &#x3D; dao.getEmpById(1); System.out.println(employee); &#125;finally &#123; openSession.close(); &#125; &#125;&#125; 1.4.3 配置文件详解测试实例源码 package com.lizhi.dao;接口EmployeeDao。java1234567891011121314151617package com.lizhi.dao;import com.lizhi.bean.Employee;public interface EmployeeDao &#123; &#x2F;&#x2F;查询 public Employee getEmpById(Integer id); &#x2F;&#x2F;更新 public int updateEmployee(Employee employee); &#x2F;&#x2F;删除 public boolean deleteEmployee(Integer id); &#x2F;&#x2F;插入 public int insertEmployee(Employee employee); &#125; EmployeeDaoAnnotation.java1234567891011121314151617181920212223242526272829package com.lizhi.dao;import org.apache.ibatis.annotations.Delete;import org.apache.ibatis.annotations.Insert;import org.apache.ibatis.annotations.Select;import org.apache.ibatis.annotations.Update;import com.lizhi.bean.Employee;&#x2F;* * 注解方法配置增删改查，虽然方便，但是不便于维护 * *&#x2F;public interface EmployeeDaoAnnotation &#123; &#x2F;&#x2F;查询 @Select(&quot;select * from t_employee where id&#x3D;#&#123;id&#125;&quot;) public Employee getEmpById(Integer id); &#x2F;&#x2F;更新 @Update(&quot;update t_employee set empname&#x3D;#&#123;empname&#125;, gender&#x3D;#&#123;gender&#125; ,email&#x3D;#&#123;email&#125; where id&#x3D;#&#123;id&#125;&quot;) public int updateEmployee(Employee employee); &#x2F;&#x2F;删除 @Delete(&quot;DELETE FROM t_employee WHERE id&#x3D;#&#123;id&#125;&quot;) public boolean deleteEmployee(Integer id); &#x2F;&#x2F;插入 @Insert(&quot;INSERT INTO t_employee(empname,gender,email) VALUES(#&#123;empname&#125;,#&#123;gender&#125;,#&#123;email&#125;)&quot;) public int insertEmployee(Employee employee); &#125; Employee.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.lizhi.bean;public class Employee &#123; private Integer id; private String empname; private String email; private Integer gender; private String loginAccount; public String getLoginAccount() &#123; return loginAccount; &#125; public void setLoginAccount(String loginAccount) &#123; this.loginAccount &#x3D; loginAccount; &#125; &#x2F;&#x2F;写完无参构造器后必须再写个有参的构造器 public Employee() &#123; super(); &#125; public Employee(Integer id, String empname, String email, Integer gender) &#123; super(); this.id &#x3D; id; this.empname &#x3D; empname; this.email &#x3D; email; this.gender &#x3D; gender; &#125; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id &#x3D; id; &#125; public String getEmpname() &#123; return empname; &#125; public void setEmpname(String empname) &#123; this.empname &#x3D; empname; &#125; public String getEmail() &#123; return email; &#125; public void setEmail(String email) &#123; this.email &#x3D; email; &#125; public Integer getGender() &#123; return gender; &#125; public void setGender(Integer gender) &#123; this.gender &#x3D; gender; &#125; @Override public String toString() &#123; return &quot;Employee [id&#x3D;&quot; + id + &quot;, empname&#x3D;&quot; + empname + &quot;, email&#x3D;&quot; + email + &quot;, gender&#x3D;&quot; + gender + &quot;, loginAccount&#x3D;&quot; + loginAccount + &quot;]&quot;; &#125;&#125; EmployeeDao.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-&#x2F;&#x2F;mybatis.org&#x2F;&#x2F;DTD Mapper 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;mybatis.org&#x2F;dtd&#x2F;mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace&#x3D;&quot;com.lizhi.dao.EmployeeDao&quot;&gt; &lt;!-- public Employee getEmpById(Integer id); 方法的实现;参数类型不用写--&gt; &lt;!-- 默认这个查询是不区分环境 --&gt; &lt;select id&#x3D;&quot;getEmpById&quot; resultType&#x3D;&quot;com.lizhi.bean.Employee&quot; &gt; &lt;!-- sql语句不要写分号 --&gt; select * from t_employee where id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; &lt;!-- 如果能精确匹配，不能就用模糊的 &lt;select id&#x3D;&quot;getEmpById&quot; resultType&#x3D;&quot;com.lizhi.bean.Employee&quot; databaseId&#x3D;&quot;mysql&quot;&gt; select * from t_employee where id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; &lt;select id&#x3D;&quot;getEmpById&quot; resultType&#x3D;&quot;com.lizhi.bean.Employee&quot; databaseId&#x3D;&quot;oracle&quot;&gt; select * from t_employee where id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; &lt;select id&#x3D;&quot;getEmpById&quot; resultType&#x3D;&quot;com.lizhi.bean.Employee&quot; databaseId&#x3D;&quot;sqlserver&quot;&gt; select * from t_employee where id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; --&gt; &lt;!-- public int updateEmployee(Employee employee);方法的实现 ;;增删改不用写返回值类型，增删改是返回影响多少行 mybatis自动判断，如果是数字(int,long),如果是boolean(影响0行自动封装false,否则true)。#&#123;属性名&#125;，从传入参数对象中取出对应属性值--&gt; &lt;update id&#x3D;&quot;updateEmployee&quot; &gt; update t_employee set empname&#x3D;#&#123;empname&#125;, gender&#x3D;#&#123;gender&#125; ,email&#x3D;#&#123;email&#125; where id&#x3D;#&#123;id&#125; &lt;&#x2F;update&gt; &lt;!-- public int deleteEmployee(Integer id); --&gt; &lt;delete id&#x3D;&quot;deleteEmployee&quot;&gt; DELETE FROM t_employee WHERE id&#x3D;#&#123;id&#125; &lt;&#x2F;delete&gt; &lt;!-- public int insertEmployee(Employee employee); --&gt; &lt;insert id&#x3D;&quot;insertEmployee&quot;&gt; INSERT INTO t_employee(empname,gender,email) VALUES(#&#123;empname&#125;,#&#123;gender&#125;,#&#123;email&#125;) &lt;&#x2F;insert&gt; &lt;&#x2F;mapper&gt; dbconfig.properties1234username&#x3D;rootpassword&#x3D;******driverclass&#x3D;com.mysql.cj.jdbc.Driverjdbcurl&#x3D;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;mybatis_0325?serverTimezone&#x3D;GMT%2B8&amp;useSSL&#x3D;false log4j.propertieslog4j.properties log4j.xmllog4j.xml mybatis-config.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-&#x2F;&#x2F;mybatis.org&#x2F;&#x2F;DTD Config 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;mybatis.org&#x2F;dtd&#x2F;mybatis-3-config.dtd&quot;&gt;&lt;configuration&gt;&lt;!-- 1. 和Spring的context,property-placeholder;引用外部配置文件 --&gt; &lt;!-- resource : 从类路径下开始引用 url: 引用磁盘路径文件或者网络路径的资源 --&gt; &lt;properties resource&#x3D;&quot;dbconfig.properties&quot;&gt;&lt;&#x2F;properties&gt; &lt;!-- 2. settings这是 MyBatis 中极为重要的调整设置，它们会改变 MyBatis 的运行时行为。 详见文档 --&gt; &lt;!-- mapUnderscoreToCamelCase: 是否开启自动驼峰命名规则（camel case）映射，即从经典数据库列名 A_COLUMN 到经典 Java 属性名 aColumn 的类似映射。 即在接口Employee中，用驼峰命名可以和数据库中用下划线命名的列名相对应,javaBean 可以自动对应 --&gt; &lt;settings&gt; &lt;!-- name: 配置项的key value:配置项的值 --&gt; &lt;!-- 数据库不区分大小写 loginAcount login_account --&gt; &lt;setting name&#x3D;&quot;mapUnderscoreToCamelCase&quot; value&#x3D;&quot;true&quot;&#x2F;&gt; &lt;&#x2F;settings&gt; &lt;!-- 3.类型别名，为常用的类型（JavaBean）起别名 --&gt; &lt;typeAliases&gt; &lt;!-- typeAlias: 就是为一个javaBean起别名；别名默认就是类名（不区分大小写），配置文件中就可以用别名了 --&gt; &lt;!-- 在EmployeeDao.xml中&lt;select id&#x3D;&quot;getEmpById&quot; resultType&#x3D;&quot;Emp&quot; &gt;标签就可以直接用Employee代替简化开发 alias&#x3D;&quot;&quot;指定一个别名如果不指定就是默认Employee--&gt; &lt;!-- &lt;typeAlias type&#x3D;&quot;com.lizhi.bean.Employee&quot; alias&#x3D;&quot;emp&quot;&#x2F;&gt;--&gt; &lt;!-- 批量起别名 ，name&#x3D;&quot;&quot;指定包名，为这个包下的所有类起别名,默认别名就是类名 也可以在指定的类上面添加@Alias(&quot;&quot;)注解的方式指定别名 --&gt; &lt;!-- &lt;package name&#x3D;&quot;com.lizhi.bean&quot;&#x2F;&gt;--&gt; &lt;!-- 推荐使用全类名 --&gt; &lt;&#x2F;typeAliases&gt;&lt;!-- 4.类型处理器 --&gt; &lt;!-- &lt;typeHandlers&gt; 自定义好的类型处理器就这么配置上了 &lt;typeHandler handler&#x3D;&quot;&quot;&#x2F;&gt; &lt;&#x2F;typeHandlers&gt; --&gt;&lt;!-- 5.插件是mybatis中的一个强大功能 --&gt; &lt;!-- 6.environments配置环境 environment: 配置一个具体的环境。都需要一个事务管理器和一个数据源。 可以配置多个environments，id是当前环境的唯一标示 default&#x3D;&quot;development&quot; : 默认使用哪个环境 后来数据源和事务管理都是Spring来做 --&gt; &lt;environments default&#x3D;&quot;development&quot;&gt; &lt;environment id&#x3D;&quot;development&quot;&gt; &lt;transactionManager type&#x3D;&quot;JDBC&quot;&#x2F;&gt; &lt;!-- 配置连接池 POOLED: 连接池--&gt; &lt;dataSource type&#x3D;&quot;POOLED&quot;&gt; &lt;!-- $&#123;&#125;取出配置值 --&gt; &lt;property name&#x3D;&quot;driver&quot; value&#x3D;&quot;$&#123;driverclass&#125;&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;url&quot; value&#x3D;&quot;$&#123;jdbcurl&#125;&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;username&quot; value&#x3D;&quot;$&#123;username&#125;&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;password&quot; value&#x3D;&quot;$&#123;password&#125;&quot;&#x2F;&gt; &lt;&#x2F;dataSource&gt; &lt;&#x2F;environment&gt; &lt;&#x2F;environments&gt; &lt;!-- 7.mybatis用来考虑数据库移植性的 ,例如ODBC和JDBC的语法不同，当切换数据库的时候会报错 type&#x3D;&quot;DB_VENDOR&quot;是固定的值--&gt; &lt;databaseIdProvider type&#x3D;&quot;DB_VENDOR&quot;&gt; &lt;!-- name&#x3D;&quot;&quot;:数据库厂商标识，value&#x3D;&quot;&quot;:给这个标识起一个好用的名字 MySQL的标识：MySQL Oracle的标识：Oracle SQL Server的标识：SQL Server --&gt; &lt;property name&#x3D;&quot;MySQL&quot; value&#x3D;&quot;mysql&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;Oracle&quot; value&#x3D;&quot;oracle&quot;&#x2F;&gt; &lt;property name&#x3D;&quot;SQL Server&quot; value&#x3D;&quot;sqlserver&quot;&#x2F;&gt; &lt;&#x2F;databaseIdProvider&gt; &lt;!-- 8.写好的Sql映射文件需要使用mappers注册进来 --&gt; &lt;mappers&gt; &lt;!-- url: 可以从磁盘或者网络路径引用 resource: 在类路径下找sql映射文件 class: 直接引用接口的全类名，可以将接口的实例化配置文件放在和接口同一个包下，且文件名和接口名一致 class的另一种方法； --&gt; &lt;!-- &lt;mapper resource&#x3D;&quot;mybatis&#x2F;EmployeeDao.xml&quot;&#x2F;&gt; --&gt; &lt;!-- &lt;mapper class&#x3D;&quot;com.lizhi.dao.EmployeeDao&quot;&#x2F;&gt; --&gt; &lt;!-- &lt;mapper class&#x3D;&quot;com.lizhi.dao.EmployeeDaoAnnotation&quot;&#x2F;&gt; --&gt; &lt;!-- 常规的配置方法和注解方法配合使用 重要的Dao可以写配置，简单的Dao就直接写注解 --&gt; &lt;!-- 批量注册name&#x3D;&quot;&quot;,Dao所在包名 这里有个问题；批量扫描只能扫描指定的包名，而不能扫描位于conf&#x2F;mybatis&#x2F;EmployeeDao.xml路径下的 EmployeeDao文件，这时将包名mybatis改成com.lizhi.dao虽然看起来是两个包，但是从类路径来看是一个包 因此都可以扫描 --&gt; &lt;package name&#x3D;&quot;com.lizhi.dao&quot;&#x2F;&gt; &lt;&#x2F;mappers&gt;&lt;&#x2F;configuration&gt; MybatisCRUDTest.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151package com.lizhi.test;import java.io.IOException;import java.io.InputStream;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.Before;import org.junit.jupiter.api.Test;import com.lizhi.bean.Employee;import com.lizhi.dao.EmployeeDao;import com.lizhi.dao.EmployeeDaoAnnotation;public class MybatisCRUDTest &#123; &#x2F;&#x2F;工厂一个 SqlSessionFactory sqlSessionFactory; @Before public void initSqlSessionFactory() throws IOException &#123; &#x2F;&#x2F;1.跟据全局配置文件先得到SqlSessionFactory对象(官方文档复制)，快捷键Ctrl+shift+O导包 String resource &#x3D; &quot;mybatis-config.xml&quot;; InputStream inputStream &#x3D; Resources.getResourceAsStream(resource); sqlSessionFactory &#x3D; new SqlSessionFactoryBuilder().build(inputStream); &#125; &#x2F;* * * 测试注解方法sql查询 EmployeeDaoAnnotation.java * mybatis-config里配置 * &lt;mapper class&#x3D;&quot;com.lizhi.dao.EmployeeDaoAnnotation&quot;&#x2F;&gt; *&#x2F; @Test public void testAnnotation() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; EmployeeDaoAnnotation mapper &#x3D; openSession.getMapper(EmployeeDaoAnnotation.class); Employee empById &#x3D; mapper.getEmpById(1); System.out.println(empById); &#125; catch (Exception e) &#123; &#x2F;&#x2F; TODO Auto-generated catch block openSession.close(); &#125; &#125; &#x2F;** * 更新 * @throws IOException * 如果运行不成功看配置文件里的mapper有没有注册 *&#x2F; @Test public void testupdateEmployee() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(true); try &#123; EmployeeDao employeeDao &#x3D; openSession.getMapper(EmployeeDao.class); int rows &#x3D; employeeDao.updateEmployee(new Employee(4 , &quot;lizhi&quot; ,&quot;lizhi@qq.com&quot;, 0 )); if (rows &gt; 0) &#123; System.out.println(&quot;您成功修改了&quot;+rows+&quot;条数据！&quot;); &#125; else &#123; System.out.println(&quot;修改失败！&quot;); &#125; &#125; finally &#123; openSession.close(); &#125; &#125; &#x2F;** * 删除 * @throws IOException *&#x2F; @Test public void testdeleteEmployee() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(true); try &#123; EmployeeDao employee &#x3D; openSession.getMapper(EmployeeDao.class); employee.deleteEmployee(3);&#x2F;&#x2F;id &#125; finally &#123; openSession.close(); &#125; &#125; &#x2F;** * 测试插入 * @throws IOException *&#x2F; @Test public void testInsert() throws IOException &#123; initSqlSessionFactory(); &#x2F;&#x2F;1.获取和数据库的一次会话 &#x2F;&#x2F;true是代表自动提交，不用手动提交 SqlSession openSession &#x3D; sqlSessionFactory.openSession(true); try &#123; &#x2F;&#x2F;2.获取到接口的映射器 EmployeeDao mapper &#x3D; openSession.getMapper(EmployeeDao.class); &#x2F;&#x2F;3.测试 int i &#x3D; mapper.insertEmployee(new Employee(null , &quot;tomcat&quot; ,&quot;tomcat@qq.com&quot;, 0 )); if (i&gt;0) &#123; System.out.println(&quot;您成功插入了&quot;+i+&quot;条数据！&quot;); &#125; else &#123; System.out.println(&quot;插入失败！&quot;); &#125; &#125; finally &#123; &#x2F;&#x2F;手动提交 &#x2F;&#x2F;openSession.commit(); &#x2F;&#x2F;关闭连接 openSession.close(); &#125; &#125; &#x2F;** * 测试查询 * @throws IOException *&#x2F; @Test public void test() throws IOException &#123; &#x2F;&#x2F;1.跟据全局配置文件先得到SqlSessionFactory initSqlSessionFactory(); &#x2F;&#x2F;2.得到SqlSession对象 SqlSession openSession &#x3D; sqlSessionFactory.openSession(); &#x2F;&#x2F;3.获取到dao接口的实现类(映射器) EmployeeDao dao &#x3D; openSession.getMapper(EmployeeDao.class); try &#123; Employee employee &#x3D; dao.getEmpById(1); System.out.println(employee); &#125;finally &#123; openSession.close(); &#125; &#125;&#125; 1.4.2.2 传参问题测试实例源码 package com.lizhi.dao;EmployeeDao.java1234567891011121314151617181920212223242526272829303132333435package com.lizhi.dao;import java.util.List;import java.util.Map;import org.apache.ibatis.annotations.MapKey;import org.apache.ibatis.annotations.Param;import com.lizhi.bean.Employee;public interface EmployeeDao &#123; &#x2F;&#x2F;查询 public Employee getEmpById(Integer id); public Employee getEmpByIdAndEmpname(@Param(&quot;id&quot;)Integer id,@Param(&quot;empname&quot;)String empname); public Employee getEmployeeAndEmpname(Map&lt;String, Object&gt;map); &#x2F;&#x2F;查询所有员工 public List&lt;Employee&gt; getAllEmps(); &#x2F;&#x2F;查询返回map，默认会把列名作为Key,值作为Value public Map&lt;String, Object&gt; getEmpByIdReturnMap(Integer id); &#x2F;&#x2F;查询多条信息,key就是记录的主键，value就是这条记录封装好的对象\\@MapKey(&quot;id&quot;)指定用查询出的id的值作为key封装查询的信息 @MapKey(&quot;id&quot;) public Map&lt;Integer, Employee&gt; getEmpsReturnMap(); &#x2F;&#x2F;更新 public int updateEmployee(Employee employee); &#x2F;&#x2F;删除 public boolean deleteEmployee(Integer id); &#x2F;&#x2F;插入 public int insertEmployee(Employee employee); public int insertEmployee2(Employee employee);&#125; CatDao.java1234567891011package com.lizhi.dao;import com.lizhi.bean.Cat;public interface CatDao &#123; public Cat getCatById(Integer id); &#125; package com.lizhi.beanEmployee.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package com.lizhi.bean;public class Employee &#123; private Integer id; private String empname; private String email; private Integer gender; private String loginAccount; public String getLoginAccount() &#123; return loginAccount; &#125; public void setLoginAccount(String loginAccount) &#123; this.loginAccount &#x3D; loginAccount; &#125; &#x2F;&#x2F;写完无参构造器后必须再写个有参的构造器 public Employee() &#123; super(); &#125; public Employee(Integer id, String empname, String email, Integer gender) &#123; super(); this.id &#x3D; id; this.empname &#x3D; empname; this.email &#x3D; email; this.gender &#x3D; gender; &#125; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id &#x3D; id; &#125; public String getEmpname() &#123; return empname; &#125; public void setEmpname(String empname) &#123; this.empname &#x3D; empname; &#125; public String getEmail() &#123; return email; &#125; public void setEmail(String email) &#123; this.email &#x3D; email; &#125; public Integer getGender() &#123; return gender; &#125; public void setGender(Integer gender) &#123; this.gender &#x3D; gender; &#125; @Override public String toString() &#123; return &quot;Employee [id&#x3D;&quot; + id + &quot;, empname&#x3D;&quot; + empname + &quot;, email&#x3D;&quot; + email + &quot;, gender&#x3D;&quot; + gender + &quot;, loginAccount&#x3D;&quot; + loginAccount + &quot;]&quot;; &#125; &#125; Cat.java1234567891011121314151617181920212223242526272829303132333435363738394041package com.lizhi.bean;public class Cat &#123; private Integer id; private String name; private Integer gender; private Integer age; public Integer getId() &#123; return id; &#125; public void setId(Integer id) &#123; this.id &#x3D; id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name &#x3D; name; &#125; public Integer getGender() &#123; return gender; &#125; public void setGender(Integer gender) &#123; this.gender &#x3D; gender; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age &#x3D; age; &#125; @Override public String toString() &#123; return &quot;cat [id&#x3D;&quot; + id + &quot;, name&#x3D;&quot; + name + &quot;, gender&#x3D;&quot; + gender + &quot;, age&#x3D;&quot; + age + &quot;]&quot;; &#125; &#125; /4.MyBatis_sqlMapper/conf/com/lizhi/dao/CatDao.xml123456789101112131415161718192021222324252627282930313233343536&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-&#x2F;&#x2F;mybatis.org&#x2F;&#x2F;DTD Mapper 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;mybatis.org&#x2F;dtd&#x2F;mybatis-3-mapper.dtd&quot;&gt; &lt;mapper namespace&#x3D;&quot;com.lizhi.dao.CatDao&quot;&gt;&lt;!-- public cat getCatById(Integer id); --&gt; &lt;!-- resultType&#x3D;&quot;com.lizhi.bean.Cat&quot;:使用默认规则，属性名列名一一对应 resultMap&#x3D;&quot;myCat&quot;:查出数据封装结果的时候，使用myCat自定义的结果集， --&gt; &lt;select id&#x3D;&quot;getCatById&quot; resultMap&#x3D;&quot;myCat&quot;&gt; &lt;!-- 这样可以用起别名的方法配合resultType&#x3D;&quot;com.lizhi.bean.Cat&quot;:使用默认规则，属性名列名一一对应 select id,cname name,cAge age,cgender gender from t_cat where id&#x3D;#&#123;id&#125; --&gt; &lt;!-- 这样不可以用 --&gt; select * from t_cat where id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; &lt;!-- 自定义结果集(resultMap)，自己定义每一列数据和JavaBean的映射规则 type&#x3D;&quot;&quot; :制定要为哪个JavaBean自定义封装规则,全类名 id&#x3D;&quot;&quot;:唯一标识 --&gt; &lt;resultMap type&#x3D;&quot;com.lizhi.bean.Cat&quot; id&#x3D;&quot;myCat&quot;&gt; &lt;!-- 指定主键列的对应规则 column&#x3D;&quot;id&quot;:指定那一列是主键列 property&#x3D;&quot;&quot;:指定Cat.java文件里的哪个属性封装id这一列数据 --&gt; &lt;id property&#x3D;&quot;id&quot; column&#x3D;&quot;id&quot;&#x2F;&gt; &lt;!-- 普通列 --&gt; &lt;result property&#x3D;&quot;name&quot; column&#x3D;&quot;cName&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;age&quot; column&#x3D;&quot;cAge&quot;&#x2F;&gt; &lt;result property&#x3D;&quot;gender&quot; column&#x3D;&quot;cgender&quot;&#x2F;&gt; &lt;association property&#x3D;&quot;&quot;&gt;&lt;&#x2F;association&gt; &lt;&#x2F;resultMap&gt;&lt;&#x2F;mapper&gt; /4.MyBatis_sqlMapper/conf/com/lizhi/dao/EmployeeDao.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-&#x2F;&#x2F;mybatis.org&#x2F;&#x2F;DTD Mapper 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;mybatis.org&#x2F;dtd&#x2F;mybatis-3-mapper.dtd&quot;&gt; &lt;!-- namespace&#x3D;&quot;&quot;:写dao接口的全类名 --&gt; &lt;mapper namespace&#x3D;&quot;com.lizhi.dao.EmployeeDao&quot;&gt; &lt;!-- 这个文件中能写能写的所有标签 cache:和缓存有关cache-ref:和缓存有关delete,update,inster,select:增删改查resultmap:结果映射，自定义结果集的封装规则sql:抽取可重用的sqldelete,update,inster,select:增删改查里的属性--&gt; &lt;!-- 传参到底能传哪些 --&gt; &lt;!-- public Employee getEmpById(Integer id); --&gt; &lt;select id&#x3D;&quot;getEmpById&quot; resultType&#x3D;&quot;com.lizhi.bean.Employee&quot;&gt; SELECT * FROM t_employee WHERE id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt;&lt;!--如果采用没有采用@param的方式 id&#x3D;#&#123;id&#125; and empname&#x3D;#&#123;empname&#125;的方式来传入参数会报如下异常 org.apache.ibatis.exceptions.PersistenceException: ### Error querying database. Cause: org.apache.ibatis.binding.BindingException: Parameter &#39;id&#39; not found. Available parameters are [arg1, arg0, param1, param2]而改成id&#x3D;#&#123;param1&#125; and empname&#x3D;#&#123;param2&#125;或者改成arg1, arg0则可运行（param1, param2，param3, param4） --&gt;&lt;!-- public Employee getEmpByIdAndEmpname(Integer id,String empname); --&gt; &lt;select id&#x3D;&quot;getEmpByIdAndEmpname&quot; resultType&#x3D;&quot;com.lizhi.bean.Employee&quot;&gt; select * from t_employee where id&#x3D;#&#123;id&#125; and empname&#x3D;#&#123;empname&#125; &lt;&#x2F;select&gt; &lt;!-- public Employee getEmployeeAndEmpname(Map&lt;String, Override&gt;map); --&gt; &lt;select id&#x3D;&quot;getEmployeeAndEmpname&quot; resultType&#x3D;&quot;com.lizhi.bean.Employee&quot;&gt; select * from $&#123;tablename&#125; where id&#x3D;#&#123;id&#125; and empname&#x3D;#&#123;empname&#125; &lt;&#x2F;select&gt; &lt;!-- 现象： 1.传入单个参数： ①基本类型： 取值：#&#123;随便写&#125; ②传入pojo: 2.多个参数： public Employee getEmpByIdAndEmpname(Integer id,String empname); 取值：#&#123;参数名&#125;的方式无效 可以用id&#x3D;#&#123;param1&#125; and empname&#x3D;#&#123;param2&#125;或者改成arg1, arg0的方式 原因：只要传入了多个参数MyBatis会自动的将这些参数封装在map中，封装是使用的key就是参数的索引和参数的第几个表示 Map&lt;String,Object&gt; map &#x3D; new HashMap&lt;&gt;(); map.put(&quot;arg1&quot;,传入的值);map.put(&quot;arg1&quot;,&quot;传入的值&quot;)； #&#123;Key&#125;就是从map中取值 3.@param:为参数指定key;命名参数；推荐使用此方法 我们可以告诉MyBatis，封装参数map的时候别乱来，使用我们指定的Key public Employee getEmpByIdAndEmpname(@Param(&quot;id&quot;)Integer id,@Param(&quot;empname&quot;)String empname); 4.传入pojo(javaBean) 取值：#&#123;pojo的属性名&#125; 5.传入map:将多个要使用的参数封装起来 取值：#&#123;key&#125; 扩展：多个参数：自动封装map;多种方式混合 method01(@param(&quot;id&quot;)Integer id,String empname,Employee employee); Integwe id -&gt;#&#123;id&#125; String empname -&gt; #&#123;param2&#125; Employee employee (取出这个里面的email) -&gt;#&#123;param3.email&#125; 无论传入什么参数都要能正确的取出值； #&#123;key&#x2F;属性名&#125; 1. #&#123;key&#125;取值的时候可以设置一些规则 id&#x3D;#&#123;id,jdbcType&#x3D;INTEFER&#125;都是自动配置的所以可以不写 javaType,jdbcType,mode,numericScale,resultMap.typeHandler,jdbcTypeName,expression 只有jdbcType才可能是需要被指定的，默认不指定jdbcType，mysql没问题，oracle没问题， 万一传入的数据是null,mysql插入null没问题，【oracle不指定null到底是什么类型】 实际上在MyBatis中，有两种取值方式： #&#123;属性名&#125;: 是参数预编译的方式，参数的位置都是用？替代，参数后来都是预编译设置进去的；安全，不会有sql注入的问题 $&#123;属性名&#125;: 不是参数预编译，而是直接和sql语句进行拼串；不安全，就是将参数直接放在指定的位置运行 &#x2F;&#x2F;id&#x3D; &#39;传入的字符串&#39; and empname&#x3D; 如果传入&#39;1 or 1&#x3D;1 or&#39;，则不管and后面有没有东西or 后面的 1&#x3D;1永远是true，就会通过验证访问数据库的内容，这就是sql注入 $&#123;属性名&#125;用处：sql语句只有参数位置是支持预编译的,表名等不支持预编译；假设有一个数据库里面有两张日志表 log_2019_11,log_2019_12 select * from log_2019_11 where id&#x3D;? and empname&#x3D;? 需求，id,empname固定查询不同日志表里的日志 解决办法：在getEmployeeAndEmpname方法里测试 采取这种方法可行select * from $&#123;tablename&#125; where id&#x3D;#&#123;id&#125; and empname&#x3D;#&#123;empname&#125; 在getEmployeeAndEmpname方法里测试 id&#x3D;$&#123;id&#125; and empname&#x3D;#&#123;empname&#125; 打印的日志 select * from t_employee where id&#x3D;1 and empname&#x3D;? id&#x3D;#&#123;id&#125; and empname&#x3D;#&#123;empname&#125; 打印的日志 select * from t_employee where id&#x3D;? and empname&#x3D;? 一般使用#&#123;&#125;因为安全，在不支持参数预编译的位置要进行取值就使用$&#123;&#125;;--&gt;&lt;!-- 查询所有员工public List&lt;Employee&gt; getAllEmps(); --&gt; &lt;!-- resultType&#x3D;&quot;&quot;如果返回的是个集合，写的是集合里面的元素的类型 --&gt; &lt;select id&#x3D;&quot;getAllEmps&quot; resultType&#x3D;&quot;com.lizhi.bean.Employee&quot;&gt; select * from t_employee &lt;&#x2F;select&gt;&lt;!-- 查询返回map --&gt; &lt;!-- public Map&lt;String, Object&gt; getEmpByIdReturnMap(Integer id); --&gt; &lt;select id&#x3D;&quot;getEmpByIdReturnMap&quot; resultType&#x3D;&quot;map&quot;&gt; select * from t_employee where id&#x3D;#&#123;id&#125; &lt;&#x2F;select&gt; &lt;!-- 查询多个返回一个map ，查询多个情况下，集合里面写元素类型 public Map&lt;Integer, Employee&gt; getEmpsReturnMap(); --&gt; &lt;select id&#x3D;&quot;getEmpsReturnMap&quot; resultType&#x3D;&quot;com.lizhi.bean.Employee&quot;&gt; select * from t_employee &lt;&#x2F;select&gt; &lt;!-- public int updateEmployee(Employee employee); --&gt; &lt;update id&#x3D;&quot;updateEmployee&quot; &gt; update t_employee set empname&#x3D;#&#123;empname&#125;, gender&#x3D;#&#123;gender&#125; ,email&#x3D;#&#123;email&#125; where id&#x3D;#&#123;id&#125; &lt;&#x2F;update&gt;&lt;!-- public boolean deleteEmployee(Integer id); --&gt; &lt;delete id&#x3D;&quot;deleteEmployee&quot;&gt; DELETE FROM t_employee WHERE id&#x3D;#&#123;id&#125; &lt;&#x2F;delete&gt;&lt;!-- public int insertEmployee(Employee employee); --&gt; &lt;!-- 让Mybatis自动的将自增的id赋值给传入的employee对象的id的属性 useGeneratedKeys&#x3D;”true”，原生jdbc获取自增主键的方法 keyProperty&#x3D;&quot;&quot;:将刚才自增的id赋值给那个属性 --&gt; &lt;insert id&#x3D;&quot;insertEmployee&quot; useGeneratedKeys&#x3D;&quot;true&quot; keyProperty&#x3D;&quot;id&quot;&gt; INSERT INTO t_employee(empname,gender,email) VALUES(#&#123;empname&#125;,#&#123;gender&#125;,#&#123;email&#125;) &lt;&#x2F;insert&gt;&lt;!-- public int insertEmployee2(Employee employee); --&gt; &lt;insert id&#x3D;&quot;insertEmployee2&quot;&gt; &lt;!-- 对于不支持自增主键的数据库采用查询主键的方式 order&#x3D;&quot;BEFORE&quot;: 在核心sql语句之前先运行一个查询个查询sql查到id,将查到的id赋值给JavaBean的哪个属性； 获取最大的id值再+1赋值给id达到自增的目的 --&gt; &lt;selectKey order&#x3D;&quot;BEFORE&quot; resultType&#x3D;&quot;integer&quot; keyProperty&#x3D;&quot;id&quot;&gt; select max(id)+1 from t_employee &lt;&#x2F;selectKey&gt; INSERT INTO t_employee(id,empname,gender,email) VALUES(#&#123;id&#125;,#&#123;empname&#125;,#&#123;gender&#125;,#&#123;email&#125;) &lt;&#x2F;insert&gt; &lt;&#x2F;mapper&gt; log4j.propertieslog4j.properties log4j.xmllog4j.xml mybatis-config.xmlmybatis-config.xml package com.lizhi.testMyBatisTest.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240package com.lizhi.test;import static org.junit.jupiter.api.Assertions.*;import java.io.IOException;import java.io.InputStream;import java.util.HashMap;import java.util.List;import java.util.Map;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.Before;import org.junit.jupiter.api.Test;import com.lizhi.bean.Employee;import com.lizhi.bean.Cat;import com.lizhi.dao.CatDao;import com.lizhi.dao.EmployeeDao;public class MyBatisTest &#123; &#x2F;&#x2F;工厂一个 SqlSessionFactory sqlSessionFactory; @Before public void initSqlSessionFactory() throws IOException &#123; &#x2F;&#x2F;1.跟据全局配置文件先得到SqlSessionFactory对象(官方文档复制)，快捷键Ctrl+shift+O导包 String resource &#x3D; &quot;mybatis-config.xml&quot;; InputStream inputStream &#x3D; Resources.getResourceAsStream(resource); sqlSessionFactory &#x3D; new SqlSessionFactoryBuilder().build(inputStream); &#125; &#x2F;* * 测试t_cat表，当数据库列名与bean中的cat.java字段不对应时该如何做 * 默认MyBatis自动封装结果集； * 1.按照列名和属性名一一对应的规则且不区分大小写 * 2.如果不一一对应 * ①，开启驼峰命名法（满足驼峰命名规则数据库中是aaa_bbb,bean中是aaaBbb） * ②，起别名，写sql语句时，由于数据库不区分大小写cName和cname是一样 * select id,cname name,cAge age,cgender gender from t_cat where id&#x3D;#&#123;id&#125; *3.使用resultMap自定义结果集 *&#x2F; @Test public void test09() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; CatDao mapper &#x3D; openSession.getMapper(CatDao.class); Cat catById &#x3D; mapper.getCatById(1); System.out.println(catById); &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125; &#x2F;** * 测试不支持自增查询id的方法 * *&#x2F; @Test public void test03() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; EmployeeDao employeeDao &#x3D; openSession.getMapper(EmployeeDao.class); Employee employee &#x3D; new Employee(1,&quot;baba&quot;,&quot;4567218@qq.com&quot;,0); int i &#x3D;employeeDao.insertEmployee(employee); System.out.println(&quot;---&gt;&quot;+i); System.out.println(&quot;刚才插入的id:&quot;+employee.getId()); &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125; &#x2F;** * 测试insert标签的useGeneratedKeys&#x3D;&quot;true&quot; keyProperty&#x3D;&quot;id&quot; * @throws IOException *&#x2F; @Test public void test01() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; EmployeeDao employeeDao &#x3D; openSession.getMapper(EmployeeDao.class); Employee employee &#x3D; new Employee(null,&quot;baba&quot;,&quot;4567218@qq.com&quot;,0); int i &#x3D;employeeDao.insertEmployee(employee); System.out.println(&quot;---&gt;&quot;+i); System.out.println(&quot;刚才插入的id:&quot;+employee.getId()); &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125; @Test public void test() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; EmployeeDao employeeDao &#x3D; openSession.getMapper(EmployeeDao.class); Employee empById &#x3D; employeeDao.getEmpById(1); System.out.println(empById); &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125; &#x2F;* * 测试查询时可以传入什么参数 * 同时查询id和empname * *&#x2F; @Test public void test04() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; EmployeeDao employeeDao &#x3D; openSession.getMapper(EmployeeDao.class); Employee emplyee &#x3D; employeeDao.getEmpByIdAndEmpname(1, &quot;baba&quot;); System.out.println(emplyee); &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125; &#x2F;* * 测试传入map的方式传入多个参数 *&#x2F; @Test public void test05() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; EmployeeDao employeeDao &#x3D; openSession.getMapper(EmployeeDao.class); Map&lt;String, Object&gt; map &#x3D; new HashMap&lt;String, Object&gt;(); map.put(&quot;id&quot;, 1); map.put(&quot;empname&quot;, &quot;admin&quot;); map.put(&quot;tablename&quot;, &quot;t_employee&quot;); Employee emplyee &#x3D; employeeDao.getEmployeeAndEmpname(map); System.out.println(emplyee); &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125; &#x2F;** * 测试传入List集合 *&#x2F; @Test public void test06() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; EmployeeDao employeeDao &#x3D; openSession.getMapper(EmployeeDao.class); List&lt;Employee&gt; allmaps &#x3D; employeeDao.getAllEmps(); for (Employee employee : allmaps) &#123; System.out.println(employee); &#125; System.out.println(); &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125; &#x2F;** * 测试查询map查询单条记录封装map *&#x2F; @Test public void test07() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; EmployeeDao employeeDao &#x3D; openSession.getMapper(EmployeeDao.class); Map&lt;String, Object&gt; map &#x3D; employeeDao.getEmpByIdReturnMap(1); System.out.println(map); &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125; &#x2F;** * 测试查询map查询多条记录封装map *&#x2F; @Test public void test08() throws IOException &#123; initSqlSessionFactory(); SqlSession openSession &#x3D; sqlSessionFactory.openSession(); try &#123; EmployeeDao employeeDao &#x3D; openSession.getMapper(EmployeeDao.class); Map&lt;Integer, Employee&gt; map &#x3D; employeeDao.getEmpsReturnMap(); System.out.println(map); &#x2F;&#x2F;这时打印的是一条记录 Employee employee &#x3D; map.get(1); System.out.println(employee); &#125; finally &#123; &#x2F;&#x2F; TODO: handle finally clause openSession.close(); &#125; &#125;&#125;","categories":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"-ssm -Spring -MyBatis","slug":"ssm-Spring-MyBatis","permalink":"http://www.studyz.club/tags/ssm-Spring-MyBatis/"}]},{"title":"软件开发框架之spring-事务管理","slug":"软件开发框架之spring-事务管理","date":"2019-10-26T11:57:24.541Z","updated":"2019-11-09T08:12:51.204Z","comments":true,"path":"posts/75b1eb49/","link":"","permalink":"http://www.studyz.club/posts/75b1eb49/","excerpt":"","text":"一， 软件开发框架之spring-事务管理 1.1 Spring中的事务管理 1.2 用事务通知声明式地管理事务 1.2.1 用 @Transactional 注解声明式地管理事务 1.3 事务传播属性 1.3.1 Spring支持的事务传播行为 1.3.1.1 REQUIRED 传播行为 1.3.1.2 REQUIRES_NEW 传播行为 1.3.3 在 Spring 2.x 事务通知中配置传播属性 1.3.4 并发事务所导致的问题 1.4 事务的隔离级别 1.4.1 Spring 支持的事务隔离级别 1.4.2 设置隔离事务属性 1.4.3 设置回滚事务属性 1.4.4 超时和只读属性 1.5 以xml的方式管理事务 一， 软件开发框架之spring-事务管理事务简介 事务管理是企业级应用程序开发中必不可少的技术, 用来确保数据的完整性和一致性. 事务就是一系列的动作, 它们被当做一个单独的工作单元. 这些动作要么全部完成, 要么全部不起作用 事务的四个关键属性(ACID) 原子性(atomicity): 事务是一个原子操作, 由一系列动作组成. 事务的原子性确保动作要么全部完成要么完全不起作用. 一致性(consistency): 一旦所有事务动作完成, 事务就被提交. 数据和资源就处于一种满足业务规则的一致性状态中. 隔离性(isolation): 可能有许多事务会同时处理相同的数据, 因此每个事物都应该与其他事务隔离开来, 防止数据损坏. 持久性(durability): 一旦事务完成, 无论发生什么系统错误, 它的结果都不应该受到影响. 通常情况下, 事务的结果被写到持久化存储器中. 事务管理的问题 问题:必须为不同的方法重写类似的样板代码这段代码是特定于 JDBC 的, 一旦选择类其它数据库存取技术, 代码需要作出相应的修改 1.1 Spring中的事务管理 作为企业级应用程序框架, Spring 在不同的事务管理 API 之上定义了一个抽象层. 而应用程序开发人员不必了解底层的事务管理 API, 就可以使用 Spring 的事务管理机制. Spring 既支持编程式事务管理, 也支持声明式的事务管理. 编程式事务管理: 将事务管理代码嵌入到业务方法中来控制事务的提交和回滚. 在编程式管理事务时, 必须在每个事务操作中包含额外的事务管理代码. 声明式事务管理: 大多数情况下比编程式事务管理更好用. 它将事务管理代码从业务方法中分离出来, 以声明的方式来实现事务管理. 事务管理作为一种横切关注点, 可以通过 AOP 方法模块化. Spring 通过 Spring AOP 框架支持声明式事务管理. Spring 中的事务管理器 Spring 从不同的事务管理 API 中抽象了一整套的事务机制. 开发人员不必了解底层的事务 API, 就可以利用这些事务机制. 有了这些事务机制, 事务管理代码就能独立于特定的事务技术了. Spring 的核心事务管理抽象是它为事务管理封装了一组独立于技术的方法. 无论使用 Spring 的哪种事务管理策略(编程式或声明式), 事务管理器都是必须的. Spring 中的事务管理器的不同实现 在应用程序中只需要处理一个数据源, 而且通过 JDBC 存取 在 JavaEE 应用服务器上用 JTA(Java Transaction API) 进行事务管理 用 Hibernate 框架存取数据库 事务管理器以普通的 Bean 形式声明在 Spring IOC 容器中 需求 数据表中的数据account表 book表 book_stock表 需求一测试实例及声明试事务管理代码 1.2 用事务通知声明式地管理事务 事务管理是一种横切关注点 为了在 Spring 2.x 中启用声明式事务管理, 可以通过 tx Schema 中定义的 &lt;tx:advice&gt; 元素声明事务通知, 为此必须事先将这个 Schema 定义添加到 &lt;beans&gt; 根元素中去. 声明了事务通知后, 就需要将它与切入点关联起来. 由于事务通知是在 &lt;aop:config&gt; 元素外部声明的, 所以它无法直接与切入点产生关联. 所以必须在 &lt;aop:config&gt; 元素中声明一个增强器通知与切入点关联起来. 由于 Spring AOP 是基于代理的方法, 所以只能增强公共方法. 因此, 只有公有方法才能通过 Spring AOP 进行事务管理. 用事务通知声明式地管理事务示例代码 1.2.1 用 @Transactional 注解声明式地管理事务 除了在带有切入点, 通知和增强器的 Bean 配置文件中声明事务外, Spring 还允许简单地用 @Transactional 注解来标注事务方法. 为了将方法定义为支持事务处理的, 可以为方法添加 @Transactional 注解. 根据 Spring AOP 基于代理机制, 只能标注公有方法. 可以在方法或者类级别上添加 @Transactional 注解. 当把这个注解应用到类上时, 这个类中的所有公共方法都会被定义成支持事务处理的. 在 Bean 配置文件中只需要启用 &lt;tx:annotation-driven&gt; 元素, 并为之指定事务管理器就可以了. 如果事务处理器的名称是 transactionManager, 就可以在&lt;tx:annotation-driven&gt; 元素中省略 transaction-manager 属性. 这个元素会自动检测该名称的事务处理器. 用 @Transactional 注解声明式地管理事务配置文件示例代码 需求一测试实例及声明试事务管理代码 1.3 事务传播属性 当事务方法被另一个事务方法调用时, 必须指定事务应该如何传播. 例如: 方法可能继续在现有事务中运行, 也可能开启一个新事务, 并在自己的事务中运行. 事务的传播行为可以由传播属性指定. Spring 定义了 7 种类传播行为 1.3.1 Spring支持的事务传播行为 1.3.1.1 REQUIRED 传播行为 当 bookService 的 purchase() 方法被另一个事务方法 checkout() 调用时, 它默认会在现有的事务内运行. 这个默认的传播行为就是 REQUIRED. 因此在 checkout() 方法的开始和终止边界内只有一个事务. 这个事务只在 checkout() 方法结束的时候被提交, 结果用户一本书都买不了 事务传播属性可以在 @Transactional 注解的 propagation 属性中定义 事务的传播行为实验测试源码 实验结果分析 首先将account表中的余额设置成300，将book_stock中书的库存同时设置成10本,1001价值100元，1002价值70元 同时买两本书1001和1002，第一次购买账户余额130，无异常 第二次购买时，先购买1001成功，余额30，然后购买1002失败，抛异常余额不足，回滚到原始状态，第二次购买失败。 1.3.1.2 REQUIRES_NEW 传播行为 另一种常见的传播行为是 REQUIRES_NEW. 它表示该方法必须启动一个新事务, 并在自己的事务内运行. 如果有事务在运行, 就应该先挂起它. 事务的传播行为实验测试源码 实验结果分析 相较于REQUIRES里的第二次购买 在购买的时候新开了一个事务，在第二次购买时。购买第一本书，余额够，购买成功，购买第二本书的时候余额不够，购买失败。事务回滚到购买第二本书之前，也就是说购买两本书，每一本书的购买都形成了一个单独的事务，第二本书的购买是否成功与第一本书无关。 1.3.3 在 Spring 2.x 事务通知中配置传播属性 在 Spring 2.x 事务通知中, 可以像下面这样在 &lt;tx:method&gt; 元素中设定传播事务属性 1.3.4 并发事务所导致的问题 当同一个应用程序或者不同应用程序中的多个事务在同一个数据集上并发执行时, 可能会出现许多意外的问题 并发事务所导致的问题可以分为下面三种类型: 脏读: 对于两个事物 T1, T2, T1 读取了已经被 T2 更新但 还没有被提交的字段. 之后, 若 T2 回滚, T1读取的内容就是临时且无效的. 不可重复读:对于两个事物 T1, T2, T1 读取了一个字段, 然后 T2 更新了该字段. 之后, T1再次读取同一个字段, 值就不同了. 幻读:对于两个事物 T1, T2, T1 从一个表中读取了一个字段, 然后 T2 在该表中插入了一些新的行. 之后, 如果 T1 再次读取同一个表, 就会多出几行. 1.4 事务的隔离级别 从理论上来说, 事务应该彼此完全隔离, 以避免并发事务所导致的问题. 然而, 那样会对性能产生极大的影响, 因为事务必须按顺序运行. 在实际开发中, 为了提升性能, 事务会以较低的隔离级别运行. 事务的隔离级别可以通过隔离事务属性指定 1.4.1 Spring 支持的事务隔离级别 事务的隔离级别要得到底层数据库引擎的支持, 而不是应用程序或者框架的支持. Oracle 支持的 2 种事务隔离级别：READ_COMMITED , SERIALIZABLE Mysql 支持 4 中事务隔离级别. 1.4.2 设置隔离事务属性 用 @Transactional 注解声明式地管理事务时可以在 @Transactional 的 isolation 属性中设置隔离级别. 在 Spring 2.x 事务通知中, 可以在 &lt;tx:method&gt; 元素中指定隔离级别 1.4.3 设置回滚事务属性 默认情况下只有未检查异常(RuntimeException和Error类型的异常)会导致事务回滚. 而受检查异常不会. 事务的回滚规则可以通过 @Transactional 注解的 rollbackFor 和 noRollbackFor 属性来定义. 这两个属性被声明为 Class[] 类型的, 因此可以为这两个属性指定多个异常类. rollbackFor: 遇到时必须进行回滚 在 Spring 2.x 事务通知中, 可以在 tx:method 元素中指定回滚规则. 如果有不止一种异常, 用逗号分隔. 隔离，回滚，超时测试实例源码 1.4.4 超时和只读属性 由于事务可以在行和表上获得锁, 因此长事务会占用资源, 并对整体性能产生影响. 如果一个事物只读取数据但不做修改, 数据库引擎可以对这个事务进行优化. 超时事务属性: 事务在强制回滚之前可以保持多久. 这样可以防止长期运行的事务占用资源. 只读事务属性: 表示这个事务只读取数据但不更新数据, 这样可以帮助数据库引擎优化事务. 设置超时和只读事务属性 超时和只读属性可以在 @Transactional 注解中定义.超时属性以秒为单位来计算. 在 Spring 2.x 事务通知中, 超时和只读属性可以在 &lt;tx:method&gt; 元素中进行指定. 隔离，回滚，超时测试实例源码 1.5 1.5 以xml的方式管理事务xmld的方式较为麻烦 1.5.1 xml配置事务管理测试实例源码 测试实例源码1.1.1 需求一测试实例代码新建包com.atguigu.spring.jdbc.txBookShopDao.java12345678910111213141516171819package com.atguigu.spring.jdbc.tx;&#x2F;** * 接口 * @author Administrator * *&#x2F;public interface BookShopDao &#123; &#x2F;&#x2F;根据书号获取书的单价 public int findBookPriceByIsbn(String isbn); &#x2F;&#x2F;更新数的库存,使书号对应的库存 -1 public void updateBookStock(String isbn); &#x2F;&#x2F;更新用户的账户余额：使username的balance - price public void updateUserAccount(String userName,int price);&#125; BookShopDaoImpl.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package com.atguigu.spring.jdbc.tx;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.stereotype.Repository;&#x2F;** * BookShopDao的实现类 * @author Administrator * *&#x2F;@Repository(&quot;bookShopDao&quot;)public class BookShopDaoImpl implements BookShopDao &#123; @Autowired private JdbcTemplate jdbcTemplate; @Override public int findBookPriceByIsbn(String isbn) &#123; &#x2F;&#x2F; TODO Auto-generated method stub String sql &#x3D; &quot;SELECT price FROM book WHERE isbn &#x3D; ?&quot;; return jdbcTemplate.queryForObject(sql, Integer.class, isbn); &#125; @Override public void updateBookStock(String isbn) &#123; &#x2F;&#x2F; TODO Auto-generated method stub &#x2F;&#x2F;检查书的库存是否足够，若不够，则抛出异常 String sql2 &#x3D; &quot;SELECT stock From book_stock WHERE isbn &#x3D; ?&quot;; int stock &#x3D; jdbcTemplate.queryForObject(sql2, Integer.class, isbn); if (stock&#x3D;&#x3D;0) &#123; throw new BookStockException(&quot;库存不足&quot;); &#125; String sql &#x3D; &quot;UPDATE book_stock SET stock &#x3D; stock-1 WHERE isbn &#x3D; ?&quot;; jdbcTemplate.update(sql,isbn); &#125; @Override public void updateUserAccount(String userName, int price) &#123; &#x2F;&#x2F; TODO Auto-generated method stub &#x2F;&#x2F;检查书余额是否足够，若不够，则抛出异常 String sql2 &#x3D; &quot;SELECT balance From account WHERE username &#x3D; ?&quot;; int balance &#x3D; jdbcTemplate.queryForObject(sql2, Integer.class, userName); if (balance&lt; price) &#123; throw new UserAccountException(&quot;余额不足&quot;); &#125; String sql &#x3D; &quot;UPDATE account SET balance &#x3D; balance - ? WHERE username &#x3D; ?&quot;; jdbcTemplate.update(sql,price,userName); &#125;&#125; applicationContext.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xmlns:context&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&quot; xmlns:tx&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;tx&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans.xsd http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&#x2F;spring-context-4.3.xsd http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;tx http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;tx&#x2F;spring-tx-4.3.xsd&quot;&gt;&lt;!-- 导入资源文件 --&gt; &lt;context:property-placeholder location&#x3D;&quot;classpath:db.properties&quot;&#x2F;&gt;&lt;!-- 配置c3p0数据源 --&gt; &lt;bean id&#x3D;&quot;dataSource&quot; class&#x3D;&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;property name&#x3D;&quot;driverClass&quot; value&#x3D;&quot;$&#123;jdbc.driverClass&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;user&quot; value&#x3D;&quot;$&#123;jdbc.user&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;password&quot; value&#x3D;&quot;$&#123;jdbc.password&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;jdbcUrl&quot; value&#x3D;&quot;$&#123;jdbc.jdbcUrl&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;initialPoolSize&quot; value&#x3D;&quot;$&#123;jdbc.initPoolSize&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;maxPoolSize&quot; value&#x3D;&quot;$&#123;jdbc.maxPoolSize&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt;&lt;!-- 配置Spring的JdbcTemplate --&gt; &lt;bean id&#x3D;&quot;jdbcTemplate&quot; class&#x3D;&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;property name&#x3D;&quot;dataSource&quot; ref&#x3D;&quot;dataSource&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt;&lt;!-- 用于测试简易模板 --&gt; &lt;context:component-scan base-package&#x3D;&quot;com.atguigu.spring&quot;&gt;&lt;&#x2F;context:component-scan&gt;&lt;!-- 配置NamedParameterJdbcTemplate，该对象可以使用具名参数，其没有无参的构造器，所以必须为其构造器指定参数 --&gt; &lt;bean id&#x3D;&quot;nameParameterJdbcTemplate&quot; class&#x3D;&quot;org.springframework.jdbc.core.namedparam.NamedParameterJdbcTemplate&quot;&gt; &lt;constructor-arg ref&#x3D;&quot;dataSource&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;&#x2F;bean&gt;&lt;!-- 配置事务管理器 --&gt; &lt;bean id&#x3D;&quot;transactionManager&quot; class&#x3D;&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name&#x3D;&quot;dataSource&quot; ref&#x3D;&quot;dataSource&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt;&lt;!-- 启用事务注解 --&gt; &lt;tx:annotation-driven transaction-manager&#x3D;&quot;transactionManager&quot;&#x2F;&gt;&lt;&#x2F;beans&gt; BookStockException.java123456789101112131415161718192021222324252627282930313233343536373839package com.atguigu.spring.jdbc.tx;&#x2F;** * 自定义异常 为了让用户见名知意知道哪里出了异常 * @author Administrator * *&#x2F;public class BookStockException extends RuntimeException&#123;&#x2F;** * 构造器Generate Constructors from superclass *&#x2F; public BookStockException() &#123; super(); &#x2F;&#x2F; TODO Auto-generated constructor stub &#125; public BookStockException(String message, Throwable cause, boolean enableSuppression, boolean writableStackTrace) &#123; super(message, cause, enableSuppression, writableStackTrace); &#x2F;&#x2F; TODO Auto-generated constructor stub &#125; public BookStockException(String message, Throwable cause) &#123; super(message, cause); &#x2F;&#x2F; TODO Auto-generated constructor stub &#125; public BookStockException(String message) &#123; super(message); &#x2F;&#x2F; TODO Auto-generated constructor stub &#125; public BookStockException(Throwable cause) &#123; super(cause); &#x2F;&#x2F; TODO Auto-generated constructor stub &#125;&#125; BookShopService.java123456789101112package com.atguigu.spring.jdbc.tx;&#x2F;** * 买书 * @author Administrator * *&#x2F;public interface BookShopService &#123; public void purchase(String userName,String isbn);&#125; BookShopServiceImpl.java12345678910111213141516171819202122232425262728293031323334353637package com.atguigu.spring.jdbc.tx;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import org.springframework.transaction.annotation.Transactional;&#x2F;** * 事务具有原子性，当余额不足的时候，无法买书 * @author Administrator * *&#x2F;@Service(&quot;bookShopService&quot;)public class BookShopServiceImpl implements BookShopService &#123; @Autowired private BookShopDao bookShopDao; &#x2F;&#x2F;添加事务注解 @Transactional @Override public void purchase(String userName, String isbn) &#123; &#x2F;&#x2F; TODO Auto-generated method stub &#x2F;&#x2F;1.获取书的单价 int price &#x3D; bookShopDao.findBookPriceByIsbn(isbn); &#x2F;&#x2F;2.更新数的库存 bookShopDao.updateBookStock(isbn); &#x2F;&#x2F;3.更新用户余额 bookShopDao.updateUserAccount(userName, price); &#125;&#125; db.properties1234567jdbc.user&#x3D;rootjdbc.password&#x3D;******jdbc.driverClass&#x3D;com.mysql.cj.jdbc.Driverjdbc.jdbcUrl&#x3D;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;spring?serverTimezone&#x3D;GMT%2B8&amp;useSSL&#x3D;falsejdbc.initPoolSize&#x3D;5jdbc.maxPoolSize&#x3D;10 测试实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.atguigu.spring.jdbc.tx;&#x2F;** * 事务测试实例 *&#x2F;import static org.junit.jupiter.api.Assertions.*;import org.junit.jupiter.api.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;class SpringTransactionTest &#123; private ApplicationContext ctx &#x3D; null; private BookShopDao bookShopDao &#x3D; null; private BookShopService bookShopService; &#123; ctx &#x3D; new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); bookShopDao &#x3D; ctx.getBean(BookShopDao.class); bookShopService &#x3D; ctx.getBean(BookShopService.class); &#125; @Test public void testBookShopService() &#123; bookShopService.purchase(&quot;AA&quot;, &quot;1001&quot;); &#125; &#x2F;** * 更新用户的余额 *&#x2F; @Test public void testBookShopDaoUpdateUserAccount() &#123; bookShopDao.updateUserAccount(&quot;AA&quot;, 50); &#125; &#x2F;** * 使书籍号对应的库存-1 @Test public void testBookShopDaoUpdateBookStock() &#123; &#x2F;&#x2F; TODO Auto-generated method stub bookShopDao.updateBookStock(&quot;1001&quot;); &#125; *&#x2F; &#x2F;** * 查找书对应的价格 *&#x2F; @Test public void testBookShopDaoFindPRiceByIsbn() &#123; System.out.println(bookShopDao.findBookPriceByIsbn(&quot;1001&quot;)); &#125;&#125; 对于结果的说明1当库存或账户余额不足时，无法购买 1.3.2 事务的传播行为实验测试源码接口Cashier.java123456789101112131415package com.atguigu.spring.jdbc.tx;import java.util.List;&#x2F;** * 测试事务的传播性 * 一个人可以买多本书 * @author Administrator * *&#x2F;public interface Cashier &#123; public void checkout(String username, List&lt;String&gt; isbns);&#125; 接口的实现类CashierImpl.java123456789101112131415161718192021222324252627282930313233343536package com.atguigu.spring.jdbc.tx;import java.util.List;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import org.springframework.transaction.annotation.Transactional;&#x2F;** * Cashier的实现类 * 测试事务的传播性 * 一个人可以买多本书 * @author Administrator * *&#x2F;@Service(&quot;cashier&quot;)public class CashierImpl implements Cashier &#123; @Autowired private BookShopService bookshopService; &#x2F;** * 用一个事务方法去调用另一个事务，看是新开一个事务还是使用上一个已有的事务 *&#x2F; @Transactional @Override public void checkout(String username, List&lt;String&gt; isbns) &#123; &#x2F;&#x2F; TODO Auto-generated method stub for (String isbn : isbns) &#123; bookshopService.purchase(username, isbn); &#125; &#125;&#125; BookShopServiceImpl.java123456789101112131415161718192021222324252627282930313233343536373839404142package com.atguigu.spring.jdbc.tx;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import org.springframework.transaction.annotation.Propagation;import org.springframework.transaction.annotation.Transactional;&#x2F;** * 事务具有原子性，当余额不足的时候，无法买书 * @author Administrator * *&#x2F;@Service(&quot;bookShopService&quot;)public class BookShopServiceImpl implements BookShopService &#123; @Autowired private BookShopDao bookShopDao; &#x2F;&#x2F;添加事务注解 &#x2F;&#x2F;使用propagation 指定事务的传播行为，及当前的事务方法被另一个事务方法调用时如何使用事务 &#x2F;&#x2F;默认取值为REQUIRED 即使用调用方法的事务(嘻嘻:去吃饭的时候碰到同学和同学并桌吃饭) &#x2F;&#x2F;REQUIRED_NEW 使用自己的事务，调用的方法的事务被挂起，即将每一本书的购买设置成单独的事务， &#x2F;&#x2F;第二本书购买是否成功不影响第一本书的购买结果(嘻嘻:去吃饭碰到同学不并桌，新开一桌) @Transactional(propagation&#x3D;Propagation.REQUIRES_NEW) @Override public void purchase(String userName, String isbn) &#123; &#x2F;&#x2F; TODO Auto-generated method stub &#x2F;&#x2F;1.获取书的单价 int price &#x3D; bookShopDao.findBookPriceByIsbn(isbn); &#x2F;&#x2F;2.更新数的库存 bookShopDao.updateBookStock(isbn); &#x2F;&#x2F;3.更新用户余额 bookShopDao.updateUserAccount(userName, price); &#125;&#125; 测试类SpringTransactionTest123456789101112131415161718192021222324252627282930313233package com.atguigu.spring.jdbc.tx;&#x2F;** * 事务测试实例 *&#x2F;import static org.junit.jupiter.api.Assertions.*;import java.util.Arrays;import org.junit.jupiter.api.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;class SpringTransactionTest &#123; private ApplicationContext ctx &#x3D; null; private Cashier cashier; &#123; ctx &#x3D; new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); cashier &#x3D; ctx.getBean(Cashier.class); &#125; &#x2F;** * 事务传播性测试 *测试的时候选中方法名在运行则只运行选中的方法 *&#x2F; @Test public void testTransactionPropagation() &#123; cashier.checkout(&quot;AA&quot;, Arrays.asList(&quot;1001&quot;,&quot;1002&quot;)); &#125;&#125; db.properties1234567jdbc.user&#x3D;rootjdbc.password&#x3D;******jdbc.driverClass&#x3D;com.mysql.cj.jdbc.Driverjdbc.jdbcUrl&#x3D;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;spring?serverTimezone&#x3D;GMT%2B8&amp;useSSL&#x3D;falsejdbc.initPoolSize&#x3D;5jdbc.maxPoolSize&#x3D;10 1.4.5 隔离，回滚，超时测试实例源码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package com.atguigu.spring.jdbc.tx;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import org.springframework.transaction.annotation.Isolation;import org.springframework.transaction.annotation.Propagation;import org.springframework.transaction.annotation.Transactional;&#x2F;** * 事务具有原子性，当余额不足的时候，无法买书 * @author Administrator * *&#x2F;@Service(&quot;bookShopService&quot;)public class BookShopServiceImpl implements BookShopService &#123; @Autowired private BookShopDao bookShopDao; &#x2F;&#x2F;添加事务注解 &#x2F;&#x2F;1. 使用propagation 指定事务的传播行为，及当前的事务方法被另一个事务方法调用时如何使用事务 &#x2F;&#x2F; 默认取值为REQUIRED 即使用调用方法的事务(嘻嘻:去吃饭的时候碰到同学和同学并桌吃饭) &#x2F;&#x2F; REQUIRED_NEW 使用自己的事务，调用的方法的事务被挂起，即将每一本书的购买设置成单独的事务， &#x2F;&#x2F; 第二本书购买是否成功不影响第一本书的购买结果(嘻嘻:去吃饭碰到同学不并桌，新开一桌) &#x2F;&#x2F;2. 使用isolation 指定事务的隔离级别，最常用的取值为 READ_COMMITTED 读，已提交 &#x2F;&#x2F;3. 默认情况下 Spring 的声明试事务对所有的运行时异常进行回滚，也可以通过对应的属性进行设置(这个回滚使用买书testBookShopService进行测试， &#x2F;&#x2F; 本来余额不足不能购买，添加noRollbackFor&#x3D;&#123;UserAccountException.class&#125;之后不对该异常进行回滚，虽然仍报异常但是书的库存会-1， &#x2F;&#x2F; 通常情况下取默认，并不对其进行设置) &#x2F;&#x2F; @Transactional(propagation&#x3D;Propagation.REQUIRES_NEW, isolation&#x3D;Isolation.READ_COMMITTED,noRollbackFor&#x3D;&#123;UserAccountException.class&#125;) &#x2F;&#x2F;4. 使用readOnly 指定事务是否为只读。表示这个事务只读取数据但不更新数据, 这样可以帮助数据库引擎优化事务.若真的是一个只读取数据库值的方法，应设置 readOnly&#x3D;true &#x2F;&#x2F;5. 使用timeout指定强制回滚之前事务可以占用的时间，以秒为单位 @Transactional(propagation&#x3D;Propagation.REQUIRES_NEW, isolation&#x3D;Isolation.READ_COMMITTED, readOnly&#x3D;false, timeout&#x3D;3) @Override public void purchase(String userName, String isbn) &#123; &#x2F;&#x2F; TODO Auto-generated method stub &#x2F;&#x2F; 测试timeout属性 &#x2F;** try &#123; Thread.sleep(5000);&#x2F;&#x2F;毫秒为单位 &#125; catch (InterruptedException e) &#123; &#x2F;&#x2F; TODO Auto-generated catch block &#125; *&#x2F; &#x2F;&#x2F;1.获取书的单价 int price &#x3D; bookShopDao.findBookPriceByIsbn(isbn); &#x2F;&#x2F;2.更新数的库存 bookShopDao.updateBookStock(isbn); &#x2F;&#x2F;3.更新用户余额 bookShopDao.updateUserAccount(userName, price); &#125;&#125; 1.5.1 xml配置事务管理测试实例源码 包com.atguigu.spring.jdbc.tx.xmlBookShopDao.java12345678910111213141516171819package com.atguigu.spring.jdbc.tx.xml;&#x2F;** * 接口 * @author Administrator * *&#x2F;public interface BookShopDao &#123; &#x2F;&#x2F;根据书号获取书的单价 public int findBookPriceByIsbn(String isbn); &#x2F;&#x2F;更新数的库存,使书号对应的库存 -1 public void updateBookStock(String isbn); &#x2F;&#x2F;更新用户的账户余额：使username的balance - price public void updateUserAccount(String userName,int price);&#125; BookShopDaoImpl.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package com.atguigu.spring.jdbc.tx.xml;import org.springframework.jdbc.core.JdbcTemplate;&#x2F;** * BookShopDao的实现类 * @author Administrator * *&#x2F;public class BookShopDaoImpl implements BookShopDao &#123; private JdbcTemplate jdbcTemplate; public void setJdbcTemplate(JdbcTemplate jdbcTemplate) &#123; this.jdbcTemplate&#x3D;jdbcTemplate; &#125; @Override public int findBookPriceByIsbn(String isbn) &#123; &#x2F;&#x2F; TODO Auto-generated method stub String sql &#x3D; &quot;SELECT price FROM book WHERE isbn &#x3D; ?&quot;; return jdbcTemplate.queryForObject(sql, Integer.class, isbn); &#125; @Override public void updateBookStock(String isbn) &#123; &#x2F;&#x2F; TODO Auto-generated method stub &#x2F;&#x2F;检查书的库存是否足够，若不够，则抛出异常 String sql2 &#x3D; &quot;SELECT stock From book_stock WHERE isbn &#x3D; ?&quot;; int stock &#x3D; jdbcTemplate.queryForObject(sql2, Integer.class, isbn); if (stock&#x3D;&#x3D;0) &#123; throw new BookStockException(&quot;库存不足&quot;); &#125; String sql &#x3D; &quot;UPDATE book_stock SET stock &#x3D; stock-1 WHERE isbn &#x3D; ?&quot;; jdbcTemplate.update(sql,isbn); &#125; @Override public void updateUserAccount(String userName, int price) &#123; &#x2F;&#x2F; TODO Auto-generated method stub &#x2F;&#x2F;检查书余额是否足够，若不够，则抛出异常 String sql2 &#x3D; &quot;SELECT balance From account WHERE username &#x3D; ?&quot;; int balance &#x3D; jdbcTemplate.queryForObject(sql2, Integer.class, userName); if (balance&lt; price) &#123; throw new UserAccountException(&quot;余额不足&quot;); &#125; String sql &#x3D; &quot;UPDATE account SET balance &#x3D; balance - ? WHERE username &#x3D; ?&quot;; jdbcTemplate.update(sql,price,userName); &#125;&#125; BookStockException.java12345678910111213141516171819202122232425262728293031323334353637package com.atguigu.spring.jdbc.tx.xml;&#x2F;** * 自定义异常 为了让用户见名知意知道哪里出了异常 * @author Administrator * *&#x2F;public class BookStockException extends RuntimeException&#123;&#x2F;** * 构造器Generate Constructors from superclass *&#x2F; public BookStockException() &#123; super(); &#x2F;&#x2F; TODO Auto-generated constructor stub &#125; public BookStockException(String message, Throwable cause, boolean enableSuppression, boolean writableStackTrace) &#123; super(message, cause, enableSuppression, writableStackTrace); &#x2F;&#x2F; TODO Auto-generated constructor stub &#125; public BookStockException(String message, Throwable cause) &#123; super(message, cause); &#x2F;&#x2F; TODO Auto-generated constructor stub &#125; public BookStockException(String message) &#123; super(message); &#x2F;&#x2F; TODO Auto-generated constructor stub &#125; public BookStockException(Throwable cause) &#123; super(cause); &#x2F;&#x2F; TODO Auto-generated constructor stub &#125;&#125; SpringTransactionTest.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.atguigu.spring.jdbc.tx.xml;&#x2F;** * 事务测试实例 *&#x2F;import java.util.Arrays;import org.junit.jupiter.api.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import com.atguigu.spring.jdbc.tx.xml.service.BookShopService;import com.atguigu.spring.jdbc.tx.xml.service.Cashier;class SpringTransactionTest &#123; private ApplicationContext ctx &#x3D; null; private BookShopDao bookShopDao &#x3D; null; private BookShopService bookShopService&#x3D;null; private Cashier cashier&#x3D;null; &#123; ctx &#x3D; new ClassPathXmlApplicationContext(&quot;applicationContext-tx-xml.xml&quot;); bookShopDao &#x3D; ctx.getBean(BookShopDao.class); bookShopService &#x3D; ctx.getBean(BookShopService.class); cashier &#x3D; ctx.getBean(Cashier.class); &#125; &#x2F;** * 事务传播性测试 *测试的时候选中方法名在运行则只运行选中的方法 *&#x2F; @Test public void testTransactionPropagation() &#123; cashier.checkout(&quot;AA&quot;, Arrays.asList(&quot;1001&quot;, &quot;1002&quot;)); &#125; &#x2F;** * 买书 *&#x2F; @Test public void testBookShopService() &#123; bookShopService.purchase(&quot;AA&quot;, &quot;1001&quot;); &#125;&#125; UserAccountException.java1234567891011121314151617181920212223242526272829303132333435363738394041package com.atguigu.spring.jdbc.tx.xml;&#x2F;** * 自定义异常 为了让用户见名知意知道哪里出了异常 * @author Administrator * *&#x2F;public class UserAccountException extends RuntimeException &#123; &#x2F;** * 构造器Generate Constructors from superclass *&#x2F; private static final long serialVersionUID &#x3D; 1L; public UserAccountException() &#123; super(); &#x2F;&#x2F; TODO Auto-generated constructor stub &#125; public UserAccountException(String message, Throwable cause, boolean enableSuppression, boolean writableStackTrace) &#123; super(message, cause, enableSuppression, writableStackTrace); &#x2F;&#x2F; TODO Auto-generated constructor stub &#125; public UserAccountException(String message, Throwable cause) &#123; super(message, cause); &#x2F;&#x2F; TODO Auto-generated constructor stub &#125; public UserAccountException(String message) &#123; super(message); &#x2F;&#x2F; TODO Auto-generated constructor stub &#125; public UserAccountException(Throwable cause) &#123; super(cause); &#x2F;&#x2F; TODO Auto-generated constructor stub &#125;&#125; 包com.atguigu.spring.jdbc.tx.xml.serviceBookShopService.java123456789101112package com.atguigu.spring.jdbc.tx.xml.service;&#x2F;** * 买书 * @author Administrator * *&#x2F;public interface BookShopService &#123; public void purchase(String userName,String isbn);&#125; Cashier.java123456789101112131415package com.atguigu.spring.jdbc.tx.xml.service;import java.util.List;&#x2F;** * 测试事务的传播性 * 一个人可以买多本书 * @author Administrator * *&#x2F;public interface Cashier &#123; public void checkout(String userName, List&lt;String&gt; isbns);&#125; com.atguigu.spring.jdbc.tx.xml.service.implBookShopServiceImpl.java1234567891011121314151617181920212223242526272829303132333435package com.atguigu.spring.jdbc.tx.xml.service.impl;import com.atguigu.spring.jdbc.tx.xml.BookShopDao;import com.atguigu.spring.jdbc.tx.xml.service.BookShopService;&#x2F;** * 事务具有原子性，当余额不足的时候，无法买书 * @author Administrator * *&#x2F;public class BookShopServiceImpl implements BookShopService &#123; private BookShopDao bookShopDao; public void setBookShopDao(BookShopDao bookShopDao) &#123; this.bookShopDao &#x3D; bookShopDao; &#125; @Override public void purchase(String userName, String isbn) &#123; &#x2F;&#x2F;1.获取书的单价 int price &#x3D; bookShopDao.findBookPriceByIsbn(isbn); &#x2F;&#x2F;2.更新数的库存 bookShopDao.updateBookStock(isbn); &#x2F;&#x2F;3.更新用户余额 bookShopDao.updateUserAccount(userName, price); &#125;&#125; CashierImpl.java123456789101112131415161718192021222324252627282930313233343536373839package com.atguigu.spring.jdbc.tx.xml.service.impl;import java.util.List;import com.atguigu.spring.jdbc.tx.xml.service.BookShopService;import com.atguigu.spring.jdbc.tx.xml.service.Cashier;&#x2F;** * Cashier的实现类 * 测试事务的传播性 * 一个人可以买多本书 * @author Administrator * *&#x2F;public class CashierImpl implements Cashier &#123; private BookShopService bookShopService; &#x2F;* * 利用自动提示写出setBookShopService *&#x2F; public void setBookShopService(BookShopService bookShopService) &#123; this.bookShopService &#x3D; bookShopService; &#125; &#x2F;** * 用一个事务方法去调用另一个事务，看是新开一个事务还是使用上一个已有的事务 *&#x2F; @Override public void checkout(String username, List&lt;String&gt; isbns) &#123; &#x2F;&#x2F; TODO Auto-generated method stub for (String isbn : isbns) &#123; bookShopService.purchase(username, isbn); &#125; &#125;&#125; applicationContext-tx-xml.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xmlns:context&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&quot; xmlns:tx&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;tx&quot; xmlns:aop&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;aop&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans.xsd http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&#x2F;spring-context-4.3.xsd http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;aop http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;aop&#x2F;spring-aop-4.3.xsd http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;tx http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;tx&#x2F;spring-tx-4.3.xsd&quot;&gt;&lt;!-- 导入资源文件 --&gt; &lt;context:property-placeholder location&#x3D;&quot;classpath:db.properties&quot;&#x2F;&gt;&lt;!-- 配置c3p0数据源 --&gt; &lt;bean id&#x3D;&quot;dataSource&quot; class&#x3D;&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;property name&#x3D;&quot;driverClass&quot; value&#x3D;&quot;$&#123;jdbc.driverClass&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;user&quot; value&#x3D;&quot;$&#123;jdbc.user&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;password&quot; value&#x3D;&quot;$&#123;jdbc.password&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;jdbcUrl&quot; value&#x3D;&quot;$&#123;jdbc.jdbcUrl&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;initialPoolSize&quot; value&#x3D;&quot;$&#123;jdbc.initPoolSize&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;maxPoolSize&quot; value&#x3D;&quot;$&#123;jdbc.maxPoolSize&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt;&lt;!-- 配置Spring的JdbcTemplate --&gt; &lt;bean id&#x3D;&quot;jdbcTemplate&quot; class&#x3D;&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;property name&#x3D;&quot;dataSource&quot; ref&#x3D;&quot;dataSource&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt;&lt;!-- 配置Bean --&gt; &lt;bean id&#x3D;&quot;bookShopDao&quot; class&#x3D;&quot;com.atguigu.spring.jdbc.tx.xml.BookShopDaoImpl&quot;&gt; &lt;property name&#x3D;&quot;jdbcTemplate&quot; ref&#x3D;&quot;jdbcTemplate&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt; &lt;bean id&#x3D;&quot;bookShopService&quot; class&#x3D;&quot;com.atguigu.spring.jdbc.tx.xml.service.impl.BookShopServiceImpl&quot;&gt; &lt;property name&#x3D;&quot;bookShopDao&quot; ref&#x3D;&quot;bookShopDao&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt; &lt;bean id&#x3D;&quot;cashier&quot; class&#x3D;&quot;com.atguigu.spring.jdbc.tx.xml.service.impl.CashierImpl&quot;&gt; &lt;property name&#x3D;&quot;bookShopService&quot; ref&#x3D;&quot;bookShopService&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt;&lt;!-- 1.配置事务管理器 --&gt; &lt;bean id&#x3D;&quot;transactionManager&quot; class&#x3D;&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name&#x3D;&quot;dataSource&quot; ref&#x3D;&quot;dataSource&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt;&lt;!-- 2.配置事务属性，回滚，超时等。。 --&gt; &lt;tx:advice id&#x3D;&quot;txAdvice&quot; transaction-manager&#x3D;&quot;transactionManager&quot;&gt; &lt;tx:attributes&gt; &lt;!-- 根据方法名指定事务的属性 --&gt; &lt;tx:method name&#x3D;&quot;purchase&quot; propagation&#x3D;&quot;REQUIRES_NEW&quot;&#x2F;&gt; &lt;tx:method name&#x3D;&quot;get*&quot; read-only&#x3D;&quot;true&quot;&#x2F;&gt; &lt;tx:method name&#x3D;&quot;find*&quot; read-only&#x3D;&quot;true&quot;&#x2F;&gt; &lt;!-- 暂时先用*代替，采用默认 --&gt; &lt;tx:method name&#x3D;&quot;*&quot;&#x2F;&gt; &lt;&#x2F;tx:attributes&gt; &lt;&#x2F;tx:advice&gt;&lt;!-- 3.配置事务切入点，以及把事务切入点和事务属性关联起来 --&gt; &lt;aop:config&gt; &lt;aop:pointcut expression&#x3D;&quot;execution(* com.atguigu.spring.jdbc.tx.xml.Service.*.*(..))&quot; id&#x3D;&quot;txPointCut&quot;&#x2F;&gt; &lt;aop:advisor advice-ref&#x3D;&quot;txAdvice&quot; pointcut-ref&#x3D;&quot;txPointCut&quot;&#x2F;&gt; &lt;&#x2F;aop:config&gt;&lt;&#x2F;beans&gt; db.properties1234567jdbc.user&#x3D;rootjdbc.password&#x3D;******jdbc.driverClass&#x3D;com.mysql.cj.jdbc.Driverjdbc.jdbcUrl&#x3D;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;spring?serverTimezone&#x3D;GMT%2B8&amp;useSSL&#x3D;falsejdbc.initPoolSize&#x3D;5jdbc.maxPoolSize&#x3D;10","categories":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"-ssm -Spring - spring-事务管理","slug":"ssm-Spring-spring-事务管理","permalink":"http://www.studyz.club/tags/ssm-Spring-spring-%E4%BA%8B%E5%8A%A1%E7%AE%A1%E7%90%86/"}]},{"title":"软件开发框架之spring-JDBC","slug":"软件开发框架之Spring-JDBC","date":"2019-10-25T11:59:27.250Z","updated":"2019-11-09T08:13:16.165Z","comments":true,"path":"posts/dd30942e/","link":"","permalink":"http://www.studyz.club/posts/dd30942e/","excerpt":"","text":"一， Spring对jdbc的支持 1.1 JdbcTemplate 简介 1.2 使用 JdbcTemplate 更新数据库 1.3 使用 JdbcTemplate 查询数据库 1.4 简化JDBC模板查询 1.5 在JDBC模板中使用具名参数 一， Spring对jdbc的支持1.1 JdbcTemplate 简介 为了使 JDBC 更加易于使用, Spring 在 JDBC API 上定义了一个抽象层, 以此建立一个 JDBC 存取框架. 作为 Spring JDBC 框架的核心, JDBC 模板的设计目的是为不同类型的 JDBC 操作提供模板方法. 每个模板方法都能控制整个过程, 并允许覆盖过程中的特定任务. 通过这种方式, 可以在尽可能保留灵活性的情况下, 将数据库存取的工作量降到最低. 1.2 使用 JdbcTemplate 更新数据库 用 sql 语句和参数更新数据库: 使用 JdbcTemplate 查询数据库 批量更新数据库: 使用 JdbcTemplate 查询数据库 1.3 使用 JdbcTemplate 查询数据库 查询单行 使用 JdbcTemplate 查询数据库 便利的 BeanPropertyRowMapper 实现 使用 JdbcTemplate 查询数据库 查询多行 使用 JdbcTemplate 查询数据库 单值查询 使用 JdbcTemplate 查询数据库 1.4 简化JDBC模板查询 每次使用都创建一个 JdbcTemplate 的新实例, 这种做法效率很低下. JdbcTemplate 类被设计成为线程安全的, 所以可以再 IOC 容器中声明它的单个实例, 并将这个实例注入到所有的 DAO 实例中. JdbcTemplate 也利用了 Java 1.5 的特定(自动装箱, 泛型, 可变长度等)来简化开发 Spring JDBC 框架还提供了一个 JdbcDaoSupport 类来简化 DAO 实现. 该类声明了 jdbcTemplate 属性, 它可以从 IOC 容器中注入, 或者自动从数据源中创建. 简化JDBC模板查询 注入 JDBC 模板示例代码 扩展 JdbcDaoSupport 示例代码 1.5 在JDBC模板中使用具名参数 在经典的 JDBC 用法中, SQL 参数是用占位符 ? 表示,并且受到位置的限制. 定位参数的问题在于, 一旦参数的顺序发生变化, 就必须改变参数绑定. 在 Spring JDBC 框架中, 绑定 SQL 参数的另一种选择是使用具名参数(named parameter). 具名参数: SQL 按名称(以冒号开头)而不是按位置进行指定. 具名参数更易于维护, 也提升了可读性. 具名参数由框架类在运行时用占位符取代 具名参数只在 NamedParameterJdbcTemplate 中得到支持 在 SQL 语句中使用具名参数时, 可以在一个 Map 中提供参数值, 参数名为键 也可以使用 SqlParameterSource 参数 批量更新时可以提供 Map 或 SqlParameterSource 的数组 在 JDBC 模板中使用具名参数测试代码 测试代码1.3.1 使用 JdbcTemplate 查询数据库新建包com.atguigu.spring.jdbc导入如下jar包 数据库文件 Department123456789101112131415161718192021222324package com.atguigu.spring.jdbc;public class Department &#123; private Integer iD; private String DEPT_NAME; public Integer getiD() &#123; return iD; &#125; public void setiD(Integer iD) &#123; this.iD &#x3D; iD; &#125; public String getDEPT_NAME() &#123; return DEPT_NAME; &#125; public void setDEPT_NAME(String dEPT_NAME) &#123; DEPT_NAME &#x3D; dEPT_NAME; &#125; @Override public String toString() &#123; return &quot;Deparment [iD&#x3D;&quot; + iD + &quot;, DEPT_NAME&#x3D;&quot; + DEPT_NAME + &quot;]&quot;; &#125;&#125; Employee.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.atguigu.spring.jdbc;public class Employee &#123; private Integer ID; private String lastName; private String EMAIL; private Department departments; public Integer getID() &#123; return ID; &#125; public void setID(Integer iD) &#123; ID &#x3D; iD; &#125; public String getLastName() &#123; return lastName; &#125; public void setLastName(String lastName) &#123; this.lastName &#x3D; lastName; &#125; public String getEMAIL() &#123; return EMAIL; &#125; public void setEMAIL(String eMAIL) &#123; EMAIL &#x3D; eMAIL; &#125; public Department getDepartments() &#123; return departments; &#125; public void setDepartments(Department departments) &#123; this.departments &#x3D; departments; &#125; @Override public String toString() &#123; return &quot;Employee [ID&#x3D;&quot; + ID + &quot;, lastName&#x3D;&quot; + lastName + &quot;, EMAIL&#x3D;&quot; + EMAIL + &quot;, departments&#x3D;&quot; + departments + &quot;]&quot;; &#125;&#125; applicationContext.xml1234567891011121314151617181920212223242526272829303132&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xmlns:context&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans.xsd http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&#x2F;spring-context-4.3.xsd&quot;&gt;&lt;!-- 导入资源文件 --&gt; &lt;context:property-placeholder location&#x3D;&quot;classpath:db.properties&quot;&#x2F;&gt;&lt;!-- 配置c3p0数据源 --&gt; &lt;bean id&#x3D;&quot;dataSource&quot; class&#x3D;&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;property name&#x3D;&quot;driverClass&quot; value&#x3D;&quot;$&#123;jdbc.driverClass&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;user&quot; value&#x3D;&quot;$&#123;jdbc.user&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;password&quot; value&#x3D;&quot;$&#123;jdbc.password&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;jdbcUrl&quot; value&#x3D;&quot;$&#123;jdbc.jdbcUrl&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;initialPoolSize&quot; value&#x3D;&quot;$&#123;jdbc.initPoolSize&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;maxPoolSize&quot; value&#x3D;&quot;$&#123;jdbc.maxPoolSize&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt;&lt;!-- 配置Spring的JdbcTemplate --&gt; &lt;bean id&#x3D;&quot;jdbcTemplate&quot; class&#x3D;&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;property name&#x3D;&quot;dataSource&quot; ref&#x3D;&quot;dataSource&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt;&lt;!-- 用于测试简易模板 --&gt; &lt;context:component-scan base-package&#x3D;&quot;com.atguigu.spring.jdbc&quot;&gt;&lt;&#x2F;context:component-scan&gt;&lt;&#x2F;beans&gt; db.properties1234567jdbc.user&#x3D;rootjdbc.password&#x3D;******jdbc.driverClass&#x3D;com.mysql.cj.jdbc.Driverjdbc.jdbcUrl&#x3D;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;spring?serverTimezone&#x3D;GMT%2B8&amp;useSSL&#x3D;falsejdbc.initPoolSize&#x3D;5jdbc.maxPoolSize&#x3D;10 JDBCTest.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132package com.atguigu.spring.jdbc;import java.sql.SQLException;import java.util.ArrayList;import java.util.List;import javax.sql.DataSource;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import org.springframework.jdbc.core.BeanPropertyRowMapper;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.jdbc.core.RowMapper;import org.springframework.util.CollectionUtils;public class JDBCTest &#123; private ApplicationContext ctx &#x3D; null; private JdbcTemplate jdbcTemplate; private EmployeeDao employeeDao; &#123; ctx &#x3D; new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); jdbcTemplate &#x3D; (JdbcTemplate) ctx.getBean(&quot;jdbcTemplate&quot;); employeeDao &#x3D; ctx.getBean(EmployeeDao.class); &#125; &#x2F;** * 获取单个列的值，或做统计查询， * 使用queryForObject(String sql, Class&lt;Long&gt; requiredType)方法 *&#x2F; @Test public void testQUeryForObject2() &#123; String sql &#x3D; &quot;SELECT count(ID) FROM employees&quot;; long count &#x3D; jdbcTemplate.queryForObject(sql, Long.class); System.out.println(count); &#125; &#x2F;** * 查找实体类的集合 * 注意调用的不是querForList方法 * 用eclipse如果筛选的条数多不是没打印，而是打印了控制台没显示出来，复制到文本里能看到 *&#x2F; @Test public void testQueryForLis() &#123; &#x2F;&#x2F; TODO Auto-generated method stub String sql &#x3D; &quot;SELECT ID, LAST_NAME lastName, EMAIL FROM employees WHERE ID&gt;?&quot;; RowMapper&lt;Employee&gt; rowMapper &#x3D; new BeanPropertyRowMapper&lt;Employee&gt;(Employee.class); List&lt;Employee&gt; employees &#x3D; jdbcTemplate.query(sql, rowMapper,5); System.out.println(employees); System.out.println(employees.size()); System.out.println(CollectionUtils.isEmpty(employees)); &#x2F;* 使用循环打印 for(int i &#x3D;0 ; i &lt; employees.size(); i ++ )&#123; System.out.println(employees.get(i)); &#125; *&#x2F; &#125; &#x2F;** * 从数据库中获取一条记录，实际得到对应的一个对象 * 注意不是调用queryForObject(String sql, Class&lt;Employee&gt; requiredType, @Nullable Object... args)方法 *而需要调用queryForObject(String sql, RowMapper&lt;Employee&gt; rowMapper, @Nullable Object... args) 方法 *1.其中的RowMapper 指定如何取映射结果集的行，常用的实现类为BeanPropertyRowMapper *2.使用SQL中列的别名完成列名和类的属性名的映射。例如 LAST_NAME lastName(注：也可以在类中直接使用SQL文件的列名免去映射) *3.不支持级联属性，JdbcTemplate 到底是JDBC的一个小工具，而不是ORM框架 *&#x2F; @Test public void testQueryForObject() &#123; String sql &#x3D; &quot;SELECT ID, LAST_NAME lastName, EMAIL, DEPT_ID FROM employees WHERE ID &#x3D;?&quot;; RowMapper&lt;Employee&gt; rowMapper &#x3D; new BeanPropertyRowMapper&lt;&gt;(Employee.class); Employee employee &#x3D; jdbcTemplate.queryForObject(sql, rowMapper,1); System.out.println(employee); &#125; &#x2F;** *测试 批量更新INSERT,UPDATE,DELETE *最后一个参数是Object[] 的List类型: 因为修改一条记录需要一个Object的数组。那么多条不需要多个object的数组 *&#x2F; @Test public void testBatchUpdate() &#123; String sql &#x3D; &quot;INSERT INTO employees(last_name,email,dept_id) VALUES(?,?,?)&quot;; List&lt;Object[]&gt; batchArgs &#x3D; new ArrayList&lt;&gt;(); &#x2F;&#x2F;这地方注意，由于没有设置ID的值，因此在设计表的时候需要将ID设成自动递增 batchArgs.add(new Object[] &#123;&quot;AA&quot;,&quot;8888@qq.com&quot;,1&#125;); batchArgs.add(new Object[] &#123;&quot;BB&quot;,&quot;7777@qq.com&quot;,2&#125;); batchArgs.add(new Object[] &#123;&quot;CC&quot;,&quot;6666@qq.com&quot;,3&#125;); batchArgs.add(new Object[] &#123;&quot;DD&quot;,&quot;5555@qq.com&quot;,3&#125;); batchArgs.add(new Object[] &#123;&quot;EE&quot;,&quot;4444@qq.com&quot;,2&#125;); jdbcTemplate.batchUpdate(sql, batchArgs); &#125; &#x2F;** * 执行INSERT,UPDATE,DELETE *&#x2F; @Test public void testUpdate() &#123; String sql &#x3D; &quot;UPDATE employees SET last_name &#x3D; ? WHERE id &#x3D; ?&quot;; jdbcTemplate.update(sql,&quot;Jack&quot;,5); &#125; @Test public void testDataSource() throws SQLException &#123; DataSource dataSource &#x3D; ctx.getBean(DataSource.class); System.out.println(dataSource.getConnection()); &#125; &#x2F;** * 简化模板测试 *&#x2F; @Test public void testEmployeeDao() &#123; System.out.println(employeeDao.get(1)); &#125;&#125; 简化查询EmployeeDao.java12345678910111213141516171819202122232425262728package com.atguigu.spring.jdbc;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.jdbc.core.BeanPropertyRowMapper;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.jdbc.core.RowMapper;import org.springframework.stereotype.Repository;&#x2F;** * 简化 JDBC 模板查询 *&#x2F;@Repositorypublic class EmployeeDao &#123; @Autowired private JdbcTemplate jdbcTemplate; public Employee get(Integer ID) &#123; String sql &#x3D; &quot;SELECT ID, LAST_NAME lastName, EMAIL, DEPT_ID FROM employees WHERE ID &#x3D;?&quot;; RowMapper&lt;Employee&gt; rowMapper &#x3D; new BeanPropertyRowMapper&lt;&gt;(Employee.class); Employee employee &#x3D; jdbcTemplate.queryForObject(sql, rowMapper,ID); return employee; &#125;&#125; 在 JDBC 模板中使用具名参数测试代码applicationContext.xml1234567891011121314151617181920212223242526272829303132333435363738&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xmlns:context&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans.xsd http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&#x2F;spring-context-4.3.xsd&quot;&gt;&lt;!-- 导入资源文件 --&gt; &lt;context:property-placeholder location&#x3D;&quot;classpath:db.properties&quot;&#x2F;&gt;&lt;!-- 配置c3p0数据源 --&gt; &lt;bean id&#x3D;&quot;dataSource&quot; class&#x3D;&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;property name&#x3D;&quot;driverClass&quot; value&#x3D;&quot;$&#123;jdbc.driverClass&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;user&quot; value&#x3D;&quot;$&#123;jdbc.user&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;password&quot; value&#x3D;&quot;$&#123;jdbc.password&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;jdbcUrl&quot; value&#x3D;&quot;$&#123;jdbc.jdbcUrl&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;initialPoolSize&quot; value&#x3D;&quot;$&#123;jdbc.initPoolSize&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;maxPoolSize&quot; value&#x3D;&quot;$&#123;jdbc.maxPoolSize&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt;&lt;!-- 配置Spring的JdbcTemplate --&gt; &lt;bean id&#x3D;&quot;jdbcTemplate&quot; class&#x3D;&quot;org.springframework.jdbc.core.JdbcTemplate&quot;&gt; &lt;property name&#x3D;&quot;dataSource&quot; ref&#x3D;&quot;dataSource&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt;&lt;!-- 用于测试简易模板 --&gt; &lt;context:component-scan base-package&#x3D;&quot;com.atguigu.spring.jdbc&quot;&gt;&lt;&#x2F;context:component-scan&gt;&lt;!-- 配置NamedParameterJdbcTemplate，该对象可以使用具名参数，其没有无参的构造器，所以必须为其构造器指定参数 --&gt; &lt;bean id&#x3D;&quot;nameParameterJdbcTemplate&quot; class&#x3D;&quot;org.springframework.jdbc.core.namedparam.NamedParameterJdbcTemplate&quot;&gt; &lt;constructor-arg ref&#x3D;&quot;dataSource&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;&#x2F;bean&gt;&lt;&#x2F;beans&gt; Employee.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package com.atguigu.spring.jdbc;public class Employee &#123; private Integer ID; private String lastName; private String EMAIL; private Integer deptId; public Integer getID() &#123; return ID; &#125; public void setID(Integer iD) &#123; ID &#x3D; iD; &#125; public String getLastName() &#123; return lastName; &#125; public void setLastName(String lastName) &#123; this.lastName &#x3D; lastName; &#125; public String getEMAIL() &#123; return EMAIL; &#125; public void setEMAIL(String eMAIL) &#123; EMAIL &#x3D; eMAIL; &#125; public Integer getDeptId() &#123; return deptId; &#125; public void setDeptId(Integer deptId) &#123; this.deptId &#x3D; deptId; &#125; @Override public String toString() &#123; return &quot;Employee [ID&#x3D;&quot; + ID + &quot;, lastName&#x3D;&quot; + lastName + &quot;, EMAIL&#x3D;&quot; + EMAIL + &quot;, deptId&#x3D;&quot; + deptId + &quot;]&quot;; &#125;&#125; Jdbcparameter.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.atguigu.spring.jdbc;import java.util.HashMap;import java.util.Map;import org.junit.Test;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.jdbc.core.namedparam.BeanPropertySqlParameterSource;import org.springframework.jdbc.core.namedparam.NamedParameterJdbcTemplate;import org.springframework.jdbc.core.namedparam.SqlParameterSource;public class Jdbcparameter &#123; private ApplicationContext ctx &#x3D; null; private NamedParameterJdbcTemplate namedParameterJdbcTemplate; &#123; ctx &#x3D; new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); namedParameterJdbcTemplate &#x3D; ctx.getBean(NamedParameterJdbcTemplate.class); &#125; &#x2F;** * 使用具名参数时，可以使用update(String sql, SqlParameterSource paramSource) 方法 * 进行更新操作 * 好处：1.SQL语句中的参数名和类的属性一致 * 2.使用SqlParameterSource的BeanPropertySqlParameterSource实现类作为参数 *&#x2F; @Test public void testNamedParameter() &#123; &#x2F;&#x2F; TODO Auto-generated method stub &#x2F;&#x2F;values的值对应的是employee类里的 String sql &#x3D; &quot;INSERT INTO employees (last_name,EMAIL,DEPT_ID) &quot;+&quot;VALUES(:lastName,:EMAIL,:deptId)&quot;; Employee employee &#x3D; new Employee(); employee.setLastName(&quot;XYZ&quot;); employee.setEMAIL(&quot;4878485@qq.com&quot;); employee.setDeptId(3); SqlParameterSource paramSource &#x3D; new BeanPropertySqlParameterSource(employee); namedParameterJdbcTemplate.update(sql, paramSource); &#125; &#x2F;** * JDBC 模板中使用具名参数 * 可以为参数取名 * 1.好处:若有多个参数则不用再去对应位置，直接对应参数名，便于维护 * 2.缺点:较为麻烦。 *&#x2F; @Test public void testNamedParameterJdbcTemlate() &#123; String sql &#x3D; &quot;INSERT INTO employees (last_name,EMAIL,DEPT_ID) VALUES(:ln,:em,:id)&quot;; Map&lt;String, Object&gt; paramMap &#x3D; new HashMap&lt;&gt;(); paramMap.put(&quot;ln&quot;, &quot;FF&quot;); paramMap.put(&quot;em&quot;, &quot;484751487@qq.com&quot;); paramMap.put(&quot;id&quot;, 2); namedParameterJdbcTemplate.update(sql, paramMap); &#125;&#125;","categories":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"-ssm -Spring - spring-jdbc","slug":"ssm-Spring-spring-jdbc","permalink":"http://www.studyz.club/tags/ssm-Spring-spring-jdbc/"}]},{"title":"软件开发框架之spring-AOP","slug":"软件开发框架之Spring-AOP","date":"2019-10-18T11:27:01.277Z","updated":"2019-11-09T08:13:06.436Z","comments":true,"path":"posts/2e9ad068/","link":"","permalink":"http://www.studyz.club/posts/2e9ad068/","excerpt":"","text":"一， AOP面向切面编程 1.1 AOP需求 1.2 AOP简介 1.3 SpringAOP 1.3.1 前置通知 1.3.2 后置通知 1.3.3 返回通知 1.3.4 异常通知 1.3.5 环绕通知 1.3.6 环绕通知示例代码 1.3.7 指定切面的优先级 1.3.8 重用切入点定义 1.3.9 引入通知 1.4 用基于 XML 的配置声明切面 一， AOP面向切面编程1.1 AOP需求为什么需要AOP 需求1-日志：在程序执行期间追踪正在发生的活动 需求2-验证：希望计算器只能处理正数的运算测试实例源码源码中用于输出日志的语句多且重复，维护不方便 代码混乱：越来越多的非业务需求(日志和验证等)加入后, 原有的业务方法急剧膨胀. 每个方法在处理核心逻辑的同时还必须兼顾其他多个关注点. 代码分散: 以日志需求为例, 只是为了满足这个单一需求, 就不得不在多个模块（方法）里多次重复相同的日志代码. 如果日志需求发生变化, 必须修改所有模块. 解决办法使用动态代理解决上述问题 代理设计模式的原理: 使用一个代理将对象包装起来, 然后用该代理对象取代原始对象. 任何对原始对象的调用都要通过代理. 代理对象决定是否以及何时将方法调用转到原始对象上.动态代理解决日志问题源码使用动态代理可以解决这个问题但是还是有些麻烦 1.2 AOP简介 AOP(Aspect-Oriented Programming, 面向切面编程): 是一种新的方法论, 是对传统 OOP(Object-Oriented Programming, 面向对象编程) 的补充. AOP 的主要编程对象是切面(aspect), 而切面模块化横切关注点. 在应用 AOP 编程时, 仍然需要定义公共功能, 但可以明确的定义这个功能在哪里, 以什么方式应用, 并且不必修改受影响的类. 这样一来横切关注点就被模块化到特殊的对象(切面)里. AOP 的好处: 每个事物逻辑位于一个位置, 代码不分散, 便于维护和升级 业务模块更简洁, 只包含核心业务代码. AOP 术语 切面(Aspect): 横切关注点(跨越应用程序多个模块的功能)被模块化的特殊对象 通知(Advice): 切面必须要完成的工作 目标(Target): 被通知的对象 代理(Proxy): 向目标对象应用通知之后创建的对象 连接点（Joinpoint）：程序执行的某个特定位置：如类某个方法调用前、调用后、方法抛出异常后等。连接点由两个信息确定：方法表示的程序执行点；相对点表示的方位。例如 ArithmethicCalculator#add() 方法执行前的连接点，执行点为 ArithmethicCalculator#add()； 方位为该方法执行前的位置 切点（pointcut）：每个类都拥有多个连接点：例如 ArithmethicCalculator 的所有方法实际上都是连接点，即连接点是程序类中客观存在的事务。AOP 通过切点定位到特定的连接点。类比：连接点相当于数据库中的记录，切点相当于查询条件。切点和连接点不是一对一的关系，一个切点匹配多个连接点，切点通过 org.springframework.aop.Pointcut 接口进行描述，它使用类和方法作为连接点的查询条件。 1.3 SpringAOP AspectJ：Java 社区里最完整最流行的 AOP 框架. 在 Spring2.0 以上版本中, 可以使用基于 AspectJ 注解或基于 XML 配置的 AOP 在 Spring 中启用 AspectJ 注解支持 要在 Spring 应用中使用 AspectJ 注解, 必须在 classpath 下包含 AspectJ 类库: aopalliance.jar、aspectj.weaver.jar 和 spring-aspects.jar 将 aop Schema 添加到 &lt;beans&gt; 根元素中. 要在 Spring IOC 容器中启用 AspectJ 注解支持, 只要在 Bean 配置文件中定义一个空的 XML 元素 &lt;aop:aspectj-autoproxy&gt; 当 Spring IOC 容器侦测到 Bean 配置文件中的 aop:aspectj-autoproxy 元素时, 会自动为与 AspectJ 切面匹配的 Bean 创建代理. 在 Spring 中启用 AspectJ 注解支持,前置通知及后置通知源码 用 AspectJ 注解声明切面 要在 Spring 中声明 AspectJ 切面, 只需要在 IOC 容器中将切面声明为 Bean 实例. 当在 Spring IOC 容器中初始化 AspectJ 切面之后, Spring IOC 容器就会为那些与 AspectJ 切面相匹配的 Bean 创建代理. 在 AspectJ 注解中, 切面只是一个带有 @Aspect 注解的 Java 类. 通知是标注有某种注解的简单的 Java 方法. AspectJ 支持 5 种类型的通知注解: @Before: 前置通知, 在方法执行之前执行 @After: 后置通知, 在方法执行之后执行 @AfterRunning: 返回通知, 在方法返回结果之后执行 @AfterThrowing: 异常通知, 在方法抛出异常之后 @Around: 环绕通知, 围绕着方法执行 1.3.1 前置通知 利用方法签名编写 AspectJ 切入点表达式 合并切入点表达式 让通知访问当前连接点的细节 1.3.2 后置通知 返回通知，异常那通知，后置通知源码1.3.3 返回通知 返回通知，异常那通知，后置通知源码在返回通知中访问连接点的返回值 1.3.4 异常通知 返回通知，异常那通知，后置通知源码1.3.5 环绕通知 1.3.6 环绕通知示例代码 返回通知，异常那通知，后置通知源码1.3.7 指定切面的优先级 在同一个连接点上应用不止一个切面时, 除非明确指定, 否则它们的优先级是不确定的. 切面的优先级可以通过实现 Ordered 接口或利用 @Order 注解指定. 实现 Ordered 接口, getOrder() 方法的返回值越小, 优先级越高. 若使用 @Order 注解, 序号出现在注解中 指定切面优先级源码 1.3.8 重用切入点定义 在编写 AspectJ 切面时, 可以直接在通知注解中书写切入点表达式. 但同一个切点表达式可能会在多个通知中重复出现. 在 AspectJ 切面中, 可以通过 @Pointcut 注解将一个切入点声明成简单的方法. 切入点的方法体通常是空的, 因为将切入点定义与应用程序逻辑混在一起是不合理的. 切入点方法的访问控制符同时也控制着这个切入点的可见性. 如果切入点要在多个切面中共用, 最好将它们集中在一个公共的类中. 在这种情况下, 它们必须被声明为 public. 在引入这个切入点时, 必须将类名也包括在内. 如果类没有与这个切面放在同一个包中, 还必须包含包名. 其他通知可以通过方法名称引入该切入点. 重用切入点定义示例代码重用切入点源码 1.3.9 引入通知 引入通知是一种特殊的通知类型. 它通过为接口提供实现类, 允许对象动态地实现接口, 就像对象已经在运行时扩展了实现类一样. 引入通知可以使用两个实现类 MaxCalculatorImpl 和 MinCalculatorImpl, 让 ArithmeticCalculatorImpl 动态地实现 MaxCalculator 和 MinCalculator 接口. 而这与从 MaxCalculatorImpl 和 MinCalculatorImpl 中实现多继承的效果相同. 但却不需要修改 ArithmeticCalculatorImpl 的源代码 引入通知也必须在切面中声明 在切面中, 通过为任意字段添加@DeclareParents 注解来引入声明. 注解类型的 value 属性表示哪些类是当前引入通知的目标. value 属性值也可以是一个 AspectJ 类型的表达式, 以将一个即可引入到多个类中. defaultImpl 属性中指定这个接口使用的实现类 引入通知示例代码 1.4 用基于 XML 的配置声明切面 除了使用 AspectJ 注解声明切面, Spring 也支持在 Bean 配置文件中声明切面. 这种声明是通过 aop schema 中的 XML 元素完成的. 正常情况下, 基于注解的声明要优先于基于 XML 的声明. 通过 AspectJ 注解, 切面可以与 AspectJ 兼容, 而基于 XML 的配置则是 Spring 专有的. 由于 AspectJ 得到越来越多的 AOP 框架支持, 所以以注解风格编写的切面将会有更多重用的机会. 基于XML—- 声明切面XML 当使用 XML 声明切面时, 需要在&lt;beans&gt;根元素中导入 aop Schema 在 Bean 配置文件中, 所有的 Spring AOP 配置都必须定义在 &lt;aop:config&gt; 元素内部. 对于每个切面而言, 都要创建一个 &lt;aop:aspect&gt; 元素来为具体的切面实现引用后端 Bean 实例. 切面 Bean 必须有一个标示符, 供 &lt;aop:aspect&gt; 元素引用 切入点 切入点使用&lt;aop:pointcut&gt;元素声明 切入点必须定义在 &lt;aop:aspect&gt; 元素下, 或者直接定义在 &lt;aop:config&gt; 元素下. 定义在 &lt;aop:aspect&gt; 元素下: 只对当前切面有效 定义在 &lt;aop:config&gt; 元素下: 对所有切面都有效 基于 XML 的 AOP 配置不允许在切入点表达式中用名称引用其他切入点. 基于XML—-声明通知 在 aop Schema 中, 每种通知类型都对应一个特定的 XML 元素. 通知元素需要使用 &lt;pointcut-ref&gt; 来引用切入点, 或用 &lt;pointcut&gt; 直接嵌入切入点表达式. method 属性指定切面类中通知方法的名称. 声明引入 可以利用 &lt;aop:declare-parents&gt; 元素在切面内部声明引入 源码AOP需求创建接口AtithmeticCalculator.java12345678910package com.atguigu.spring.aop.helloworld;public interface AtithmeticCalculator &#123; int add(int i,int j); int sub(int i,int j); int mul(int i,int j); int div(int i,int j);&#125; 对接口进行实例化AtithmeticCalculatorImpl.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package com.atguigu.spring.aop.helloworld;public class AtithmeticCalculatorImpl implements AtithmeticCalculator &#123; @Override public int add(int i, int j) &#123; &#x2F;&#x2F; TODO Auto-generated method stub System.out.println(&quot;日志：The method add begins with [&quot; + i + &quot;,&quot;+j+ &quot;] &quot;);&#x2F;&#x2F;日志代码 int result &#x3D; i+j; System.out.println(&quot;日志：The method add ends with &quot;+ result); return result; &#125; @Override public int sub(int i, int j) &#123; &#x2F;&#x2F; TODO Auto-generated method stub System.out.println(&quot;日志：The method sub begins with [&quot; + i + &quot;,&quot;+j+ &quot;] &quot;); int result &#x3D; i-j; System.out.println(&quot;日志：The method sub ends with &quot;+ result); return result; &#125; @Override public int mul(int i, int j) &#123; &#x2F;&#x2F; TODO Auto-generated method stub System.out.println(&quot;日志：The method mul begins with [&quot; + i + &quot;,&quot;+j+ &quot;] &quot;); int result &#x3D; i*j; System.out.println(&quot;日志：The method mul ends with &quot;+ result); return result; &#125; @Override public int div(int i, int j) &#123; &#x2F;&#x2F; TODO Auto-generated method stub System.out.println(&quot;日志：The method div begins with [&quot; + i + &quot;,&quot;+j+ &quot;] &quot;); int result &#x3D; i&#x2F;j; System.out.println(&quot;日志：The method div ends with &quot;+ result); return result; &#125;&#125; 编写测试主方法Main，java12345678910111213141516package com.atguigu.spring.aop.helloworld;public class Main &#123; public static void main(String[] args) &#123; &#x2F;&#x2F; TODO Auto-generated method stub AtithmeticCalculator atithmeticCalculator &#x3D; new AtithmeticCalculatorImpl(); int result &#x3D; atithmeticCalculator.add(1, 2); System.out.println(&quot;--&gt;&quot;+ result); atithmeticCalculator.div(4, 2); System.out.println(&quot;--&gt;&quot;+ result); &#125;&#125; 测试结果1234567日志：The method add begins with [1,2]日志：The method add ends with 3--&gt;3日志：The method div begins with [4,2]日志：The method div ends with 2--&gt;3 动态代理方式处理日志问题创建上一个接口的实现类ArithmeticCalculatorLoggingImpl.java123456789101112131415161718192021222324252627282930313233343536package com.atguigu.spring.aop.helloworld;public class ArithmeticCalculatorLoggingImpl implements ArithmeticCalculator &#123;&#x2F;** * 动态代理的方法处理日志问题 *&#x2F; @Override public int add(int i, int j) &#123; &#x2F;&#x2F; TODO Auto-generated method stub int result &#x3D; i+j; return result; &#125; @Override public int sub(int i, int j) &#123; &#x2F;&#x2F; TODO Auto-generated method stub int result &#x3D; i-j; return result; &#125; @Override public int mul(int i, int j) &#123; &#x2F;&#x2F; TODO Auto-generated method stub int result &#x3D; i*j; return result; &#125; @Override public int div(int i, int j) &#123; &#x2F;&#x2F; TODO Auto-generated method stub int result &#x3D; i&#x2F;j; return result; &#125;&#125; 动态代理文件ArithmeticCalculatorLoggingProxy.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.atguigu.spring.aop.helloworld;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.util.Arrays;public class ArithmeticCalculatorLoggingProxy &#123; &#x2F;&#x2F;代理的对象 private ArithmeticCalculator target; public ArithmeticCalculatorLoggingProxy(ArithmeticCalculator target) &#123; &#x2F;&#x2F; TODO Auto-generated constructor stub this.target &#x3D; target; &#125; public ArithmeticCalculator getLoggingProxy() &#123; ArithmeticCalculator proxy &#x3D; null; &#x2F;&#x2F;代理器由哪一个类加载器负责加载 ClassLoader loader &#x3D; target.getClass().getClassLoader(); &#x2F;&#x2F;代理对象的类型 其中有哪些方法 Class [] interfaces &#x3D; new Class[] &#123;ArithmeticCalculator.class&#125;; &#x2F;&#x2F; 当调用代理对象其中的方法时 该执行的代码在这里面 InvocationHandler h &#x3D;new InvocationHandler() &#123; &#x2F;** * * proxy:正在返回的对象，一般情况下，在invoke 方法中都不使用该对象。 * method: 正在被调用的方法 * args: 调用方法时，传入的参数 *&#x2F; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; &#x2F;&#x2F; TODO Auto-generated method stub String methodName &#x3D; method.getName(); &#x2F;&#x2F;日志 System.out.println(&quot;The method&quot;+&quot; &quot;+ methodName +&quot; &quot;+&quot;begin with&quot; + Arrays.asList(args)); &#x2F;&#x2F;执行 Object resuObject &#x3D; method.invoke(target, args); &#x2F;&#x2F;日志 System.out.println(&quot;The method &quot;+ &quot; &quot;+ methodName +&quot; &quot;+ &quot;end with&quot; + resuObject); return resuObject; &#125; &#125;; proxy &#x3D; (ArithmeticCalculator) Proxy.newProxyInstance(loader, interfaces, h); return proxy; &#125;&#125; 主方法12345678910111213141516package com.atguigu.spring.aop.helloworld;public class Main &#123; public static void main(String[] args) &#123; &#x2F;&#x2F; TODO Auto-generated method stub &#x2F;&#x2F; 使用动态代理 ArithmeticCalculator target &#x3D; new ArithmeticCalculatorImpl(); ArithmeticCalculator proxy &#x3D; new ArithmeticCalculatorLoggingProxy(target).getLoggingProxy(); int result &#x3D; proxy.add(1, 2); System.out.println(&quot;--&gt;&quot;+ result); proxy.div(4, 2); System.out.println(&quot;--&gt;&quot;+ result); &#125;&#125; 在 Spring 中启用 AspectJ 注解支持源码首先导入如图所示的包 创建包com.atguigu.spring.aop.impl创建接口文件ArithmeticCalculator.java123456789package com.atguigu.spring.aop.impl;public interface ArithmeticCalculator &#123; int add(int i,int j); int sub(int i,int j); int mul(int i,int j); int div(int i,int j);&#125; ArithmeticCalculatorImpl.java123456789101112131415161718192021222324252627282930313233343536package com.atguigu.spring.aop.impl;import org.springframework.stereotype.Component;@Componentpublic class ArithmeticCalculatorImpl implements ArithmeticCalculator &#123; @Override public int add(int i, int j) &#123; &#x2F;&#x2F; TODO Auto-generated method stub int result &#x3D; i+j; return result; &#125; @Override public int sub(int i, int j) &#123; &#x2F;&#x2F; TODO Auto-generated method stub int result &#x3D; i-j; return result; &#125; @Override public int mul(int i, int j) &#123; &#x2F;&#x2F; TODO Auto-generated method stub int result &#x3D; i*j; return result; &#125; @Override public int div(int i, int j) &#123; &#x2F;&#x2F; TODO Auto-generated method stub int result &#x3D; i&#x2F;j; return result; &#125;&#125; LoggingAspect.java1234567891011121314151617181920212223242526272829303132333435package com.atguigu.spring.aop.impl;import java.util.Arrays;import java.util.List;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.annotation.After;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;import org.springframework.stereotype.Component;&#x2F;&#x2F;把这个类声明成一个切面：需要把该类放入到IOC容器中，再声明为一个切面@Component@Aspectpublic class LoggingAspect &#123; &#x2F;&#x2F;声明该方法是一个前置通知 : 在目标方法开始之前执行 @Before(&quot;execution(public int com.atguigu.spring.aop.impl.ArithmeticCalculator.*(int,int))&quot;) public void beforemethod(JoinPoint joinPoint) &#123; String methodName &#x3D; joinPoint.getSignature().getName(); List&lt;Object&gt; argsList&#x3D; Arrays.asList(joinPoint.getArgs()); System.out.println(&quot;The method &quot;+ methodName + &quot;begins with&quot; + argsList); &#125; &#x2F;&#x2F;后置通知: 在目标方法执行后(无论是否发生异常)，执行通知 &#x2F;&#x2F;在后置通知中还不能访问目标方法的执行结果 @After(&quot;execution(* com.atguigu.spring.aop.impl.*.*(int,int))&quot;) public void afterMethod(JoinPoint joinPoint) &#123; String methodName &#x3D; joinPoint.getSignature().getName(); System.out.println(&quot;The Method &quot;+ methodName +&quot; ends&quot;); &#125;&#125; applicationContext.xml123456789101112131415&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xmlns:aop&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;aop&quot; xmlns:context&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans.xsd http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&#x2F;spring-context-4.3.xsd http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;aop http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;aop&#x2F;spring-aop-4.3.xsd&quot;&gt; &lt;!-- 配置自动扫描的包 --&gt; &lt;context:component-scan base-package&#x3D;&quot;com.atguigu.spring.aop.impl&quot;&gt;&lt;&#x2F;context:component-scan&gt; &lt;!-- 使AspectJ 注解起作用：自动为匹配的可生成代理对象 --&gt; &lt;aop:aspectj-autoproxy&gt;&lt;&#x2F;aop:aspectj-autoproxy&gt;&lt;&#x2F;beans&gt; Main.java1234567891011121314151617181920212223242526package com.atguigu.spring.aop.impl;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class Main &#123; public static void main(String[] args) &#123; &#x2F;&#x2F;1.创建Spring的IOC容器 ApplicationContext ctx &#x3D; new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); &#x2F;&#x2F;2.从IOC容器中获取Bean实例 ArithmeticCalculator arithmeticCalculator &#x3D; ctx.getBean(ArithmeticCalculator.class); &#x2F;&#x2F;3.使用Bean int result &#x3D; arithmeticCalculator.add(3, 6); System.out.println(&quot;result:&quot; + result); result &#x3D; arithmeticCalculator.div(12, 6);&#x2F;&#x2F;这里即使是除以0也会有后置通知，只不过result没有值 System.out.println(&quot;result:&quot; + result); &#125;&#125; 运行结果1234567The method addbegins with[3, 6]The Method add endsresult:9The method divbegins with[12, 6]The Method div endsresult:2 返回通知，异常那通知，后置通知源码创建一个新的包com.atguigu.spring.aop接口ArithmeticCalculator.java12345678910package com.atguigu.spring.aop;public interface ArithmeticCalculator &#123; int add(int i,int j); int sub(int i,int j); int mul(int i,int j); int div(int i,int j);&#125; ArithmeticCalculatorImpl.java123456789101112131415161718192021222324252627282930313233343536package com.atguigu.spring.aop;import org.springframework.stereotype.Component;@Component(&quot;arithmeticCalculator&quot;)public class ArithmeticCalculatorImpl implements ArithmeticCalculator &#123; @Override public int add(int i, int j) &#123; &#x2F;&#x2F; TODO Auto-generated method stub int result &#x3D; i+j; return result; &#125; @Override public int sub(int i, int j) &#123; &#x2F;&#x2F; TODO Auto-generated method stub int result &#x3D; i-j; return result; &#125; @Override public int mul(int i, int j) &#123; &#x2F;&#x2F; TODO Auto-generated method stub int result &#x3D; i*j; return result; &#125; @Override public int div(int i, int j) &#123; &#x2F;&#x2F; TODO Auto-generated method stub int result &#x3D; i&#x2F;j; return result; &#125;&#125; LoggingAspect.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697package com.atguigu.spring.aop;import java.util.Arrays;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.After;import org.aspectj.lang.annotation.AfterReturning;import org.aspectj.lang.annotation.AfterThrowing;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;import org.springframework.stereotype.Component;@Aspect@Componentpublic class LoggingAspect &#123; &#x2F;** * 在com.atguigu.spring.aop.ArithmeticCalculator 接口的每一个实现类的每一个方法开始之前执行一段代码 *&#x2F; @Before(&quot;execution(public int com.atguigu.spring.aop.ArithmeticCalculator.*(..))&quot;) public void beforeMethod(JoinPoint joinPoint) &#123; &#x2F;&#x2F; TODO Auto-generated method stub String methodName &#x3D; joinPoint.getSignature().getName(); Object[] args &#x3D; joinPoint.getArgs(); System.out.println(&quot;The method &quot;+ methodName+&quot; begins with &quot;+Arrays.asList(args)); &#125; &#x2F;**后置通知 * 在方法执行之后执行的代码，无论该方法是否出现异常 *&#x2F; @After(&quot;execution(public int com.atguigu.spring.aop.ArithmeticCalculator.*(..))&quot;) public void afterMethod(JoinPoint joinPoint) &#123; String methodName &#x3D; joinPoint.getSignature().getName(); System.out.println(&quot;The method &quot;+ methodName+&quot; ends&quot;); &#125; &#x2F;** * 在方法正常结束受执行的代码 * 返回通知是可以访问到方法的返回值的 *&#x2F; @AfterReturning(value&#x3D;&quot;execution(public int com.atguigu.spring.aop.ArithmeticCalculator.*(..))&quot;, returning&#x3D;&quot;result&quot;) public void afterReturning(JoinPoint joinPoint,Object result) &#123; String methodName &#x3D; joinPoint.getSignature().getName(); System.out.println(&quot;The method &quot;+ methodName+&quot; ends with &quot;+result); &#125; &#x2F;** * 异常通知，在出异常的时候才会出现，例如div的时候除以0等 * 可以访问到异常对象；且可以指定在出现特定异常时在执行通知代码 *&#x2F; @AfterThrowing(value&#x3D;&quot;execution(public int com.atguigu.spring.aop.ArithmeticCalculator.*(..))&quot;, throwing&#x3D;&quot;ex&quot;) &#x2F;&#x2F;可以将Exception换成NullPointerException则只有空指针异常时才执行 public void afterThrowing(JoinPoint joinPoint,Exception ex) &#123; String methodName &#x3D; joinPoint.getSignature().getName(); System.out.println(&quot;The method &quot;+ methodName+&quot; occurs excetion: &quot;+ex); &#125; &#x2F;** * 环绕通知是最强的，但却不是最常用的 * 环绕通知需要携带ProceedingJoinPoint类型的参数， * 环绕通知类似于动态代理的全过程：ProceedingJoinPoint 类型的参数可以决定是否执行目标方法 * 环绕通知必须有返回值，返回值即为目标方法的返回值 * @param pjdJoinPoint *&#x2F;&#x2F;* @Around(&quot;execution(public int com.atguigu.spring.aop.ArithmeticCalculator.*(..))&quot;) public Object aroundMethod(ProceedingJoinPoint pjdJoinPoint) &#123; Object result &#x3D; null; String methodName &#x3D; pjdJoinPoint.getSignature().getName(); &#x2F;&#x2F;这下面写的几种通知和上面效果一样，因此打印的时候会打印两遍 &#x2F;&#x2F;执行目标方法 try &#123; &#x2F;&#x2F;前置通知 System.out.println(&quot;The method &quot;+ methodName+&quot; beging with &quot;+Arrays.asList(pjdJoinPoint.getArgs())); result &#x3D; pjdJoinPoint.proceed(); &#x2F;&#x2F;返回通知 System.out.println(&quot;The method &quot;+ methodName+&quot; ends&quot; +result); &#125; catch (Throwable e) &#123; &#x2F;&#x2F; TODO Auto-generated catch block e.printStackTrace(); &#x2F;&#x2F;异常通知 System.out.println(&quot;The method &quot;+ methodName+&quot; occurs excetion: &quot;+e); throw new RuntimeException(e); &#125; &#x2F;&#x2F;后置通知 System.out.println(&quot;The method &quot;+ methodName+&quot; ends&quot; +result); return result; &#125;*&#x2F;&#125; applicationContext.xml1234567891011121314151617&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xmlns:context&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&quot; xmlns:aop&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;aop&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans.xsd http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&#x2F;spring-context-4.3.xsd http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;aop http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;aop&#x2F;spring-aop-4.3.xsd&quot;&gt;&lt;!-- 配置自动扫描的包 --&gt; &lt;context:component-scan base-package&#x3D;&quot;com.atguigu.spring.aop&quot;&gt;&lt;&#x2F;context:component-scan&gt;&lt;!-- 配置自动为匹配aspectj 注释的java类生成对象 --&gt;&lt;aop:aspectj-autoproxy&gt;&lt;&#x2F;aop:aspectj-autoproxy&gt;&lt;&#x2F;beans&gt; Main.java1234567891011121314151617181920212223242526package com.atguigu.spring.aop;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class Main &#123; public static void main(String[] args) &#123; &#x2F;&#x2F; TODO Auto-generated method stub ApplicationContext ctxApplicationContext &#x3D; new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); &#x2F;&#x2F;getBean()里是component里填的内容 ArithmeticCalculator arithmeticCalculator &#x3D; (ArithmeticCalculator) ctxApplicationContext.getBean(&quot;arithmeticCalculator&quot;); &#x2F;&#x2F;看有没有被代理 System.out.println(arithmeticCalculator.getClass().getName()); int result &#x3D; arithmeticCalculator.add(1, 2); System.out.println(&quot;result:&quot; + result); result &#x3D; arithmeticCalculator.div(10, 2);&#x2F;&#x2F;这里即使是除以0也会有后置通知，只不过result没有值 System.out.println(&quot;result:&quot; + result); &#125;&#125; 运行结果12345678910com.sun.proxy.$Proxy19The method add begins with [1, 2]The method add endsThe method add ends with 3result:3The method div begins with [10, 2]The method div endsThe method div ends with 5result:5 设置切面优先级源码VlidationAspect.java12345678910111213141516171819202122232425262728package com.atguigu.spring.aop;import java.util.Arrays;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;import org.springframework.core.annotation.Order;import org.springframework.stereotype.Component;&#x2F;&#x2F;验证切面,用于指定切面的优先级&#x2F;** * 可以使用@Order指定切面的优先级，值越小优先级越高 * * *&#x2F;@Order(1)@Aspect@Componentpublic class VlidationAspect &#123; @Before(&quot;execution(public int com.atguigu.spring.aop.ArithmeticCalculator.*(..))&quot;) public void vlidationAspect(JoinPoint joinPoint) &#123; System.out.println(&quot;validate: &quot;+ Arrays.asList(joinPoint.getArgs())); &#125;&#125; 在之前的切面文件LoggingAspect.java 加上@Order(2)注解测试结果123456789101112com.sun.proxy.$Proxy20validate: [1, 2]The method add begins with [1, 2]The method add endsThe method add ends with 3result:3validate: [10, 2]The method div begins with [10, 2]The method div endsThe method div ends with 5result:5 重用切入点源码将LoggingAspect.java做如下修改123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120package com.atguigu.spring.aop;import java.util.Arrays;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.After;import org.aspectj.lang.annotation.AfterReturning;import org.aspectj.lang.annotation.AfterThrowing;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;import org.aspectj.lang.annotation.Pointcut;import org.springframework.core.annotation.Order;import org.springframework.stereotype.Component;&#x2F;** * 可以使用@Order指定切面的优先级，值越小优先级越高 * * *&#x2F;@Order(2)@Aspect@Componentpublic class LoggingAspect &#123; &#x2F;** * 定义一个方法用于声明切入点表达式。一般该方法中不需要添加其他的代码 * 使用@Pointcut来声明切入点表达式 * 后面的其他通知直接使用方法名来引用当前的切入点表达式。别的包的如需引用前面需要加上包名 *&#x2F; @Pointcut(&quot;execution(public int com.atguigu.spring.aop.ArithmeticCalculator.*(..))&quot;) public void declareJointPointExpression() &#123; &#125; &#x2F;** * 在com.atguigu.spring.aop.ArithmeticCalculator 接口的每一个实现类的每一个方法开始之前执行一段代码 *&#x2F; @Before(&quot;declareJointPointExpression()&quot;) public void beforeMethod(JoinPoint joinPoint) &#123; &#x2F;&#x2F; TODO Auto-generated method stub String methodName &#x3D; joinPoint.getSignature().getName(); Object[] args &#x3D; joinPoint.getArgs(); System.out.println(&quot;The method &quot;+ methodName+&quot; begins with &quot;+Arrays.asList(args)); &#125; &#x2F;**后置通知 * 在方法执行之后执行的代码，无论该方法是否出现异常 *&#x2F; @After(&quot;declareJointPointExpression()&quot;) public void afterMethod(JoinPoint joinPoint) &#123; String methodName &#x3D; joinPoint.getSignature().getName(); System.out.println(&quot;The method &quot;+ methodName+&quot; ends&quot;); &#125; &#x2F;** * 在方法正常结束受执行的代码 * 返回通知是可以访问到方法的返回值的 *&#x2F; @AfterReturning(value&#x3D;&quot;declareJointPointExpression()&quot;, returning&#x3D;&quot;result&quot;) public void afterReturning(JoinPoint joinPoint,Object result) &#123; String methodName &#x3D; joinPoint.getSignature().getName(); System.out.println(&quot;The method &quot;+ methodName+&quot; ends with &quot;+result); &#125; &#x2F;** * 异常通知，在出异常的时候才会出现，例如div的时候除以0等 * 可以访问到异常对象；且可以指定在出现特定异常时在执行通知代码 *&#x2F; @AfterThrowing(value&#x3D;&quot;declareJointPointExpression()&quot;, throwing&#x3D;&quot;ex&quot;) &#x2F;&#x2F;可以将Exception换成NullPointerException则只有空指针异常时才执行 public void afterThrowing(JoinPoint joinPoint,Exception ex) &#123; String methodName &#x3D; joinPoint.getSignature().getName(); System.out.println(&quot;The method &quot;+ methodName+&quot; occurs excetion: &quot;+ex); &#125; &#x2F;** * 环绕通知是最强的，但却不是最常用的 * 环绕通知需要携带ProceedingJoinPoint类型的参数， * 环绕通知类似于动态代理的全过程：ProceedingJoinPoint 类型的参数可以决定是否执行目标方法 * 环绕通知必须有返回值，返回值即为目标方法的返回值 * @param pjdJoinPoint *&#x2F;&#x2F;* @Around(&quot;execution(public int com.atguigu.spring.aop.ArithmeticCalculator.*(..))&quot;) public Object aroundMethod(ProceedingJoinPoint pjdJoinPoint) &#123; Object result &#x3D; null; String methodName &#x3D; pjdJoinPoint.getSignature().getName(); &#x2F;&#x2F;这下面写的几种通知和上面效果一样，因此打印的时候会打印两遍 &#x2F;&#x2F;执行目标方法 try &#123; &#x2F;&#x2F;前置通知 System.out.println(&quot;The method &quot;+ methodName+&quot; beging with &quot;+Arrays.asList(pjdJoinPoint.getArgs())); result &#x3D; pjdJoinPoint.proceed(); &#x2F;&#x2F;返回通知 System.out.println(&quot;The method &quot;+ methodName+&quot; ends&quot; +result); &#125; catch (Throwable e) &#123; &#x2F;&#x2F; TODO Auto-generated catch block e.printStackTrace(); &#x2F;&#x2F;异常通知 System.out.println(&quot;The method &quot;+ methodName+&quot; occurs excetion: &quot;+e); throw new RuntimeException(e); &#125; &#x2F;&#x2F;后置通知 System.out.println(&quot;The method &quot;+ methodName+&quot; ends&quot; +result); return result; &#125;*&#x2F;&#125; 将VlidationAspect.java做如下修改12345678910111213141516171819202122232425262728package com.atguigu.spring.aop;import java.util.Arrays;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Before;import org.springframework.core.annotation.Order;import org.springframework.stereotype.Component;&#x2F;&#x2F;验证切面,用于指定切面的优先级&#x2F;** * 可以使用@Order指定切面的优先级，值越小优先级越高 * * *&#x2F;@Order(1)@Aspect@Componentpublic class VlidationAspect &#123; @Before(&quot;com.atguigu.spring.aop.LoggingAspect.declareJointPointExpression()&quot;) public void vlidationAspect(JoinPoint joinPoint) &#123; System.out.println(&quot;validate: &quot;+ Arrays.asList(joinPoint.getArgs())); &#125;&#125; 基于XML的配置声明切面的测试代码新建包com.atguigu.spring.aop.xmlArithmeticCalculator.java12345678910package com.atguigu.spring.aop.xml;public interface ArithmeticCalculator &#123; int add(int i,int j); int sub(int i,int j); int mul(int i,int j); int div(int i,int j);&#125; ArithmeticCalculatorImpl.java123456789101112131415161718192021222324252627282930313233package com.atguigu.spring.aop.xml;public class ArithmeticCalculatorImpl implements ArithmeticCalculator &#123; @Override public int add(int i, int j) &#123; &#x2F;&#x2F; TODO Auto-generated method stub int result &#x3D; i+j; return result; &#125; @Override public int sub(int i, int j) &#123; &#x2F;&#x2F; TODO Auto-generated method stub int result &#x3D; i-j; return result; &#125; @Override public int mul(int i, int j) &#123; &#x2F;&#x2F; TODO Auto-generated method stub int result &#x3D; i*j; return result; &#125; @Override public int div(int i, int j) &#123; &#x2F;&#x2F; TODO Auto-generated method stub int result &#x3D; i&#x2F;j; return result; &#125;&#125; LoggingAspect.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package com.atguigu.spring.aop.xml;import java.util.Arrays;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.ProceedingJoinPoint;public class LoggingAspect &#123; public void beforeMethod(JoinPoint joinPoint) &#123; &#x2F;&#x2F; TODO Auto-generated method stub String methodName &#x3D; joinPoint.getSignature().getName(); Object[] args &#x3D; joinPoint.getArgs(); System.out.println(&quot;The method &quot;+ methodName+&quot; begins with &quot;+Arrays.asList(args)); &#125; public void afterMethod(JoinPoint joinPoint) &#123; String methodName &#x3D; joinPoint.getSignature().getName(); System.out.println(&quot;The method &quot;+ methodName+&quot; ends&quot;); &#125; public void afterReturning(JoinPoint joinPoint,Object result) &#123; String methodName &#x3D; joinPoint.getSignature().getName(); System.out.println(&quot;The method &quot;+ methodName+&quot; ends with &quot;+result); &#125; public void afterThrowing(JoinPoint joinPoint,Exception ex) &#123; String methodName &#x3D; joinPoint.getSignature().getName(); System.out.println(&quot;The method &quot;+ methodName+&quot; occurs excetion: &quot;+ex); &#125; public Object aroundMethod(ProceedingJoinPoint pjdJoinPoint) &#123; Object result &#x3D; null; String methodName &#x3D; pjdJoinPoint.getSignature().getName(); &#x2F;&#x2F;这下面写的几种通知和上面效果一样，因此打印的时候会打印两遍 &#x2F;&#x2F;执行目标方法 try &#123; &#x2F;&#x2F;前置通知 System.out.println(&quot;The method &quot;+ methodName+&quot; beging with &quot;+Arrays.asList(pjdJoinPoint.getArgs())); result &#x3D; pjdJoinPoint.proceed(); &#x2F;&#x2F;返回通知 System.out.println(&quot;The method &quot;+ methodName+&quot; ends&quot; +result); &#125; catch (Throwable e) &#123; &#x2F;&#x2F; TODO Auto-generated catch block e.printStackTrace(); &#x2F;&#x2F;异常通知 System.out.println(&quot;The method &quot;+ methodName+&quot; occurs excetion: &quot;+e); throw new RuntimeException(e); &#125; &#x2F;&#x2F;后置通知 System.out.println(&quot;The method &quot;+ methodName+&quot; ends&quot; +result); return result; &#125;&#125; VlidationAspect.java1234567891011121314package com.atguigu.spring.aop.xml;import java.util.Arrays;import org.aspectj.lang.JoinPoint;public class VlidationAspect &#123; public void validateArgs(JoinPoint joinPoint) &#123; System.out.println(&quot;validate: &quot;+ Arrays.asList(joinPoint.getArgs())); &#125;&#125; applicationContext-xml.xml1234567891011121314151617181920212223242526272829303132&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xmlns:aop&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;aop&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans.xsd http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;aop http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;aop&#x2F;spring-aop-4.3.xsd&quot;&gt;&lt;!-- 配置Bean --&gt; &lt;bean id&#x3D;&quot;arithmeticCalculator&quot; class&#x3D;&quot;com.atguigu.spring.aop.xml.ArithmeticCalculatorImpl&quot;&gt;&lt;&#x2F;bean&gt;&lt;!-- 配置切面的bean --&gt; &lt;bean id&#x3D;&quot;loggingAspect&quot; class&#x3D;&quot;com.atguigu.spring.aop.xml.LoggingAspect&quot;&gt;&lt;&#x2F;bean&gt; &lt;bean id&#x3D;&quot;vlidationAspect&quot; class&#x3D;&quot;com.atguigu.spring.aop.xml.VlidationAspect&quot;&gt;&lt;&#x2F;bean&gt;&lt;!-- 配置AOP --&gt; &lt;aop:config&gt; &lt;!-- 配置切点表达式 --&gt; &lt;aop:pointcut expression&#x3D;&quot;execution(* com.atguigu.spring.aop.xml.ArithmeticCalculator.*(int,int))&quot; id&#x3D;&quot;pointcut&quot;&#x2F;&gt; &lt;!-- 配置切面及通知 --&gt; &lt;aop:aspect ref&#x3D;&quot;loggingAspect&quot; order&#x3D;&quot;2&quot;&gt; &lt;aop:before method&#x3D;&quot;beforeMethod&quot; pointcut-ref&#x3D;&quot;pointcut&quot;&#x2F;&gt; &lt;aop:after method&#x3D;&quot;afterMethod&quot; pointcut-ref&#x3D;&quot;pointcut&quot;&#x2F;&gt; &lt;aop:after-throwing method&#x3D;&quot;afterThrowing&quot; pointcut-ref&#x3D;&quot;pointcut&quot; throwing&#x3D;&quot;ex&quot;&#x2F;&gt; &lt;aop:after-returning method&#x3D;&quot;afterReturning&quot; pointcut-ref&#x3D;&quot;pointcut&quot; returning&#x3D;&quot;result&quot;&#x2F;&gt; &lt;!-- &lt;aop:around method&#x3D;&quot;aroundMethod&quot; pointcut-ref&#x3D;&quot;pointcut&quot;&#x2F;&gt; --&gt; &lt;&#x2F;aop:aspect&gt; &lt;aop:aspect ref&#x3D;&quot;vlidationAspect&quot; order&#x3D;&quot;1&quot;&gt; &lt;aop:before method&#x3D;&quot;validateArgs&quot; pointcut-ref&#x3D;&quot;pointcut&quot;&#x2F;&gt; &lt;&#x2F;aop:aspect&gt; &lt;&#x2F;aop:config&gt;&lt;&#x2F;beans&gt; 测试结果123456789101112com.sun.proxy.$Proxy4validate: [1, 2]The method add begins with [1, 2]The method add endsThe method add ends with 3result:3validate: [10, 2]The method div begins with [10, 2]The method div endsThe method div ends with 5result:5","categories":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"ssm","slug":"ssm","permalink":"http://www.studyz.club/tags/ssm/"},{"name":"Spring","slug":"Spring","permalink":"http://www.studyz.club/tags/Spring/"},{"name":"AOP","slug":"AOP","permalink":"http://www.studyz.club/tags/AOP/"}]},{"title":"软件开发框架之spring二","slug":"软件开发框架之Spring二","date":"2019-10-05T02:00:05.236Z","updated":"2019-11-09T08:13:43.642Z","comments":true,"path":"posts/4aec9fb8/","link":"","permalink":"http://www.studyz.club/posts/4aec9fb8/","excerpt":"","text":"一 自动装配 1.1 XML 配置里的 Bean 自动装配 1.1.2 自动装配源码 二， bean 之间的关系：继承；依赖 2.1 继承 Bean 配置 2.2 依赖 Bean 配置 2.3 bean 之间的关系：继承；依赖–源码 三， bean 的作用域：singleton；prototype；WEB 环境作用域 3.1 Bean 的作用域 3.2 Bean 的作用域-源码 四， 使用外部属性文件 五， Spring表达式语言：SpEL 5.1 知识点 5.2 spEl测试用例源码 六， IOC容器中Bean的生命周期 6.1 IOC 容器中 Bean 的生命周期方法 6.2 创建 Bean 后置处理器 6.3 生命周期测试源码 6.4 生命周期测试源码的源码 一， 自动装配1.1 XML 配置里的 Bean 自动装配 byName配置文件123456789&lt;bean id&#x3D;&quot;adress&quot; class&#x3D;&quot;com.atguigu.spring.beans.autowire.Address&quot; p:city&#x3D;&quot;Beijing&quot; p:street&#x3D;&quot;HuilongGuan&quot;&gt;&lt;&#x2F;bean&gt; &lt;bean id&#x3D;&quot;car&quot; class&#x3D;&quot;com.atguigu.spring.beans.autowire.Car&quot; p:brand&#x3D;&quot;Audi&quot; p:price&#x3D;&quot;300000&quot;&gt;&lt;&#x2F;bean&gt; &lt;!-- byName自动装配Person里有对应的adress,car 指向配置文件的id名 即根据bean的名字和当前bean的setter风格的属性名进行自动装配--&gt; &lt;bean id&#x3D;&quot;person&quot; class&#x3D;&quot;com.atguigu.spring.beans.autowire.Person&quot; p:name&#x3D;&quot;Tome&quot; autowire&#x3D;&quot;byName&quot;&gt;&lt;&#x2F;bean&gt; 运行结果(如果配置文件里的id和Person类里的不对应则会指向null，例如id=”address2”,通过byName装配运行结果的address=null) 1Person [name&#x3D;Tome, address&#x3D;Address [city&#x3D;Beijing, street&#x3D;HuilongGuan], car&#x3D;Car [brand&#x3D;Audi, price&#x3D;300000.0]] byType配置文件如果是按类型装配，当前Bean（IOC容器中）中有两个类型一样的，则无法装配。例如id=&quot;address&quot;和id=&quot;address2&quot;会报错。1234567 &lt;bean id&#x3D;&quot;adress&quot; class&#x3D;&quot;com.atguigu.spring.beans.autowire.Address&quot; p:city&#x3D;&quot;Beijing&quot; p:street&#x3D;&quot;HuilongGuan&quot;&gt;&lt;&#x2F;bean&gt; &lt;bean id&#x3D;&quot;car&quot; class&#x3D;&quot;com.atguigu.spring.beans.autowire.Car&quot; p:brand&#x3D;&quot;Audi&quot; p:price&#x3D;&quot;300000&quot;&gt;&lt;&#x2F;bean&gt;&lt;!-- byType 的类型和当前bean的属性的类型进行自动装配 --&gt; &lt;bean id&#x3D;&quot;person&quot; class&#x3D;&quot;com.atguigu.spring.beans.autowire.Person&quot; p:name&#x3D;&quot;Tom&quot; autowire&#x3D;&quot;byType&quot;&gt;&lt;&#x2F;bean&gt; 运行结果 12Person [name&#x3D;Tom, address&#x3D;Address [city&#x3D;Beijing, street&#x3D;HuilongGuan], car&#x3D;Car [brand&#x3D;Audi, price&#x3D;300000.0]] 二， bean 之间的关系：继承；依赖2.1 继承 Bean 配置 12345&lt;bean id&#x3D;&quot;address&quot; class&#x3D;&quot;com.atguigu.spring.beans.autowire.Address&quot; p:city&#x3D;&quot;Beijing&quot; p:street&#x3D;&quot;WuDaoKou&quot;&gt;&lt;&#x2F;bean&gt; &lt;!-- bean 配置的继承：使用Bean的parent属性指定继承那个属性的配置 --&gt; &lt;bean id&#x3D;&quot;address2&quot; p:street&#x3D;&quot;TianMen&quot; parent&#x3D;&quot;address&quot;&gt;&lt;&#x2F;bean&gt; 运行结果 12Address [city&#x3D;Beijing, street&#x3D;WuDaoKou]Address [city&#x3D;Beijing, street&#x3D;TianMen] 如果给id=”address”加上abstract=“true”,则不能进行实例化.只有adress2能实例化能运行,只能用来被继承。如果一个Bean的class属性没有被指定，则该Bean必须是一个抽象Bean即带abstract属性 1Address [city&#x3D;Beijing, street&#x3D;TianMen] 2.2 依赖 Bean 配置 12345678&lt;!-- bean 配置的继承：使用Bean的parent属性指定继承那个属性的配置 --&gt; &lt;bean id&#x3D;&quot;address2&quot; p:street&#x3D;&quot;TianMen&quot; parent&#x3D;&quot;address&quot;&gt;&lt;&#x2F;bean&gt; &lt;bean id&#x3D;&quot;car&quot; class&#x3D;&quot;com.atguigu.spring.beans.autowire.Car&quot; p:brand&#x3D;&quot;Audi&quot; p:price&#x3D;&quot;30000&quot;&gt;&lt;&#x2F;bean&gt; &lt;!-- 要求在配置person的时候，必须有一个关联的car!换句话说person这个bean依赖于Car的这个Bean --&gt; &lt;bean id&#x3D;&quot;person&quot; class&#x3D;&quot;com.atguigu.spring.beans.autowire.Person&quot; p:name&#x3D;&quot;Tom&quot; p:address-ref&#x3D;&quot;address2&quot; depends-on&#x3D;&quot;car&quot;&gt;&lt;&#x2F;bean&gt; 运行结果 1Person [name&#x3D;Tom, address&#x3D;Address [city&#x3D;Beijing, street&#x3D;TianMen], car&#x3D;null] 三， bean 的作用域：singleton；prototype；WEB 环境作用域3.1 Bean 的作用域singleton和prototype是最常用的两种作用域 singleton是Spring容器默认的作用域，当Bean的作用域为singleton时，Spring容器就只会存在一个共享的Bean实例。singleton作用域对于无会话状态的Bean（如Dao 组件、Service组件）来说，是最理想的选择。 对需要保持会话状态的Bean（如Struts 2的Action类）应该使用prototype作用域。在使用prototype作用域时，Spring容器会为每个对该Bean的请求都创建一个新的实例。 先在包com.atguigu.spring.beans.autowire中创建了一个无参的方法 12345&#x2F;&#x2F;用来测试Bean的作用域,看调用getBean()方法时被调用了几次 public Car() &#123; &#x2F;&#x2F; TODO Auto-generated constructor stub System.out.println(&quot;Car&#39;s Constructor...&quot;); &#125; 配置文件beans-scope.xml 12345678&lt;!-- 使用Bean的scope属性来配置bean的作用域 singleton(默认):容器初始时创建bean实例，在整个容器的生命周期内之创建者一个bean，单例的 prototype：原型的，容器初始化时，不创建Bean的实例，而在每次请求时都创建一个新的Bean实例并返回。 --&gt; &lt;bean id&#x3D;&quot;car&quot; class&#x3D;&quot;com.atguigu.spring.beans.autowire.Car&quot; scope&#x3D;&quot;prototype&quot;&gt; &lt;property name&#x3D;&quot;brand&quot; value&#x3D;&quot;Audi&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;price&quot; value&#x3D;&quot;30000&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt; 测试实例 123456789101112131415161718package com.atguigu.spring.beans.scope;import org.springframework.context.support.AbstractApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import com.atguigu.spring.beans.autowire.Car;public class Main &#123; public static void main(String[] args) &#123; &#x2F;&#x2F;创建容器 AbstractApplicationContext ctx &#x3D; new ClassPathXmlApplicationContext(&quot;beans-scope.xml&quot;); &#x2F;&#x2F;用相等来判断，每次调用getBean方法有没有重新实例化，输出true或false Car car &#x3D; (Car) ctx.getBean(&quot;car&quot;); Car car2 &#x3D; (Car) ctx.getBean(&quot;car&quot;); System.out.println(car &#x3D;&#x3D; car2); &#125;&#125; 四， 使用外部属性文件 先导入jar包c3p0-0.9.5.2.jar和mysql-connector-java-8.0.11.jar 注册 PropertyPlaceholderConfigurer 创建配置文件beans-properties.xml（Namespace有bean,context,jdbc）1234567891011121314151617181920212223&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xmlns:context&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&quot; xmlns:jdbc&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;jdbc&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;jdbc http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;jdbc&#x2F;spring-jdbc-4.3.xsd http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans.xsd http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&#x2F;spring-context-4.3.xsd&quot;&gt; &lt;!-- 测试外部属性链接 --&gt; &lt;!-- 导入属性文件 --&gt; &lt;context:property-placeholder location&#x3D;&quot;classpath:db.properties&quot; &#x2F;&gt; &lt;bean id&#x3D;&quot;dataSource&quot; class&#x3D;&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;!-- 使用外部化属性文件的属性 --&gt; &lt;property name&#x3D;&quot;driverClass&quot; value&#x3D;&quot;$&#123;jdbc.driverclass&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;user&quot; value&#x3D;&quot;$&#123;jdbc.user&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;password&quot; value&#x3D;&quot;$&#123;jdbc.password&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;jdbcUrl&quot; value&#x3D;&quot;$&#123;jdbc.jdbcurl&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt;&lt;&#x2F;beans&gt; 在src目录下创建配置文件db.properties1234jdbc.driverclass&#x3D;com.mysql.cj.jdbc.Driverjdbc.jdbcurl&#x3D;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;mysql?serverTimezone&#x3D;GMT%2B8jdbc.password&#x3D;******jdbc.user&#x3D;root 测试类Main.java123456789101112131415161718package com.atguigu.spring.beans.proterties;import java.sql.SQLException;import javax.sql.DataSource;import org.springframework.context.support.AbstractApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class Main &#123; public static void main(String[] args) throws SQLException&#123; &#x2F;&#x2F;创建容器 AbstractApplicationContext ctx &#x3D; new ClassPathXmlApplicationContext(&quot;beans-properties.xml&quot;); DataSource dataSource &#x3D; (DataSource) ctx.getBean(&quot;dataSource&quot;); System.out.println(dataSource.getConnection()); &#125;&#125; 五， Spring表达式语言：SpEL5.1 知识点 六， IOC容器中Bean的生命周期6.1 IOC 容器中 Bean 的生命周期方法 了解Spring中Bean的生命周期的意义就在于，可以利用Bean在其存活期间的特定时刻完成一些相关操作。这种时刻可能有很多，但一般情况下，常会在Bean的postinitiation(初始化后)和predestruction（销毁前）执行一些相关操作。 测试结果123456Car&#39;s Constructor...setBrand...init...com.atguigu.spring.beans.cycle.Car@68567e20destroy... 结果说明：先调用了构造器然后setBrand设置属性，然后调用init初始化，然后返回bean，然后调用close()方法destroy6.2 创建 Bean 后置处理器 新建一个文件MyBeanPostProcessor.java测试内容：将之前Bean里的brand=Audi改为brand=Ford 实现Bean接口，并具体提供一下两种方法的实现 Object postProcessBeforeInitialization(Object bean,String beanName):在init-method之前被调用。 Object postProcessAfterInitialization(Object bean,String beanName) :在init-method之后被调用 bean: bean实例本身 beanName : IOC容器配置的bean的名字 返回值: 是实际上返回给用户的那个Bean，注意:可以在以上两个方法中修改返回的bean,甚至返回一个新的bean 测试结果1234567891011Car&#39;s Constructor... &#x2F;&#x2F;通过构造器或工厂方法创建 Bean 实例setBrand... &#x2F;&#x2F;为 Bean 的属性设置值postProcessBeforeInitialization:Car [brand&#x3D;Audi],car &#x2F;&#x2F;init...postProcessAfterInitialization:Car [brand&#x3D;Audi],car &#x2F;&#x2F;将 Bean 实例传递给 Bean 后置处理器的 postProcessBeforeInitialization 方法调用 Bean 的初始化方法Car&#39;s Constructor... &#x2F;&#x2F;将 Bean 实例传递给 Bean 后置处理器的 postProcessAfterInitialization方法Bean 可以使用了setBrand... &#x2F;&#x2F;调用setBrand重新赋值为FordCar [brand&#x3D;Ford] &#x2F;&#x2F;当容器关闭时, 调用 Bean 的销毁方法destroy... 源码1.1.2 自动装配源码 包名com.atguigu.spring.beans.autowireAddress.java1234567891011121314151617181920212223package com.atguigu.spring.beans.autowire;public class Address &#123; private String city; private String street; public String getCity() &#123; return city; &#125; public void setCity(String city) &#123; this.city &#x3D; city; &#125; public String getStreet() &#123; return street; &#125; public void setStreet(String street) &#123; this.street &#x3D; street; &#125; @Override public String toString() &#123; return &quot;Address [city&#x3D;&quot; + city + &quot;, street&#x3D;&quot; + street + &quot;]&quot;; &#125;&#125; Car.Java1234567891011121314151617181920212223package com.atguigu.spring.beans.autowire;public class Car &#123; private String brand; private double price; public String getBrand() &#123; return brand; &#125; public void setBrand(String brand) &#123; this.brand &#x3D; brand; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price &#x3D; price; &#125; @Override public String toString() &#123; return &quot;Car [brand&#x3D;&quot; + brand + &quot;, price&#x3D;&quot; + price + &quot;]&quot;; &#125;&#125; Main.Java1234567891011121314package com.atguigu.spring.beans.autowire;import org.springframework.context.support.AbstractApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class Main &#123; public static void main(String[] args) &#123; AbstractApplicationContext ctx &#x3D; new ClassPathXmlApplicationContext(&quot;beans-autowire.xml&quot;); Person person &#x3D; (Person) ctx.getBean(&quot;person&quot;); System.out.println(person); &#125;&#125; Person.Java12345678910111213141516171819202122232425262728293031package com.atguigu.spring.beans.autowire;public class Person &#123; private String name; private Address address; private Car car; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name &#x3D; name; &#125; public Address getAddress() &#123; return address; &#125; public void setAddress(Address address) &#123; this.address &#x3D; address; &#125; public Car getCar() &#123; return car; &#125; public void setCar(Car car) &#123; this.car &#x3D; car; &#125; @Override public String toString() &#123; return &quot;Person [name&#x3D;&quot; + name + &quot;, address&#x3D;&quot; + address + &quot;, car&#x3D;&quot; + car + &quot;]&quot;; &#125;&#125; beans-autowire.xml1234567891011121314151617181920212223&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xmlns:p&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;p&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans.xsd&quot;&gt; &lt;bean id&#x3D;&quot;adress&quot; class&#x3D;&quot;com.atguigu.spring.beans.autowire.Address&quot; p:city&#x3D;&quot;Beijing&quot; p:street&#x3D;&quot;HuilongGuan&quot;&gt;&lt;&#x2F;bean&gt; &lt;bean id&#x3D;&quot;car&quot; class&#x3D;&quot;com.atguigu.spring.beans.autowire.Car&quot; p:brand&#x3D;&quot;Audi&quot; p:price&#x3D;&quot;300000&quot;&gt;&lt;&#x2F;bean&gt; &lt;!-- byName自动装配Person里有对应的adress,car 指向配置文件的id名 &lt;bean id&#x3D;&quot;person&quot; class&#x3D;&quot;com.atguigu.spring.beans.autowire.Person&quot; p:name&#x3D;&quot;Tome&quot; autowire&#x3D;&quot;byName&quot;&gt;&lt;&#x2F;bean&gt; --&gt; &lt;!-- byType 的类型和当前bean的属性的类型进行自动装配 --&gt; &lt;bean id&#x3D;&quot;person&quot; class&#x3D;&quot;com.atguigu.spring.beans.autowire.Person&quot; p:name&#x3D;&quot;Tom&quot; autowire&#x3D;&quot;byType&quot;&gt;&lt;&#x2F;bean&gt;&lt;&#x2F;beans&gt; 2.3 bean 之间的关系：继承；依赖–源码包名com.atguigu.spring.beans.realtionMain.java123456789101112131415161718192021package com.atguigu.spring.beans.realtion;import org.springframework.context.support.AbstractApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import com.atguigu.spring.beans.autowire.Address;import com.atguigu.spring.beans.autowire.Person;public class Main &#123; public static void main(String[] args) &#123; AbstractApplicationContext ctx &#x3D; new ClassPathXmlApplicationContext(&quot;beans-realtion.xml&quot;); Address address &#x3D; (Address) ctx.getBean(&quot;address2&quot;); System.out.println(address); Person person &#x3D; (Person) ctx.getBean(&quot;person&quot;); System.out.println(person); &#125;&#125; beans-realtion.xml1234567891011121314151617181920&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xmlns:p&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;p&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans.xsd&quot;&gt; &lt;bean id&#x3D;&quot;address&quot; class&#x3D;&quot;com.atguigu.spring.beans.autowire.Address&quot; p:city&#x3D;&quot;Beijing&quot; p:street&#x3D;&quot;WuDaoKou&quot; abstract&#x3D;&quot;true&quot; &gt;&lt;&#x2F;bean&gt; &lt;!-- bean 配置的继承：使用Bean的parent属性指定继承那个属性的配置 --&gt; &lt;bean id&#x3D;&quot;address2&quot; p:street&#x3D;&quot;TianMen&quot; parent&#x3D;&quot;address&quot;&gt;&lt;&#x2F;bean&gt; &lt;bean id&#x3D;&quot;car&quot; class&#x3D;&quot;com.atguigu.spring.beans.autowire.Car&quot; p:brand&#x3D;&quot;Audi&quot; p:price&#x3D;&quot;30000&quot;&gt;&lt;&#x2F;bean&gt; &lt;!-- 要求在配置person的时候，必须有一个关联的car!换句话说person这个bean依赖于Car的这个Bean --&gt; &lt;bean id&#x3D;&quot;person&quot; class&#x3D;&quot;com.atguigu.spring.beans.autowire.Person&quot; p:name&#x3D;&quot;Tom&quot; p:address-ref&#x3D;&quot;address2&quot; depends-on&#x3D;&quot;car&quot;&gt;&lt;&#x2F;bean&gt;&lt;&#x2F;beans&gt; 3.2 Bean 的作用域-源码新建包com.atguigu.spring.beans.scopeMain.java1234567891011121314151617package com.atguigu.spring.beans.scope;import org.springframework.context.support.AbstractApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import com.atguigu.spring.beans.autowire.Car;public class Main &#123; public static void main(String[] args) &#123; &#x2F;&#x2F;创建容器 AbstractApplicationContext ctx &#x3D; new ClassPathXmlApplicationContext(&quot;beans-scope.xml&quot;); Car car &#x3D; (Car) ctx.getBean(&quot;car&quot;); Car car2 &#x3D; (Car) ctx.getBean(&quot;car&quot;); System.out.println(car &#x3D;&#x3D; car2); &#125;&#125; 配置文件beans-scope.xml12345678910111213141516&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans.xsd&quot;&gt; &lt;!-- 使用Bean的scope属性来配置bean的作用域 singleton(默认):容器初始时创建bean实例，在整个容器的生命周期内之创建者一个bean，单例的 prototype：原型的，容器初始化时，不创建Bean的实例，而在每次请求时都创建一个新的Bean实例并返回。 --&gt; &lt;bean id&#x3D;&quot;car&quot; class&#x3D;&quot;com.atguigu.spring.beans.autowire.Car&quot; scope&#x3D;&quot;prototype&quot;&gt; &lt;property name&#x3D;&quot;brand&quot; value&#x3D;&quot;Audi&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;price&quot; value&#x3D;&quot;30000&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt;&lt;&#x2F;beans&gt; 在包com.atguigu.spring.beans.autowire.Car中添加了一个无参的构造方法12345&#x2F;&#x2F;用来测试Bean的作用域,看调用getBean()方法时被调用了几次 public Car() &#123; &#x2F;&#x2F; TODO Auto-generated constructor stub System.out.println(&quot;Car&#39;s Constructor...&quot;); &#125; 5.2 spEl测试用例源码新建包com.atguigu.spring.beans.spelCar.java12345678910111213141516171819202122232425262728293031323334package com.atguigu.spring.beans.spel;public class Car &#123; private String brand; private double price; &#x2F;&#x2F;轮胎周长 private double tyrePerimeter; public String getBrand() &#123; return brand; &#125; public void setBrand(String brand) &#123; this.brand &#x3D; brand; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price &#x3D; price; &#125; public double getTyrePerimeter() &#123; return tyrePerimeter; &#125; public void setTyrePerimeter(double tyrePerimeter) &#123; this.tyrePerimeter &#x3D; tyrePerimeter; &#125; @Override public String toString() &#123; return &quot;Car [brand&#x3D;&quot; + brand + &quot;, price&#x3D;&quot; + price + &quot;, tyrePerimeter&#x3D;&quot; + tyrePerimeter + &quot;]&quot;; &#125;&#125; Person.java123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.atguigu.spring.beans.spel;public class Person &#123; private String name; private Car car; &#x2F;&#x2F;引用address bean的city属性 private String city; &#x2F;&#x2F;根据car的属性确定info；car的price&gt;&#x3D;300000 ：金领 &#x2F;&#x2F;否则则为：白领 private String info; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name &#x3D; name; &#125; public Car getCar() &#123; return car; &#125; public void setCar(Car car) &#123; this.car &#x3D; car; &#125; public String getCity() &#123; return city; &#125; public void setCity(String city) &#123; this.city &#x3D; city; &#125; public String getInfo() &#123; return info; &#125; public void setInfo(String info) &#123; this.info &#x3D; info; &#125; @Override public String toString() &#123; return &quot;Person [name&#x3D;&quot; + name + &quot;, car&#x3D;&quot; + car + &quot;, city&#x3D;&quot; + city + &quot;, info&#x3D;&quot; + info + &quot;]&quot;; &#125;&#125; 创建配置文件beans-spel.xml123456789101112131415161718192021222324252627282930&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans.xsd&quot;&gt; &lt;bean id&#x3D;&quot;address&quot; class&#x3D;&quot;com.atguigu.spring.beans.spel.Address&quot;&gt; &lt;!-- 使用spel 为属性赋一个字面值 --&gt; &lt;property name&#x3D;&quot;city&quot; value&#x3D;&quot;#&#123;&#39;Beijing&#39;&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;street&quot; value&#x3D;&quot;#&#123;&#39;TianAnMen&#39;&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt; &lt;bean id&#x3D;&quot;car&quot; class&#x3D;&quot;com.atguigu.spring.beans.spel.Car&quot;&gt; &lt;property name&#x3D;&quot;brand&quot; value&#x3D;&quot;Audi&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;price&quot; value&#x3D;&quot;500000&quot;&gt;&lt;&#x2F;property&gt; &lt;!-- 使用spel 引用类的静态属性 tyrePerimeter指轮胎的周长 --&gt; &lt;property name&#x3D;&quot;tyrePerimeter&quot; value&#x3D;&quot;#&#123;T(java.lang.Math).PI*80&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt; &lt;bean id&#x3D;&quot;person&quot; class&#x3D;&quot;com.atguigu.spring.beans.spel.Person&quot;&gt; &lt;!-- 使用spel来引用其他的bean --&gt; &lt;property name&#x3D;&quot;car&quot; value&#x3D;&quot;#&#123;car&#125;&quot;&gt;&lt;&#x2F;property&gt;&lt;!-- car是bean的id --&gt; &lt;!-- 使用spel来引用其他的bean 的属性 --&gt; &lt;property name&#x3D;&quot;city&quot; value&#x3D;&quot;#&#123;address.city&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;!-- 在spel中使用运算符 --&gt; &lt;property name&#x3D;&quot;info&quot; value&#x3D;&quot;#&#123;car.price &gt; 300000? &#39;金领&#39; : &#39;白领&#39;&#125;&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;name&quot; value&#x3D;&quot;Tom&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt;&lt;&#x2F;beans&gt; 测试Main.java123456789101112131415161718192021package com.atguigu.spring.beans.spel;import org.springframework.context.support.AbstractApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class Main &#123; public static void main(String[] args) &#123; AbstractApplicationContext ctx &#x3D; new ClassPathXmlApplicationContext(&quot;beans-spel.xml&quot;); Address address &#x3D; (Address) ctx.getBean(&quot;address&quot;); System.out.println(address); Car car &#x3D;(Car) ctx.getBean(&quot;car&quot;); System.out.println(car); Person person &#x3D; (Person) ctx.getBean(&quot;person&quot;); System.out.println(person); &#125;&#125; 运行结果1234Address [city&#x3D;Beijing, street&#x3D;TianAnMen]Car [brand&#x3D;Audi, price&#x3D;500000.0, tyrePerimeter&#x3D;251.32741228718345]Person [name&#x3D;Tom, car&#x3D;Car [brand&#x3D;Audi, price&#x3D;500000.0, tyrePerimeter&#x3D;251.32741228718345], city&#x3D;Beijing, info&#x3D;金领] 6.3 生命周期测试源码新建包com.atguigu.spring.beans.cycle配置文件beans-cycle.xml12345678910111213&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans.xsd&quot;&gt; &lt;!-- IOC 容器中 Bean 的生命周期&#96; --&gt; &lt;bean id&#x3D;&quot;car&quot; class&#x3D;&quot;com.atguigu.spring.beans.cycle.Car&quot; init-method&#x3D;&quot;init&quot; destroy-method&#x3D;&quot;destroy&quot;&gt; &lt;property name&#x3D;&quot;brand&quot; value&#x3D;&quot;Audi&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt;&lt;&#x2F;beans&gt; Car.java123456789101112131415161718192021222324package com.atguigu.spring.beans.cycle;public class Car &#123; public Car() &#123; System.out.println(&quot;Car&#39;s Constructor...&quot;); &#125; private String brand; public void setBrand(String brnd) &#123; System.out.println(&quot;setBrand...&quot;); this.brand&#x3D;brand; &#125; public void init() &#123; System.out.println(&quot;init...&quot;); &#125; public void destroy() &#123; System.out.println(&quot;destroy...&quot;); &#125;&#125; Main.java1234567891011121314151617181920package com.atguigu.spring.beans.cycle;import org.springframework.context.ApplicationContext;import org.springframework.context.support.AbstractApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class Main &#123; public static void main(String[] args) &#123; &#x2F;&#x2F; TODO Auto-generated method stub ApplicationContext ctx &#x3D; new ClassPathXmlApplicationContext(&quot;beans-cycle.xml&quot;); Car car &#x3D; (Car) ctx.getBean(&quot;car&quot;); System.out.println(car); &#x2F;&#x2F;关闭IOC容器 ((AbstractApplicationContext) ctx).close(); &#125;&#125; 6.4 生命周期测试源码的源码beans-cycle.xml12345678910111213141516&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans.xsd&quot;&gt; &lt;!-- IOC 容器中 Bean 的生命周期&#96; --&gt; &lt;bean id&#x3D;&quot;car&quot; class&#x3D;&quot;com.atguigu.spring.beans.cycle.Car&quot; init-method&#x3D;&quot;init&quot; destroy-method&#x3D;&quot;destroy&quot; &gt; &lt;property name&#x3D;&quot;brand&quot; value&#x3D;&quot;Audi&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt; &lt;!-- 配置bean的后置处理器: 不需要配置id，IOC容器自动识别是一个BeanPostProcessor --&gt; &lt;bean class&#x3D;&quot;com.atguigu.spring.beans.cycle.MyBeanPostProcessor&quot;&gt;&lt;&#x2F;bean&gt;&lt;&#x2F;beans&gt; Car.java12345678910111213141516171819202122232425262728293031323334package com.atguigu.spring.beans.cycle;public class Car &#123; public Car() &#123; System.out.println(&quot;Car&#39;s Constructor...&quot;); &#125; private String brand; public String getBrand() &#123; return brand; &#125; public void setBrand(String brand) &#123; System.out.println(&quot;setBrand...&quot;); this.brand &#x3D; brand; &#125; public void init() &#123; System.out.println(&quot;init...&quot;); &#125; public void destroy() &#123; System.out.println(&quot;destroy...&quot;); &#125; @Override public String toString() &#123; return &quot;Car [brand&#x3D;&quot; + brand + &quot;]&quot;; &#125;&#125; MyBeanPostProcessor.java12345678910111213141516171819202122232425262728293031package com.atguigu.spring.beans.cycle;import org.springframework.beans.BeansException;import org.springframework.beans.factory.config.BeanPostProcessor;public class MyBeanPostProcessor implements BeanPostProcessor &#123;&#x2F;&#x2F;在文件上右键可以创建接口 &#x2F;&#x2F;Bean的后置处理器 @Override public Object postProcessBeforeInitialization(Object bean,String beanName) throws BeansException&#123; System.out.println(&quot;postProcessBeforeInitialization:&quot; + bean + &quot;,&quot; + beanName); &#x2F;&#x2F;可以对bean进行过滤，处理指定的bean if (&quot;car&quot;.equals(beanName)) &#123; &#x2F;&#x2F;... &#125; return bean; &#125; @Override public Object postProcessAfterInitialization(Object bean,String beanName) throws BeansException&#123; System.out.println(&quot;postProcessAfterInitialization:&quot; + bean + &quot;,&quot; + beanName); Car car &#x3D; new Car(); car.setBrand(&quot;Ford&quot;); return car; &#125;&#125; Main.java1234567891011121314151617package com.atguigu.spring.beans.cycle;import org.springframework.context.support.ClassPathXmlApplicationContext;import org.springframework.context.support.AbstractApplicationContext;public class Main &#123; public static void main(String[] args) &#123; &#x2F;&#x2F; TODO Auto-generated method stub AbstractApplicationContext ctx &#x3D; new ClassPathXmlApplicationContext(&quot;beans-cycle.xml&quot;); Car car &#x3D; (Car) ctx.getBean(&quot;car&quot;); System.out.println(car); &#x2F;&#x2F;关闭IOC容器 ctx.close(); &#125;&#125;","categories":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"ssm","slug":"ssm","permalink":"http://www.studyz.club/tags/ssm/"},{"name":"Spring","slug":"Spring","permalink":"http://www.studyz.club/tags/Spring/"},{"name":"Bean","slug":"Bean","permalink":"http://www.studyz.club/tags/Bean/"}]},{"title":"软件开发框架之spring","slug":"软件开发框架之Spring","date":"2019-09-19T00:12:57.746Z","updated":"2019-11-09T08:14:33.895Z","comments":true,"path":"posts/bb6fcc79/","link":"","permalink":"http://www.studyz.club/posts/bb6fcc79/","excerpt":"","text":"一Spring的基本应用 1.1 什么是Spring 1.2 Spring框架的优点 1.3 Spring的体系结构 1.4 Spring–HelloWorld 二，Spring 中的 Bean 配置 2.1 IOC &amp; DI 概述 2.2 Bean的配置 基于XML文件的方式 2.2.1 通过全类名 2.2.2 通过调用静态工厂方法创建 Bean 2.2.3 通过调用实例工厂方法创建 Bean 2.2.4 通过调用静态工厂和实例工厂方法创建 Bean的源码 2.2.5 实现 FactoryBean 接口在 Spring IOC 容器中配置 Bean 2.2.6 实现 FactoryBean 接口在 Spring IOC 容器中配置 BeaBean的源码基于注解的方式(基于注解来配置bean;基于注解来装配bean的属性) 2.2.7 在 classpath 中扫描组件 2.2.8 组件装配 使用 @Autowired 自动装配 Bean 2.2.9 使用 @Resource 或 @Inject 自动装配 Bean 2.2.0 2.3 依赖注入的方式 2.4 注入属性值细节 2.4.1 字面值 2.4.2 引用其它 Bean 2.4.3 内部 Bean 2.4.4 注入参数详解：null 值和级联属性 2.4.5 集合属性 2.4.6 使用 utility scheme 定义集合 2.4.7 使用 p 命名空间 2.4.8 泛型依赖注入 2.4.9 泛型依赖注入测试源码 2.5 本节所用源码 2.5.1 包com.atguigu.spring.beans 2.5.2 包com.atguigu.spring.beans.collection 2.5.3 applicationContext.xml 一 ，Spring的基本应用1.1 什么是Spring Spring是分层的JavaSE/EE full-stack 轻量级开源框架，以IoC（Inverse of Control 控制反转）和AOP（Aspect Oriented Programming 面向切面编程）为内核，使用基本的JavaBean来完成以前只可能由EJB完成的工作，取代了EJB的臃肿、低效的开发模式。 在实际开发中，通常服务器端在采用三层体系架构，分别为表示层(Web)、业务逻辑层(Service)、持久层(Dao)， Spring对每一层都提供了技术支持。 表示层：在表示层提供了与Struts等框架的整合 业务逻辑层: 在业务逻辑层可以管理事务、记录日志等 持久层：在持久层可以整合Hibernate、JdbcTemplate 等技术 1.2 Spring框架的优点 Spring具有简单、可测试和松耦合等特点。Spring不仅可以用于服务器端开发，也可以应用于任何Java应用的开发中。 Spring框架的7大优点 非侵入式设计 方便解耦、简化开发 支持AOP 支持声明式事务处理 方便程序测试 方便集成各种优秀框架 降低Java EE API的使用难度 1.3 Spring的体系结构 Spring容器会负责控制程序之间的关系，而不是由程序代码直接控制。Spring为我们提供了``两种核心容器s，分别为BeanFactory和ApplicationContext。 BeanFactory 创建BeanFactory实例时，需要提供Spring所管理容器的详细配置信息，这些信息通常采用XML文件形式来管理，其加载配置信息的语法如下： 12BeanFactory beanFactory &#x3D; new XmlBeanFactory(new FileSystemResource(&quot;F: &#x2F;applicationContext.xml&quot;));&#x2F;&#x2F;XML配置文件的位置,这种加载方式在实际开发中并不多用，了解即可。 ApplicationContext ApplicationContext是BeanFactory的子接口，是另一种常用的Spring核心容器。它由org.springframework.context.ApplicationContext接口定义，不仅包含了BeanFactory的所有功能，还添加了对国际化、资源访问、事件传播等方面的支持。创建ApplicationContext接口实例，通常采用两种方法，具体如下： 通过ClassPathXmlApplicationContext创建 1ApplicationContext applicationContext &#x3D; new ClassPathXmlApplicationContext(String configLocation); ClassPathXmlApplicationContext会从类路径classPath中寻找指定的XML配置文件，找到并装载完成ApplicationContext的实例化工作。 通过FileSystemXmlApplicationContext创建 1ApplicationContext applicationContext &#x3D;new FileSystemXmlApplicationContext(String configLocation); FileSystemXmlApplicationContext会从指定的文件系统路径（绝对路径）中寻找指定的XML配置文件，找到并装载完成ApplicationContext的实例化工作。 在Java项目中，会通过ClassPathXmlApplicationContext类来实例化ApplicationContext容器。而在Web项目中，ApplicationContext容器的实例化工作会交由Web服务器来完成。 Web服务器实例化ApplicationContext容器时，通常会使用ContextLoaderListener来实现，此种方式只需要在web.xml中添加如下代码：1234567891011&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;&#x2F;param-name&gt; &lt;param-value&gt; classpath:spring&#x2F;applicationContext.xml &lt;&#x2F;param-value&gt; &lt;&#x2F;context-param&gt; &lt;listener&gt; &lt;listener-class&gt; org.springframework.web.context.ContextLoaderListener &lt;&#x2F;listener-class&gt; &lt;&#x2F;listener&gt; 1.4 Spring–HelloWorld 首先装完eclipse对应的Spring插件后，新建一个Java项目，将如下图所示的jar包导入。 编写HelloWorld.java 1234567891011121314package com.atguigu.spring.beans;public class HelloWorld &#123; private String name; public void setName(String name) &#123; this.name = name; &#125; public void hello () &#123; System.out.println(&quot;hello:&quot; + name ); &#125;&#125; 编写测试类Main方法 1234567891011121314151617 package com.atguigu.spring.beans; public class Main &#123;public static void main(String[] args) &#123; &#x2F;&#x2F; TODO Auto-generated method stub &#x2F;&#x2F; 创建 HelloWorld的一个对象 HelloWorld helloWorld &#x3D; new HelloWorld(); &#x2F;&#x2F; 为name属性赋值 helloWorld.setName(&quot;lizhi&quot;); &#x2F;&#x2F; 调用hello方法 helloWorld.hello();&#125; &#125; 运行结果 1hello:lizhi 配置applicationContext.xml文件（在src文件夹上右键–&gt;Spring–&gt;Spring Bean Configuration File） 12345 &lt;!-- 配置Bean --&gt;&lt;bean id &#x3D; &quot;helloWorld&quot; class &#x3D; &quot;com.atguigu.spring.beans.HelloWorld&quot;&gt; &lt;property name&#x3D;&quot;name&quot; value&#x3D;&quot;Spring&quot;&gt;&lt;&#x2F;property&gt;&lt;&#x2F;bean&gt; 更改 测试类Main的内容 123456789101112131415161718192021package com.atguigu.spring.beans;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class Main &#123; public static void main(String[] args) &#123; &#x2F;&#x2F; TODO Auto-generated method stub &#x2F;&#x2F; 1. 创建Spring的IOC容器对象 ApplicationContext ctx &#x3D; new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); &#x2F;&#x2F; 2. 从IOC容器中获取Bean实例 HelloWorld helloWorld &#x3D; (HelloWorld) ctx.getBean(&quot;helloWorld&quot;);&#x2F;&#x2F; 这里填xml里配置的id &#x2F;&#x2F; 3. 调用hello方法 helloWorld.hello(); &#125;&#125; 输出结果 12hello:Spring 二，Spring 中的 Bean 配置2.1 IOC &amp; DI 概述 IOC(Inversion of Control)：其思想是反转资源获取的方向. 传统的资源查找方式要求组件向容器发起请求查找资源. 作为回应, 容器适时的返回资源. 而应用了 IOC 之后, 则是容器主动地将资源推送给它所管理的组件, 组件所要做的仅是选择一种合适的方式来接受资源. 这种行为也被称为查找的被动形式 DI(Dependency Injection) — IOC 的另一种表述方式：即组件以一些预先定义好的方式(例如: setter 方法)接受来自如容器的资源注入. 相对于 IOC 而言，这种表述更直接 2.1.1 IOC 前生 — 分离接口与实现 需求: 生成 HTML 或 PDF 格式的不同类型的报表 2.1.2 IOC 前生 — 采用工厂设计模式 2.1.3 IOC — 采用反转控制 2.2 Bean的配置 什么是Spring中的Bean？ 如果把Spring看做一个大型工厂，则Spring容器中的Bean就是该工厂的产品。要想使用这个工厂生产和管理Bean，就需要在配置文件中告诉它需要哪些Bean，以及需要使用何种方式将这些Bean装配到一起。 Bean的本质就是Java中的类，而Spring中的Bean其实就是对实体类的引用，来生产Java类对象，从而实现生产和管理Bean 2. 通过全类名 XML文件 XML配置文件的根元素是&lt;beans&gt;，&lt;beans&gt;中包含了多个&lt;bean&gt;子元素，每一个&lt;bean&gt;子元素定义了一个Bean，并描述了该Bean如何被装配到Spring容器中。关于&lt;beans&gt;元素的常用属性如下表所示： id：Bean 的名称。 在 IOC 容器中必须是唯一的 若 id 没有指定，Spring 自动将权限定性类名作为 Bean 的名字 id 可以指定多个名字，名字之间可用逗号、分号、或空格分隔 在配置文件中，通常一个普通的Bean只需要定义id（或name）和class 两个属性即可，定义Bean的方式如下所示： 12345678&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt; &lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot;xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beanshttp:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans.xsd&quot;&gt; &lt;bean id&#x3D;&quot;bean1&quot; class&#x3D;&quot;com.dqsy.Bean1&quot; &#x2F;&gt; &lt;bean name&#x3D;&quot;bean2&quot; class&#x3D;&quot;com.dqsy.Bean2&quot; &#x2F;&gt; &lt;&#x2F;beans&gt; 123&#x2F;&#x2F;ApplicationContext 代表IOC容器&#x2F;&#x2F;ClassPathXmlApplicationContext是ApplicationContext接口的实现类，该实现类从类路径下加载配置文件ApplicationContext applicationContext &#x3D; new ClassPathXmlApplicationContext(String configLocation); 2.2.2 通过调用静态工厂方法创建 Bean 2.2.3 通过调用实例工厂方法创建 Bean 2.2.5 实现 FactoryBean 接口在 Spring IOC 容器中配置 Bean再配置bean的时候需要用到IOC容器中的其他Bean，这时用FactoryBean最好 2.2.7 在 classpath 中扫描组件 2.2.8 组件装配 使用 @Autowired 自动装配 Bean组件装配 context:component-scan 元素还会自动注册 AutowiredAnnotationBeanPostProcessor 实例, 该实例可以自动装配具有 @Autowired 和 @Resource 、@Inject注解的属性. 2.2.9 使用 @Resource 或 @Inject 自动装配 Bean 2.3 依赖注入的方式 Spring 支持 3 种依赖注入的方式 属性注入 构造器注入 工厂方法注入（很少使用，不推荐） 2.3.1 属性注入 2.3.2 构造方法注入 构造器快捷方式 右键–&gt;source–&gt;Generate Constructor using Fields 这里面也有toString方法，getter/setter方法等 2.4 注入属性值细节2.4.1 字面值 1234&lt;!-- 特殊字符用&lt;![CDATA[]]&gt; --&gt; &lt;constructor-arg type&#x3D;&quot;java.lang.String&quot;&gt; &lt;value&gt;&lt;![CDATA[&lt;shanghai&gt;]]&gt;&lt;&#x2F;value&gt; &lt;&#x2F;constructor-arg&gt; 运行结果 12&#x2F;&#x2F;结果中shanghai多了特殊字符尖括号Car [company&#x3D;BMW, brand&#x3D;&lt;shanghai&gt;, maxSpeed&#x3D;250, price&#x3D;0.0] 2.4.2 引用其它 Bean 1234567891011&lt;!-- 引用其它 Bean --&gt; &lt;bean id&#x3D;&quot;person&quot; class&#x3D;&quot;com.atguigu.spring.beans.Person&quot;&gt; &lt;property name&#x3D;&quot;name&quot; value&#x3D;&quot;tom&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;age&quot; value&#x3D;&quot;24&quot;&gt;&lt;&#x2F;property&gt; &lt;!-- 可以使用property的ref属性建立bean之间的引用关系。也可以使用ref标签 &lt;property name&#x3D;&quot;car&quot;&gt; &lt;ref bean&#x3D;&quot;car2&quot; &#x2F;&gt; &lt;&#x2F;property&gt; --&gt; &lt;property name&#x3D;&quot;car&quot; ref&#x3D;&quot;car2&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt; 运行结果 1Person [name&#x3D;tom, age&#x3D;24, car&#x3D;Car [company&#x3D;BMW, brand&#x3D;&lt;shanghai&gt;, maxSpeed&#x3D;250, price&#x3D;0.0]] 2.4.3 内部 Bean内部Bean不能被外部引用 123456789101112131415161718192021 &lt;bean id&#x3D;&quot;person&quot; class&#x3D;&quot;com.atguigu.spring.beans.Person&quot;&gt; &lt;property name&#x3D;&quot;name&quot; value&#x3D;&quot;tom&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;age&quot; value&#x3D;&quot;24&quot;&gt;&lt;&#x2F;property&gt; &lt;!-- 内部bean --&gt; &lt;property name&#x3D;&quot;car&quot;&gt; &lt;bean class&#x3D;&quot;com.atguigu.spring.beans.Car&quot;&gt; &lt;constructor-arg value&#x3D;&quot;Ford&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;constructor-arg value&#x3D;&quot;Changan&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;constructor-arg value&#x3D;&quot;200000&quot; type&#x3D;&quot;double&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;&#x2F;bean&gt; &lt;&#x2F;property&gt; &lt;&#x2F;bean&gt;或者&lt;bean id&#x3D;&quot;person2&quot; class&#x3D;&quot;com.atguigu.spring.beans.Person&quot;&gt; &lt;constructor-arg value&#x3D;&quot;Jerry&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;constructor-arg value&#x3D;&quot;25&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;constructor-arg ref&#x3D;&quot;car&quot;&gt;&lt;&#x2F;constructor-arg&gt;&lt;&#x2F;bean&gt; 运行结果 123Person [name&#x3D;tom, age&#x3D;24, car&#x3D;Car [company&#x3D;Ford, brand&#x3D;Changan, maxSpeed&#x3D;0, price&#x3D;200000.0]]Person [name&#x3D;Jerry, age&#x3D;25, car&#x3D;Car [company&#x3D;Audi, brand&#x3D;shanghai, maxSpeed&#x3D;0, price&#x3D;30000.0]] 2.4.4 注入参数详解：null 值和级联属性 123456&lt;bean id&#x3D;&quot;person2&quot; class&#x3D;&quot;com.atguigu.spring.beans.Person&quot;&gt; &lt;constructor-arg value&#x3D;&quot;Jerry&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;constructor-arg value&#x3D;&quot;25&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;!-- 测试赋值null --&gt; &lt;constructor-arg&gt;&lt;null&#x2F;&gt;&lt;&#x2F;constructor-arg&gt;&lt;&#x2F;bean&gt; 运行结果 1Person [name&#x3D;Jerry, age&#x3D;25, car&#x3D;null] 级联属性(可以在Bean里对Car里的属性进行赋值) 属性需要先进性初始化后才可以为级联属性赋值，否则会有异常，和Struts不同。 1234567&lt;bean id&#x3D;&quot;person2&quot; class&#x3D;&quot;com.atguigu.spring.beans.Person&quot;&gt; &lt;constructor-arg value&#x3D;&quot;Jerry&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;constructor-arg value&#x3D;&quot;25&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;!-- 测试级联属性 --&gt; &lt;constructor-arg ref&#x3D;&quot;car&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;property name&#x3D;&quot;car.maxSpeed&quot; value&#x3D;&quot;400&quot;&gt;&lt;&#x2F;property&gt;&lt;&#x2F;bean&gt; 运行结果 1Person [name&#x3D;Jerry, age&#x3D;25, car&#x3D;Car [company&#x3D;Audi, brand&#x3D;shanghai, maxSpeed&#x3D;400, price&#x3D;30000.0]] 2.4.5 集合属性1–list新建一个包com.atguigu.spring.beans.collection，源码 1234567891011121314151617181920212223242526272829303132333435363738394041package com.atguigu.spring.beans.collection;import java.util.List;import com.atguigu.spring.beans.Car;&#x2F;&#x2F;集合属性测试public class Person &#123; private String name; private int age; private List&lt;Car&gt; cars; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name &#x3D; name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age &#x3D; age; &#125; public List&lt;Car&gt; getCars() &#123; return cars; &#125; public void setCars(List&lt;Car&gt; cars) &#123; this.cars &#x3D; cars; &#125; @Override public String toString() &#123; return &quot;Person [name&#x3D;&quot; + name + &quot;, age&#x3D;&quot; + age + &quot;, cars&#x3D;&quot; + cars + &quot;]&quot;; &#125;&#125; 配置文件 123456789101112&lt;!-- 测试如何配置集合属性 --&gt; &lt;bean id&#x3D;&quot;person3 class&#x3D;&quot;com.atguigu.spring.beans.collection.Person&quot;&gt; &lt;property name&#x3D;&quot;name&quot; value&#x3D;&quot;Mike&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;age&quot; value&#x3D;&quot;27&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;cars&quot; &gt; &lt;!-- 使用List节点 为List属性赋值 --&gt; &lt;list&gt; &lt;ref bean&#x3D;&quot;car&quot;&#x2F;&gt; &lt;ref bean&#x3D;&quot;car2&quot;&#x2F;&gt; &lt;&#x2F;list&gt; &lt;&#x2F;property&gt; &lt;&#x2F;bean&gt; 测试类 123456789101112131415package com.atguigu.spring.beans.collection;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class Main &#123; public static void main(String[] args) &#123; ApplicationContext ctx &#x3D; new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); Person person &#x3D; (Person) ctx.getBean(&quot;person3&quot;); System.out.println(person); &#125;&#125; 集合属性2–map配置文件 123456789101112&lt;!-- 集合属性配置之map --&gt; &lt;bean id&#x3D;&quot;newPerson&quot; class&#x3D;&quot;com.atguigu.spring.beans.collection.newPerson&quot;&gt; &lt;property name&#x3D;&quot;name&quot; value&#x3D;&quot;Rose&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;age&quot; value&#x3D;&quot;18&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;cars&quot;&gt; &lt;!-- 使用map节点及map的entry 子节点配置Map类型的成员变量--&gt; &lt;map&gt; &lt;entry key&#x3D;&quot;AA&quot; value-ref&#x3D;&quot;car&quot;&gt;&lt;&#x2F;entry&gt; &lt;entry key&#x3D;&quot;BB&quot; value-ref&#x3D;&quot;car2&quot;&gt;&lt;&#x2F;entry&gt; &lt;&#x2F;map&gt; &lt;&#x2F;property&gt; &lt;&#x2F;bean&gt; 运行结果 1newPerson [name&#x3D;Rose, age&#x3D;18, cars&#x3D;&#123;AA&#x3D;Car [company&#x3D;Audi, brand&#x3D;shanghai, maxSpeed&#x3D;400, price&#x3D;30000.0], BB&#x3D;Car [company&#x3D;BMW, brand&#x3D;&lt;shanghai&gt;, maxSpeed&#x3D;250, price&#x3D;0.0]&#125;] 集合属性3–PropertiesDataSource 1234567891011121314151617181920212223package com.atguigu.spring.beans.collection;import java.util.Properties;public class DataSource &#123; private Properties properties; public Properties getProperties() &#123; return properties; &#125; public void setProperties(Properties properties) &#123; this.properties &#x3D; properties; &#125; @Override public String toString() &#123; return &quot;DataSource [properties&#x3D;&quot; + properties + &quot;]&quot;; &#125;&#125; 配置文件 1234567891011121314&lt;!-- 配置Properties 属性值 --&gt; &lt;bean id&#x3D;&quot;dataSource&quot; class&#x3D;&quot;com.atguigu.spring.beans.collection.DataSource&quot;&gt; &lt;property name&#x3D;&quot;properties&quot;&gt; &lt;!-- 使用props和prop子节点来为 Properties属性赋值 这里模拟了数据库连接 --&gt; &lt;props&gt; &lt;prop key&#x3D;&quot;user&quot;&gt;root&lt;&#x2F;prop&gt; &lt;prop key&#x3D;&quot;password&quot;&gt;123&lt;&#x2F;prop&gt; &lt;prop key&#x3D;&quot;jdbcUrl&quot;&gt;jdbc:mysql:&#x2F;&#x2F;&#x2F;test&lt;&#x2F;prop&gt; &lt;prop key&#x3D;&quot;driverClass&quot;&gt;com.mysql.jdbc.Driver&lt;&#x2F;prop&gt; &lt;&#x2F;props&gt; &lt;&#x2F;property&gt; &lt;&#x2F;bean&gt; 运行结果 1DataSource [properties&#x3D;&#123;driverClass&#x3D;com.mysql.jdbc.Driver, user&#x3D;root, password&#x3D;123, jdbcUrl&#x3D;jdbc:mysql:&#x2F;&#x2F;&#x2F;test&#125;] 2.4.6 使用 utility scheme 定义集合配置文件//定义完cars之后可以被其他的bean共享和引用 1234567891011&lt;!-- 配置单例的集合bean，以供多个bean进行引用,需要导入util命名空间 --&gt; &lt;util:list id&#x3D;&quot;cars&quot;&gt; &lt;ref bean&#x3D;&quot;car&quot;&#x2F;&gt; &lt;ref bean&#x3D;&quot;car2&quot;&#x2F;&gt; &lt;&#x2F;util:list&gt; &lt;bean id&#x3D;&quot;person4&quot; class&#x3D;&quot;com.atguigu.spring.beans.collection.Person&quot;&gt; &lt;property name&#x3D;&quot;name&quot; value&#x3D;&quot;Jack&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;age&quot; value&#x3D;&quot;18&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;cars&quot; ref&#x3D;&quot;cars&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt; 运行结果 1Person [name&#x3D;Jack, age&#x3D;18, cars&#x3D;[Car [company&#x3D;Audi, brand&#x3D;shanghai, maxSpeed&#x3D;400, price&#x3D;30000.0], Car [company&#x3D;BMW, brand&#x3D;&lt;shanghai&gt;, maxSpeed&#x3D;250, price&#x3D;0.0]]] 2.4.7 使用 p 命名空间配置文件 123&lt;!-- 通过p命名空间为bean的属性赋值，需要先导入p命名空间 ,相对于传统的方式更简洁--&gt; &lt;bean id&#x3D;&quot;person5&quot; class&#x3D;&quot;com.atguigu.spring.beans.collection.Person&quot; p:age&#x3D;&quot;30&quot; p:name&#x3D;&quot;Queen&quot; p:cars-ref&#x3D;&quot;cars&quot;&gt;&lt;&#x2F;bean&gt; 运行结果 12Person [name&#x3D;Queen, age&#x3D;30, cars&#x3D;[Car [company&#x3D;Audi, brand&#x3D;shanghai, maxSpeed&#x3D;400, price&#x3D;30000.0], Car [company&#x3D;BMW, brand&#x3D;&lt;shanghai&gt;, maxSpeed&#x3D;250, price&#x3D;0.0]]] 2.4.8 泛型依赖注入 2.5 本节所用源码 2.5.1 包com.atguigu.spring.beansCar.java12345678910111213141516171819202122232425262728293031323334353637package com.atguigu.spring.beans;public class Car &#123; private String company; private String brand; private int maxSpeed; private double price; &#x2F;&#x2F;构造器 public Car(String company, String brand, double price) &#123; super(); this.company &#x3D; company; this.brand &#x3D; brand; this.price &#x3D; price; &#125; public Car(String company, String brand, int maxSpeed) &#123; super(); this.company &#x3D; company; this.brand &#x3D; brand; this.maxSpeed &#x3D; maxSpeed; &#125; @Override public String toString() &#123; return &quot;Car [company&#x3D;&quot; + company + &quot;, brand&#x3D;&quot; + brand + &quot;, maxSpeed&#x3D;&quot; + maxSpeed + &quot;, price&#x3D;&quot; + price + &quot;]&quot;; &#125; &#x2F;&#x2F;测试级联属性 public void setMaxSpeed(int maxSpeed) &#123; this.maxSpeed &#x3D; maxSpeed; &#125;&#125; HelloWorld.Java1234567891011121314151617package com.atguigu.spring.beans;public class HelloWorld &#123; private String name; public void setName(String name) &#123; this.name &#x3D; name; &#125; public void hello() &#123; System.out.println(&quot;hello:&quot; + name); &#125;&#125; Main.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.atguigu.spring.beans;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class Main &#123; public static void main(String[] args) &#123; &#x2F;&#x2F; TODO Auto-generated method stub &#x2F;* &#x2F;&#x2F;创建 HelloWorld的一个对象 HelloWorld helloWorld &#x3D; new HelloWorld(); &#x2F;&#x2F;为name属性赋值 helloWorld.setName(&quot;lizhi&quot;); &#x2F;&#x2F;调用hello方法 helloWorld.hello(); *&#x2F; &#x2F;&#x2F;1. 创建Spring的IOC容器对象 ApplicationContext ctx &#x3D; new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); &#x2F;&#x2F;2. 从IOC容器中获取Bean实例 HelloWorld helloWorld &#x3D; (HelloWorld) ctx.getBean(&quot;helloWorld&quot;);&#x2F;&#x2F;这里填xml里配置de &#x2F;&#x2F;3. 调用hello方法 helloWorld.hello(); Car car &#x3D; (Car) ctx.getBean(&quot;car&quot;);&#x2F;&#x2F;car为配置文件里的id System.out.println(car); &#x2F;&#x2F; 构造法注入测试 car &#x3D; (Car) ctx.getBean(&quot;car2&quot;); System.out.println(car); &#x2F;&#x2F;引用其它 Bean测试 Person person &#x3D; (Person) ctx.getBean(&quot;person&quot;); System.out.println(person); person &#x3D; (Person) ctx.getBean(&quot;person2&quot;); System.out.println(person); &#125;&#125; Person.java123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.atguigu.spring.beans;public class Person &#123; private String name; private int age; private Car car; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name &#x3D; name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age &#x3D; age; &#125; public Car getCar() &#123; return car; &#125; public void setCar(Car car) &#123; this.car &#x3D; car; &#125; @Override public String toString() &#123; return &quot;Person [name&#x3D;&quot; + name + &quot;, age&#x3D;&quot; + age + &quot;, car&#x3D;&quot; + car + &quot;]&quot;; &#125; &#x2F;&#x2F;这里对应的是配置文件里的person2 &#x2F;&#x2F;无参构造器 public Person()&#123; &#125; &#x2F;&#x2F;有参构造器 public Person(String name, int age, Car car) &#123; super(); this.name &#x3D; name; this.age &#x3D; age; this.car &#x3D; car; &#125;&#125; 2.5.2 包com.atguigu.spring.beans.collectionDataSource.java12345678910111213141516171819202122package com.atguigu.spring.beans.collection;import java.util.Properties;public class DataSource &#123; private Properties properties; public Properties getProperties() &#123; return properties; &#125; public void setProperties(Properties properties) &#123; this.properties &#x3D; properties; &#125; @Override public String toString() &#123; return &quot;DataSource [properties&#x3D;&quot; + properties + &quot;]&quot;; &#125;&#125; Main.java12345678910111213141516171819202122232425262728293031package com.atguigu.spring.beans.collection;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class Main &#123; public static void main(String[] args) &#123; ApplicationContext ctx &#x3D; new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); Person person &#x3D; (Person) ctx.getBean(&quot;person3&quot;); System.out.println(person); &#x2F;&#x2F;集合测试之map测试类 newPerson person2 &#x3D; (newPerson) ctx.getBean(&quot;newPerson&quot;); System.out.println(person2); &#x2F;&#x2F;集合测试之Properties DataSource dataSource &#x3D; (DataSource) ctx.getBean(&quot;dataSource&quot;); System.out.println(dataSource); &#x2F;&#x2F;测试使用utility scheme 定义集合 person &#x3D; (Person) ctx.getBean(&quot;person4&quot;); System.out.println(person); &#x2F;&#x2F;测试p命名空间 Person person5 &#x3D; (Person) ctx.getBean(&quot;person5&quot;); System.out.println(person5); &#125;&#125; newPerson.java12345678910111213141516171819202122232425262728293031323334353637package com.atguigu.spring.beans.collection;import java.util.Map;import com.atguigu.spring.beans.Car;&#x2F;&#x2F;集合属性测试public class newPerson &#123; private String name; private int age; private Map&lt;String, Car&gt; cars; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name &#x3D; name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age &#x3D; age; &#125; public Map&lt;String, Car&gt; getCars() &#123; return cars; &#125; public void setCars(Map&lt;String, Car&gt; cars) &#123; this.cars &#x3D; cars; &#125; @Override public String toString() &#123; return &quot;newPerson [name&#x3D;&quot; + name + &quot;, age&#x3D;&quot; + age + &quot;, cars&#x3D;&quot; + cars + &quot;]&quot;; &#125;&#125; Person.java123456789101112131415161718192021222324252627282930313233343536373839package com.atguigu.spring.beans.collection;import java.util.List;import com.atguigu.spring.beans.Car;&#x2F;&#x2F;集合属性测试public class Person &#123; private String name; private int age; private List&lt;Car&gt; cars; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name &#x3D; name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age &#x3D; age; &#125; public List&lt;Car&gt; getCars() &#123; return cars; &#125; public void setCars(List&lt;Car&gt; cars) &#123; this.cars &#x3D; cars; &#125; @Override public String toString() &#123; return &quot;Person [name&#x3D;&quot; + name + &quot;, age&#x3D;&quot; + age + &quot;, cars&#x3D;&quot; + cars + &quot;]&quot;; &#125;&#125; 2.5.3 applicationContext.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xmlns:util&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;util&quot; xmlns:p&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;p&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans.xsd http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;util http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;util&#x2F;spring-util-4.3.xsd&quot;&gt;&lt;!-- 配置Bean --&gt; &lt;bean id &#x3D; &quot;helloWorld&quot; class &#x3D; &quot;com.atguigu.spring.beans.HelloWorld&quot;&gt; &lt;property name&#x3D;&quot;name&quot; value&#x3D;&quot;Spring&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt;&lt;!-- 通过构造方法来配置Bean的属性 --&gt; &lt;bean id&#x3D;&quot;car&quot; class&#x3D;&quot;com.atguigu.spring.beans.Car&quot;&gt; &lt;constructor-arg value&#x3D;&quot;Audi&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;constructor-arg value&#x3D;&quot;shanghai&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;constructor-arg value&#x3D;&quot;30000&quot; type&#x3D;&quot;double&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;&#x2F;bean&gt;&lt;!-- 用constructor-arg的排列顺序不能区分的时候要添加type使用构造器注入属性值可以指定参数的位置和参数的类型。以区分重载的构造器--&gt; &lt;bean id&#x3D;&quot;car2&quot; class&#x3D;&quot;com.atguigu.spring.beans.Car&quot;&gt; &lt;constructor-arg value&#x3D;&quot;BMW&quot; type&#x3D;&quot;java.lang.String&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;!-- 特殊字符用&lt;![CDATA[]]&gt; --&gt; &lt;constructor-arg type&#x3D;&quot;java.lang.String&quot;&gt; &lt;value&gt;&lt;![CDATA[&lt;shanghai&gt;]]&gt;&lt;&#x2F;value&gt; &lt;&#x2F;constructor-arg&gt; &lt;constructor-arg type&#x3D;&quot;int&quot;&gt; &lt;value&gt;250&lt;&#x2F;value&gt; &lt;&#x2F;constructor-arg&gt; &lt;&#x2F;bean&gt;&lt;!-- 引用其它 Bean --&gt; &lt;bean id&#x3D;&quot;person&quot; class&#x3D;&quot;com.atguigu.spring.beans.Person&quot;&gt; &lt;property name&#x3D;&quot;name&quot; value&#x3D;&quot;tom&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;age&quot; value&#x3D;&quot;24&quot;&gt;&lt;&#x2F;property&gt; &lt;!-- 可以使用property的ref属性建立bean之间的引用关系。也可以使用ref标签 &lt;property name&#x3D;&quot;car&quot;&gt; &lt;ref bean&#x3D;&quot;car2&quot; &#x2F;&gt; &lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;car&quot; ref&#x3D;&quot;car2&quot;&gt;&lt;&#x2F;property&gt; --&gt;&lt;!-- 内部bean --&gt; &lt;property name&#x3D;&quot;car&quot;&gt; &lt;bean class&#x3D;&quot;com.atguigu.spring.beans.Car&quot;&gt; &lt;constructor-arg value&#x3D;&quot;Ford&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;constructor-arg value&#x3D;&quot;Changan&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;constructor-arg value&#x3D;&quot;200000&quot; type&#x3D;&quot;double&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;&#x2F;bean&gt; &lt;&#x2F;property&gt; &lt;&#x2F;bean&gt; &lt;bean id&#x3D;&quot;person2&quot; class&#x3D;&quot;com.atguigu.spring.beans.Person&quot;&gt; &lt;constructor-arg value&#x3D;&quot;Jerry&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;constructor-arg value&#x3D;&quot;25&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;!-- &lt;constructor-arg ref&#x3D;&quot;car&quot;&gt;&lt;&#x2F;constructor-arg&gt; --&gt; &lt;!-- 测试赋值null --&gt; &lt;!-- &lt;constructor-arg&gt;&lt;null&#x2F;&gt;&lt;&#x2F;constructor-arg&gt; --&gt; &lt;!-- 测试级联属性 --&gt; &lt;constructor-arg ref&#x3D;&quot;car&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;property name&#x3D;&quot;car.maxSpeed&quot; value&#x3D;&quot;400&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt; &lt;!-- 测试如何配置集合属性 --&gt; &lt;bean id&#x3D;&quot;person3&quot; class&#x3D;&quot;com.atguigu.spring.beans.collection.Person&quot;&gt; &lt;property name&#x3D;&quot;name&quot; value&#x3D;&quot;Mike&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;age&quot; value&#x3D;&quot;27&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;cars&quot; &gt; &lt;!-- 使用List节点 为List属性赋值 --&gt; &lt;list&gt; &lt;ref bean&#x3D;&quot;car&quot;&#x2F;&gt; &lt;ref bean&#x3D;&quot;car2&quot;&#x2F;&gt; &lt;&#x2F;list&gt; &lt;&#x2F;property&gt; &lt;&#x2F;bean&gt; &lt;!-- 集合属性配置之map --&gt; &lt;bean id&#x3D;&quot;newPerson&quot; class&#x3D;&quot;com.atguigu.spring.beans.collection.newPerson&quot;&gt; &lt;property name&#x3D;&quot;name&quot; value&#x3D;&quot;Rose&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;age&quot; value&#x3D;&quot;18&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;cars&quot;&gt; &lt;!-- 使用map节点及map的entry 子节点配置Map类型的成员变量--&gt; &lt;map&gt; &lt;entry key&#x3D;&quot;AA&quot; value-ref&#x3D;&quot;car&quot;&gt;&lt;&#x2F;entry&gt; &lt;entry key&#x3D;&quot;BB&quot; value-ref&#x3D;&quot;car2&quot;&gt;&lt;&#x2F;entry&gt; &lt;&#x2F;map&gt; &lt;&#x2F;property&gt; &lt;&#x2F;bean&gt; &lt;!-- 配置Properties 属性值 --&gt; &lt;bean id&#x3D;&quot;dataSource&quot; class&#x3D;&quot;com.atguigu.spring.beans.collection.DataSource&quot;&gt; &lt;property name&#x3D;&quot;properties&quot;&gt; &lt;!-- 使用props和prop子节点来为 Properties属性赋值 这里模拟了数据库连接 --&gt; &lt;props&gt; &lt;prop key&#x3D;&quot;user&quot;&gt;root&lt;&#x2F;prop&gt; &lt;prop key&#x3D;&quot;password&quot;&gt;123&lt;&#x2F;prop&gt; &lt;prop key&#x3D;&quot;jdbcUrl&quot;&gt;jdbc:mysql:&#x2F;&#x2F;&#x2F;test&lt;&#x2F;prop&gt; &lt;prop key&#x3D;&quot;driverClass&quot;&gt;com.mysql.jdbc.Driver&lt;&#x2F;prop&gt; &lt;&#x2F;props&gt; &lt;&#x2F;property&gt; &lt;&#x2F;bean&gt; &lt;!-- 配置单例的集合bean，以供多个bean进行引用,需要导入util命名空间 --&gt; &lt;util:list id&#x3D;&quot;cars&quot;&gt; &lt;ref bean&#x3D;&quot;car&quot;&#x2F;&gt; &lt;ref bean&#x3D;&quot;car2&quot;&#x2F;&gt; &lt;&#x2F;util:list&gt; &lt;bean id&#x3D;&quot;person4&quot; class&#x3D;&quot;com.atguigu.spring.beans.collection.Person&quot;&gt; &lt;property name&#x3D;&quot;name&quot; value&#x3D;&quot;Jack&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;age&quot; value&#x3D;&quot;18&quot;&gt;&lt;&#x2F;property&gt; &lt;property name&#x3D;&quot;cars&quot; ref&#x3D;&quot;cars&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt; &lt;!-- 通过p命名空间为bean的属性赋值，需要先导入p命名空间 ,相对于传统的方式更简洁--&gt; &lt;bean id&#x3D;&quot;person5&quot; class&#x3D;&quot;com.atguigu.spring.beans.collection.Person&quot; p:age&#x3D;&quot;30&quot; p:name&#x3D;&quot;Queen&quot; p:cars-ref&#x3D;&quot;cars&quot;&gt;&lt;&#x2F;bean&gt;&lt;&#x2F;beans&gt; 源码2.2.4 通过调用静态工厂和实例工厂方法创建 Bean的源码创建包com.atguigu.spring.beans.factoryCar.java12345678910111213141516171819202122232425262728293031323334353637383940package com.atguigu.spring.beans.factory;public class Car &#123; private String brand; private double price; public String getBrand() &#123; return brand; &#125; public void setBrand(String brand) &#123; this.brand &#x3D; brand; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price &#x3D; price; &#125; @Override public String toString() &#123; return &quot;Car [brand&#x3D;&quot; + brand + &quot;, price&#x3D;&quot; + price + &quot;]&quot;; &#125; &#x2F;&#x2F;用来测试Bean的作用域,看调用getBean()方法时被调用了几次 public Car() &#123; &#x2F;&#x2F; TODO Auto-generated constructor stub System.out.println(&quot;Car&#39;s Constructor...&quot;); &#125; &#x2F;&#x2F;创建个带参的构造器 &#x2F;&#x2F;Generate Constructor use fields public Car(String brand, double price) &#123; super(); this.brand &#x3D; brand; this.price &#x3D; price; &#125;&#125; StaticCarFactory.java1234567891011121314151617181920212223242526package com.atguigu.spring.beans.factory;&#x2F;** * 静态工厂方法: 直接调用某一个类的静态方法就可以返回Bean的实例 * @author Administrator * *&#x2F;import java.util.HashMap;import java.util.Map;public class StaticCarFactory &#123; private static Map&lt;String,Car&gt; cars &#x3D; new HashMap&lt;String, Car&gt;(); &#x2F;&#x2F;静态代码块 static &#123; cars.put(&quot;audi&quot;,new Car(&quot;audi&quot;,30000)); cars.put(&quot;ford&quot;,new Car(&quot;audi&quot;,400000)); &#125; &#x2F;&#x2F;静态工厂方法 public static Car getCar(String name) &#123; return cars.get(name); &#125;&#125; InstanceCarFactory.java123456789101112131415161718192021222324package com.atguigu.spring.beans.factory;import java.util.HashMap;import java.util.Map;&#x2F;** * 实例工厂的方法: 即先需要创建工厂本身，再调用工厂的实例化方法返回bean的实例 * @author Administrator * *&#x2F;public class InstanceCarFactory &#123; private Map&lt;String, Car&gt; cars &#x3D; null; public InstanceCarFactory() &#123; cars &#x3D; new HashMap&lt;String, Car&gt;(); cars.put(&quot;audi&quot;,new Car(&quot;audi&quot;,300000)); cars.put(&quot;ford&quot;,new Car(&quot;fort&quot;,400000)); &#125; public Car getCar(String brand) &#123; return cars.get(brand); &#125;&#125; beans-factory.xml12345678910111213141516171819202122232425262728&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans.xsd&quot;&gt; &lt;!-- 通过静态工厂方法来配置Bean，注意不是配置静态工厂实例，而是配置bean实例 --&gt; &lt;!-- class属性:指向静态工厂方法的全类名 factory-method: 指向静态工厂方法的名字 Constructor-arg : 如果工厂方法需要传入参数，则使用constructor-arg 来配置 --&gt; &lt;bean id&#x3D;&quot;car1&quot; class&#x3D;&quot;com.atguigu.spring.beans.factory.StaticCarFactory&quot; factory-method&#x3D;&quot;getCar&quot;&gt; &lt;constructor-arg value&#x3D;&quot;audi&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;&#x2F;bean&gt; &lt;!-- 配置工厂的实例 --&gt; &lt;bean id&#x3D;&quot;carFactory&quot; class&#x3D;&quot;com.atguigu.spring.beans.factory.InstanceCarFactory&quot;&gt;&lt;&#x2F;bean&gt; &lt;!-- 通过实例工厂的方法来配置Bean factory-bean 属性:指向实例工厂方法的bean factory-method: 指向工厂方法的名字 Constructor-arg : 如果工厂方法需要传入参数，则使用constructor-arg 来配置 --&gt; &lt;bean id&#x3D;&quot;car2&quot; factory-bean&#x3D;&quot;carFactory&quot; factory-method&#x3D;&quot;getCar&quot;&gt; &lt;constructor-arg value&#x3D;&quot;ford&quot;&gt;&lt;&#x2F;constructor-arg&gt; &lt;&#x2F;bean&gt;&lt;&#x2F;beans&gt; Main.java123456789101112131415161718192021package com.atguigu.spring.beans.factory;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class Main &#123; public static void main(String[] args) &#123; &#x2F;&#x2F; TODO Auto-generated method stub ApplicationContext ctx &#x3D; new ClassPathXmlApplicationContext(&quot;beans-factory.xml&quot;); &#x2F;&#x2F;静态工厂方法 Car car1 &#x3D; (Car) ctx.getBean(&quot;car1&quot;); System.out.println(car1); &#x2F;&#x2F;实例化工厂的方法 Car car2 &#x3D; (Car) ctx.getBean(&quot;car2&quot;); System.out.println(car2); &#125;&#125; 测试结果123Car [brand&#x3D;audi, price&#x3D;30000.0]Car [brand&#x3D;fort, price&#x3D;400000.0] 2.2.6 实现 FactoryBean 接口在 Spring IOC 容器中配置 BeaBean的源码新建包com.atguigu.spring.beans.factorybeanCar.java12345678910111213141516171819202122232425262728293031323334353637383940package com.atguigu.spring.beans.factorybean;public class Car &#123; private String brand; private double price; public String getBrand() &#123; return brand; &#125; public void setBrand(String brand) &#123; this.brand &#x3D; brand; &#125; public double getPrice() &#123; return price; &#125; public void setPrice(double price) &#123; this.price &#x3D; price; &#125; @Override public String toString() &#123; return &quot;Car [brand&#x3D;&quot; + brand + &quot;, price&#x3D;&quot; + price + &quot;]&quot;; &#125; &#x2F;&#x2F;用来测试Bean的作用域,看调用getBean()方法时被调用了几次 public Car() &#123; &#x2F;&#x2F; TODO Auto-generated constructor stub System.out.println(&quot;Car&#39;s Constructor...&quot;); &#125; &#x2F;&#x2F;创建个带参的构造器 &#x2F;&#x2F;Generate Constructor use fields public Car(String brand, double price) &#123; super(); this.brand &#x3D; brand; this.price &#x3D; price; &#125;&#125; CarFactoryBean.java12345678910111213141516171819202122232425262728293031323334353637package com.atguigu.spring.beans.factorybean;import org.springframework.beans.factory.FactoryBean;public class CarFactoryBean implements FactoryBean&lt;Car&gt;&#123; private String brand; public String getBrand() &#123; return brand; &#125; public void setBrand(String brand) &#123; this.brand &#x3D; brand; &#125; &#x2F;&#x2F;返回Bean的对象 @Override public Car getObject() throws Exception &#123; &#x2F;&#x2F; TODO Auto-generated method stub return new Car(brand,5000000); &#125; &#x2F;** * 返回bean的类型 *&#x2F; @Override public Class&lt;?&gt; getObjectType() &#123; &#x2F;&#x2F; TODO Auto-generated method stub return Car.class; &#125; @Override public boolean isSingleton() &#123; return true; &#125;&#125; beans-beanFactory.xml1234567891011&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans.xsd&quot;&gt; &lt;bean id&#x3D;&quot;car&quot; class&#x3D;&quot;com.atguigu.spring.beans.factorybean.CarFactoryBean&quot;&gt; &lt;property name&#x3D;&quot;brand&quot; value&#x3D;&quot;BMW&quot;&gt;&lt;&#x2F;property&gt; &lt;&#x2F;bean&gt;&lt;&#x2F;beans&gt; Main.java1234567891011121314151617package com.atguigu.spring.beans.factorybean;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class Main &#123; public static void main(String[] args) &#123; &#x2F;&#x2F; TODO Auto-generated method stub ApplicationContext ctx &#x3D; new ClassPathXmlApplicationContext(&quot;beans-beanFactory.xml&quot;); Car car &#x3D; (Car) ctx.getBean(&quot;car&quot;); System.out.println(car); &#125;&#125; 测试结果1Car [brand&#x3D;BMW, price&#x3D;5000000.0] 2.2.0 测试实例先导入spring-aop-5.2.0.RELEASE.jar TestObject.java123456789package com.atguigu.spring.beans.annotation;import org.springframework.stereotype.Component;@Componentpublic class TestObject &#123;&#125; 接口UserRepository.java123456package com.atguigu.spring.beans.annotation.repository;public interface UserRepository &#123; void save();&#125; UserRepositoryImpl.java123456789101112131415package com.atguigu.spring.beans.annotation.repository;import org.springframework.stereotype.Repository;@Repository(&quot;userRepository&quot;)public class UserRepositoryImpl implements UserRepository &#123; @Override public void save() &#123; &#x2F;&#x2F; TODO Auto-generated method stub System.out.println(&quot;UserRepository Save...&quot;); &#125;&#125; UserService.java12345678910111213141516171819package com.atguigu.spring.beans.annotation.service;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import com.atguigu.spring.beans.annotation.repository.UserRepository;@Servicepublic class UserService &#123; @Autowired private UserRepository userRepository; public void add() &#123; System.out.println(&quot;UserService add...&quot;); userRepository.save(); &#125;&#125; UserController.java1234567891011121314151617181920package com.atguigu.spring.beans.annotation.controller;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import com.atguigu.spring.beans.annotation.service.UserService;@Controllerpublic class UserController &#123; @Autowired private UserService userService; public void execute() &#123; System.out.println(&quot;UserController execute...&quot;); userService.add(); &#125;&#125; beans-annotation.xml1234567891011121314151617181920&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xmlns:context&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans.xsd http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&#x2F;spring-context-4.3.xsd&quot;&gt; &lt;!-- 指定Spring IOC 容器扫描的包 ,扫描的包为指定包及其子包--&gt; &lt;context:component-scan base-package&#x3D;&quot;com.atguigu.spring.beans.annotation&quot; &gt;&lt;&#x2F;context:component-scan&gt; &lt;!-- &lt;context:exclude-filter 子节点指定排除哪些指定表达式的组件 &lt;context:include-filter&gt; 子节点表示要包含的目标类 &lt;context:exclude-filter&gt; 子节点表示要排除在外的目标类,该子节点需要和use-default-filters配合使用 &lt;context:component-scan base-package&#x3D;&quot;com.atguigu.spring.beans.annotation&quot; use-default-filters&#x3D;&quot;false&quot;&gt; &lt;context:exclude-filter type&#x3D;&quot;annotation&quot; expression&#x3D;&quot;org.springframework.stereotype.Repository&quot;&#x2F;&gt; &lt;&#x2F;context:component-scan&gt; --&gt;&lt;&#x2F;beans&gt; Main》java1234567891011121314151617181920212223242526272829303132333435package com.atguigu.spring.beans.annotation;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import com.atguigu.spring.beans.annotation.controller.UserController;import com.atguigu.spring.beans.annotation.repository.UserRepository;import com.atguigu.spring.beans.annotation.service.UserService;public class Main &#123; public static void main(String[] args) &#123; &#x2F;&#x2F; TODO Auto-generated method stub ApplicationContext ctx &#x3D; new ClassPathXmlApplicationContext(&quot;beans-annotation.xml&quot;); TestObject tO &#x3D; (TestObject) ctx.getBean(&quot;testObject&quot;); System.out.println(tO); UserController userController &#x3D; (UserController) ctx.getBean(&quot;userController&quot;); System.out.println(userController); &#x2F;&#x2F;测试Autowired自动装配 userController.execute(); UserService userService &#x3D; (UserService) ctx.getBean(&quot;userService&quot;); System.out.println(userService); UserRepository userRepository &#x3D; (UserRepository) ctx.getBean(&quot;userRepository&quot;);&#x2F;&#x2F;对应的java文件首字母小写 System.out.println(userRepository); &#125;&#125; 运行结果12345678com.atguigu.spring.beans.annotation.TestObject@93081b6com.atguigu.spring.beans.annotation.controller.UserController@cd1e646UserController execute...UserService add...UserRepository Save...com.atguigu.spring.beans.annotation.service.UserService@7ba8c737com.atguigu.spring.beans.annotation.repository.UserRepositoryImpl@1890516e 2.4.9 泛型依赖注入测试源码新建包com.atguigu.spring.beans.generic.diBaseRepository.java123456package com.atguigu.spring.beans.generic.di;public class BaseRepository&lt;T&gt; &#123;&#125; BaseService.java12345678910111213141516package com.atguigu.spring.beans.generic.di;import org.springframework.beans.factory.annotation.Autowired;public class BaseService&lt;T&gt;&#123; @Autowired protected BaseRepository repository; public void add() &#123; System.out.println(&quot;add...&quot;); System.out.println(repository); &#125;&#125; UserService.java123456789package com.atguigu.spring.beans.generic.di;import org.springframework.stereotype.Service;@Servicepublic class UserService extends BaseService&lt;User&gt;&#123;&#125; User.java123456package com.atguigu.spring.beans.generic.di;public class User &#123;&#125; UserRepository.java12345678package com.atguigu.spring.beans.generic.di;import org.springframework.stereotype.Repository;@Repositorypublic class UserRepository extends BaseRepository&lt;User&gt; &#123;&#125; beans-generic-di.xml12345678910&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;&lt;beans xmlns&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot; xmlns:context&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&quot; xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;beans&#x2F;spring-beans.xsd http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context http:&#x2F;&#x2F;www.springframework.org&#x2F;schema&#x2F;context&#x2F;spring-context-4.3.xsd&quot;&gt; &lt;context:component-scan base-package&#x3D;&quot;com.atguigu.spring.beans.generic.di&quot;&gt;&lt;&#x2F;context:component-scan&gt;&lt;&#x2F;beans&gt; Main.java1234567891011121314151617package com.atguigu.spring.beans.generic.di;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;public class Main &#123; public static void main(String[] args) &#123; &#x2F;&#x2F; TODO Auto-generated method stub ApplicationContext ctx &#x3D; new ClassPathXmlApplicationContext(&quot;beans-generic-di.xml&quot;); UserService userService &#x3D; (UserService) ctx.getBean(&quot;userService&quot;); userService.add(); &#125;&#125; 测试结果12add...com.atguigu.spring.beans.generic.di.UserRepository@93081b6","categories":[{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"ssm","slug":"ssm","permalink":"http://www.studyz.club/tags/ssm/"},{"name":"Spring","slug":"Spring","permalink":"http://www.studyz.club/tags/Spring/"},{"name":"Bean","slug":"Bean","permalink":"http://www.studyz.club/tags/Bean/"}]},{"title":"计算机网络······网络层","slug":"计算机网络之网络层","date":"2019-07-21T00:58:01.637Z","updated":"2019-10-04T16:02:45.440Z","comments":true,"path":"posts/f389f85/","link":"","permalink":"http://www.studyz.club/posts/f389f85/","excerpt":"","text":"计算机网络之网络层网络层提供的两种服务一，网络层提供的两种服务","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://www.studyz.club/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://www.studyz.club/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"网络层","slug":"网络层","permalink":"http://www.studyz.club/tags/%E7%BD%91%E7%BB%9C%E5%B1%82/"}]},{"title":"计算机网络······数据链路层","slug":"计算机网络之数据链路层","date":"2019-06-25T10:16:34.109Z","updated":"2019-06-30T12:59:52.553Z","comments":true,"path":"posts/a265c126/","link":"","permalink":"http://www.studyz.club/posts/a265c126/","excerpt":"","text":"1 数据链路层 1.2 使用点对点信道的数据链路层 1.2.1 数据链路和帧 1.2.2 封装成帧 1.2.3 透明传输 1.2.4 差错检测 1.2.4.1 循环冗余检验 CRC 1.2.4.2 帧检验序列 FCS 1.3 点对点协议 PPP 1.3.1 字符填充 1.3.2 零比特填充 1.3.3 PPP 协议的工作状态 1.4 使用广播信道的数据链路层 1.4.1 局域网的数据链路层 1.4.1.1 以太网的两个标准 1.4.1.2 适配器(网卡)的作用 1.4.2 CSMA/CD 协议 1.4.2.1 CSMA/CD协议要点 1.4.2.2 CSMA/CD 协议的重要特性 1.5 使用集线器的星形拓扑 1.6 以太网的 MAC 层 1.6.1 MAC 层的硬件地址 1.6.2 MAC 帧的格式 1.7 扩展的以太网 1.7.1 在物理层扩展以太网 集线器 1.7.2 在数据链路层扩展以太网 交换机 1.7.3 从总线以太网到星形以太网 1.7.4 虚拟局域网 1.8 高速以太网 1 数据链路层 数据链路层是实现设备之间通信的非常重要的一层。 网络中的主机、路由器等都必须实现数据链路层 数据链路层的作用 数据与链路层的简单模型: 数据链路层不必考虑物理层如何实现比特传输的细节。甚至还可以更简单地设想好像是沿着两个数据链路层之间的水平方向把帧直接发送到对方。 常常在两个对等的数据链路层之间画出一个数字管道，而在这条数字管道上传输的数据单位是帧。 数据链路层使用的主要信道有一下两种: 点对点信道，这种信道使用一对一的点对点通信方式。 广播信道，这种信道使用一对多的广播通信方式，广播信道上连接的主机很多，因此必须使用专用的共享信道协议来协调这些主机数据的发送。 1.2 使用点对点信道的数据链路层1.2.1 数据链路和帧 链路 (link) 是一条无源的点到点的物理线路段，中间没有任何其他的交换结点。 一条链路只是一条通路的一个组成部分。 数据链路 (data link) 除了物理线路外，还必须有通信协议来控制这些数据的传输。若把实现这些协议的硬件和软件加到链路上，就构成了数据链路。 现在最常用的方法是使用适配器（即网卡）来实现这些协议的硬件和软件。 一般的适配器都包括了数据链路层和物理层这两层的功能 也有人采用另外的术语。这就是把链路分为物理链路和逻辑链路。 物理链路就是上面所说的链路。 逻辑链路就是上面的数据链路，是物理链路加上必要的通信协议。 数据链路层传送的是帧 数据链路层不必考虑物理层如何实现比特传输的细节。甚至还可以更简单地设想好像是沿着两个数据链路层之间的水平方向把帧直接发送到对方 三个基本问题数据链路层协议有许多种，但有三个基本问题则是共同的。这三个基本问题是： 封装成帧 透明传输 差错控制 1.2.2 封装成帧 封装成帧 (framing) 就是在一段数据的前后分别添加首部和尾部，然后就构成了一个帧。 首部和尾部的一个重要作用就是进行帧定界。 为了提高效率数据部分应尽可能的大于帧的首部和尾部的长度。 每一种链路层协议都规定了所能传送的帧的数据部分长度上限--最大传输单元MTU(Maximum Transfer Unit) 用控制字符进行帧定界的方法举例 当数据是由可打印的 ASCII 码组成的文本文件时，帧定界可以使用特殊的帧定界符。 控制字符 SOH (Start Of Header) 放在一帧的最前面，表示帧的首部开始。另一个控制字符 EOT (End Of Transmission) 表示帧的结束。 SOH和EOH都是控制字符的名称。他们的二进制编码分别是01(00000001)和04(00000100)。 帧定界符的作用。假定发送端在尚未发送完一个帧时突然出现了故障，中断了发送。但随后很快又恢复了正常，于是重新从头开始发送这个未发送完的帧。由于有了帧定界符，接收端就知道前面收到的数据是个不完整的帧(只有首部开始符SOH而没有传输结束符EOT)，必须丢弃。而后面收到的数据有明显的帧定界符(SOH和EOT)，因此这是一个完整的帧，应当收下。 1.2.3 透明传输 由于帧的开始和结束标记使用专门指明的控制符，因此，所传输的数据中的任何8比特组合一定不允许和用作帧定界符的控制符的比特编码一样，否则就会出现帧定界错误。 当传送的是文本时，(文本由键盘输入)不会出现和帧定界控制符一样的二进制位。可见不管从键盘上输入什么都能放在帧中传输，这样的传输就是透明传输。 当传输的是飞ASCII码文件时(如二进制代码的计算机程序或图像等)，如果数据中的某个字节的二进制码正好和SOH或EOH一样，数据链路层就会错误地找到帧定界符，把部分帧收下，而把剩下的那部分丢弃。 解决透明传输问题 解决方法：字节填充 (byte stuffing) 或字符填充 (character stuffing)。 发送端的数据链路层在数据中出现控制字符“SOH”或“EOT”的前面插入一个转义字符“ESC”(其十六进制编码是1B)。 接收端的数据链路层在将数据送往网络层之前删除插入的转义字符。 如果转义字符也出现在数据当中，那么应在转义字符前面插入一个转义字符 ESC。当接收端收到连续的两个转义字符时，就删除其中前面的一个。 “在数据链路层透明传送数据”表示无论发送什么样的比特组合的数据，这些数据都能够按照原样没有差错地通过这个数据链路层。用“字节填充”法解决透明传输的问题 1.2.4 差错检测 在传输过程中可能会产生比特差错：1 可能会变成 0， 而 0 也可能变成 1。 在一段时间内，传输错误的比特占所传输比特总数的比率称为误码率 BER (Bit Error Rate)。 误码率与信噪比有很大的关系。 为了保证数据传输的可靠性，在计算机网络传输数据时，必须采用各种差错检测措施。 在数据链路层传送的帧中，广泛使用了循环冗余检验 CRC 的检错技术。 1.2.4.1 循环冗余检验的原理 在发送端，先把数据划分为组。假定每组 k 个比特。 在每组 M 后面再添加供差错检测用的 n 位冗余码，然后一起发送出去。 冗余码的计算 用二进制的模 2 运算进行 2n 乘 M 的运算，这相当于在 M 后面添加 n 个 0。 得到的 (k + n) 位的数除以事先选定好的长度为 (n + 1) 位的除数 P，得出商是 Q 而余数是 R，余数 R 比除数 P 少 1 位，即 R 是 n 位。 将余数 R 作为冗余码拼接在数据 M 后面，一起发送出去。 接收端对收到的每一帧进行 CRC 检验 (1) 若得出的余数 R = 0，则判定这个帧没有差错，就接受 (accept)。 (2) 若余数 R ≠ 0，则判定这个帧有差错，就丢弃。 但这种检测方法并不能确定究竟是哪一个或哪几个比特出现了差错。 只要经过严格的挑选，并使用位数足够多的除数 P，那么出现检测不到的差错的概率就很小很小。 冗余码的计算举例 现在 k = 6, M = 101001。 设 n = 3, 除数 P = 1101， 被除数是 2nM = 101001000。模 2 运算的结果是：商 Q = 110101，余数 R = 001。 把余数 R 作为冗余码添加在数据 M 的后面发送出去。发送的数据是：2nM + R，即：101001001，共 (k + n) 位。 模2运算又称异或运算：进行加法时不进位，例如，1111+1010=0101。减法和加法一样，按加法规则计算。a与b相同时为0，不相同时为1，不涉及借位，只要看是否相同。 循环冗余检验的原理说明 一种较方便的方法是用多项式来表示循环冗余检验过程。在上面的例子中，用多项式 P(X)=X^3+X^2+1表示上面的除数 P=1101（最高位对应于 X^3, 最低位对应于 X^0）。多项式 P(X) 称为生成多项式。现在广泛使用的生成多项式 P(X) 有以下几种：CRC-16=X^16+X^15+X^2+1CRC-CCITT=X^16+X^12+X^5+1CRC-32=X^32+X^26+X^23+X^22+X^16+X^12+X^11+X^10+X^8+X^7+X^5+X^4+X^2+X+1 1.2.4.2 帧检验序列 FCS 在数据后面添加上的冗余码称为帧检验序列 FCS (Frame Check Sequence)。 循环冗余检验 CRC 和帧检验序列 FCS 并不等同。 CRC 是一种常用的检错方法，而 FCS 是添加在数据后面的冗余码。 FCS 可以用 CRC 这种方法得出，但 CRC 并非用来获得 FCS 的唯一方法。 注意 仅用循环冗余检验 CRC 差错检测技术只能做到无差错接受 (accept)。 “无差错接受”是指：“凡是接受的帧（即不包括丢弃的帧），我们都能以非常接近于 1 的概率认为这些帧在传输过程中没有产生差错”。 也就是说：“凡是接收端数据链路层接受的帧都没有传输差错”（有差错的帧就丢弃而不接受）。 单纯使用 CRC 差错检测技术不能实现“无差错传输”或“可靠传输”。 应当明确，“无比特差错”与“无传输差错”是不同的概念。 在数据链路层使用 CRC 检验，能够实现无比特差错的传输，但这还不是可靠传输。 要做到“无差错传输”（即发送什么就收到什么）就必须再加上确认和重传机制。 本章介绍的数据链路层协议都不是可靠传输的协议。 如果出现帧丢失，帧重复，帧失序这类传输类差错则检验不出来。因此在CRC的基础上添加帧编号，确认，和重传机制。收到正确的帧要向发送端发送确认，发送端在一段时间期限内没有收到接收端的确认，就认为出现了差错，因而进行重传，直到收到对方确认为止。现在对于因为通信链路质量不好引起的，差错概率已经大大降低，数据链路层不使用确认和重传机制，即不需要数据链路层向上提供可靠传输。而是由上层协议(例如TCP协议)来检测和改正差错，对于通信质量较差的无线传输线路，数据链路层协议使用确认和重传机制，数据链路层向上提供可靠传输。这样做可以提高通信效率。 1.3 点对点协议 PPP 对于点对点的链路，目前使用得最广泛的数据链路层协议是``点对点协议 PPP` (Point-to-Point Protocol)。 PPP 协议在 1994 年就已成为互联网的正式标准。 用户到 ISP 的链路使用 PPP 协议 PPP 协议应满足的需求 简单 —— 这是首要的要求。 封装成帧 —— 必须规定特殊的字符作为帧定界符。 透明性 —— 必须保证数据传输的透明性。 多种网络层协议 —— 能够在同一条物理链路上同时支持多种网络层协议。 多种类型链路 —— 能够在多种类型的链路上运行。 差错检测 —— 能够对接收端收到的帧进行检测，并立即丢弃有差错的帧。 检测连接状态 —— 能够及时自动检测出链路是否处于正常工作状态。 最大传送单元 —— 必须对每一种类型的点对点链路设置最大传送单元 MTU 的标准默认值，促进各种实现之间的互操作性。 网络层地址协商 —— 必须提供一种机制使通信的两个网络层实体能够通过协商知道或能够配置彼此的网络层地址。 数据压缩协商 —— 必须提供一种方法来协商使用数据压缩算法 PPP 协议不需要的功能 纠错 ，流量控制，序号 在TCP/IP协议族中，可靠传输由运输层的TCP协议负责。因此ppp协议不需要纠错，设置序号，也不需要进行流量控制。 多点线路 多点线路是指一个主站轮流和链路上的多个从站进行通信。 ppp协议只支持点对点的链路通信。 半双工或单工链路 ppp协议只支持全双工通信 PPP 协议的组成PPP 协议有三个组成部分： 一个将 IP 数据报封装到串行链路的方法。 链路控制协议 LCP (Link Control Protocol)。 网络控制协议 NCP (Network Control Protocol)。 PPP 协议的帧格式 PPP 帧的首部和尾部分别为 4 个字段和 2 个字段。 标志字段 F = 0x7E （符号“0x”表示后面的字符是用十六进制表示。十六进制的 7E 的二进制表示是 01111110）。 地址字段 A 只置为 0xFF。地址字段实际上并不起作用。 控制字段 C 通常置为 0x03。 PPP 是面向字节的，所有的 PPP 帧的长度都是整数字节。 透明传输问题 当 PPP 用在异步传输时，就使用一种特殊的字符填充法。 当 PPP 用在同步传输链路时，协议规定采用硬件来完成比特填充（和 HDLC 的做法一样）。 同步传输方式中发送方和接收方的时钟是统一的、字符与字符间的传输是同步无间隔的。异步传输方式并不要求发送方和接收方的时钟完全一样，字符与字符间的传输是异步的。 异步传输是面向字符的传输，而同步传输是面向比特的传输。异步传输的单位是字符而同步传输的单位是桢。异步传输通过字符起止的开始和停止码抓住再同步的机会，而同步传输则是以数据中抽取同步信息。异步传输对时序的要求较低，同步传输往往通过特定的时钟线路协调时序。异步传输相对于同步传输效率较低。https://zhidao.baidu.com/question/88453623.html 1.3.1 字符填充 当信息字段中出现和标志字段一样的比特(0X7E)组合时，就必须采取一些措施使这种形式上和标记字段一样的比特组合不出现在信息字段中。 将信息字段中出现的每一个 0x7E 字节转变成为 2 字节序列 (0x7D, 0x5E)。 若信息字段中出现一个 0x7D 的字节, 则将其转变成为 2 字节序列 (0x7D, 0x5D)。 若信息字段中出现 ASCII 码的控制字符（即数值小于 0x20 的字符），则在该字符前面要加入一个 0x7D 字节，同时将该字符的编码加以改变。 1.3.2 零比特填充 PPP 协议用在 SONET/SDH 链路时，使用同步传输（一连串的比特连续传送）。这时 PPP 协议采用零比特填充方法来实现透明传输。 在发送端，只要发现有 5 个连续 1，则立即填入一个 0。 接收端对帧中的比特流进行扫描。每当发现 5 个连续1时，就把这 5 个连续 1 后的一个 0 删除。 不提供使用序号和确认的可靠传输 PPP 协议之所以不使用序号和确认机制是出于以下的考虑： 在数据链路层出现差错的概率不大时，使用比较简单的 PPP 协议较为合理。 在因特网环境下，PPP 的信息字段放入的数据是 IP 数据报。数据链路层的可靠传输并不能够保证网络层的传输也是可靠的。 帧检验序列 FCS 字段可保证无差错接受。 1.3.3 PPP 协议的工作状态 当用户拨号接入 ISP 时，路由器的调制解调器对拨号做出确认，并建立一条物理连接。 PC 机向路由器发送一系列的 LCP 分组（封装成多个 PPP 帧）。 这些分组及其响应选择一些 PPP 参数，并进行网络层配置，NCP 给新接入的 PC 机分配一个临时的 IP 地址，使 PC 机成为因特网上的一个主机。 通信完毕时，NCP 释放网络层连接，收回原来分配出去的 IP 地址。接着，LCP 释放数据链路层连接。最后释放的是物理层的连接。 可见，PPP 协议已不是纯粹的数据链路层的协议，它还包含了物理层和网络层的内容。 ppp线路的起始和终止状态永远是链路静止(Link Dead)状态,这时在用户个人电脑和ISP的路由器之间并不存在物理层的链接。 用户个人电脑通过调制解调器呼叫路由器时(通常是在屏幕上用鼠标点击一个连接按钮)，路由器能够检测到调制解调器发出的载波信号。双方建立了物理层的连接后，ppp就进入了链路建立(Link Establish)状态。其目的是建立链路层的LCP连接。 这时LCP开始协商一写配置选项，即发送LCP的配置请求帧(Configure-Request)。链路的另一端可以发送以下几种响应的一种: 配置确认帧(Configure-Ack) 所有选项都接受。 配置否认帧(Configure-Nak) 所有选项都理解但不能接受。 配置拒绝帧(Configure-Reject) 选项有的无法识别或不能接受，需要协商。(p81) 1.4 使用广播信道的数据链路层1.4.1 局域网的数据链路层 局域网最主要的特点是： 网络为一个单位所拥有； 地理范围和站点数目均有限。 局域网具有如下主要优点： 具有广播功能，从一个站点可很方便地访问全网。局域网上的主机可共享连接在局域网上的各种硬件和软件资源。 便于系统的扩展和逐渐地演变，各设备的位置可灵活调整和改变。 提高了系统的可靠性、可用性和残存性。 局域网的拓扑结构 除了这三种拓扑结构还有网形和树形局域网可以使用多种传输媒体。双绞线最便宜且已成为主流，当数据率很高时，往往需要使用光纤作为传输媒体。 共享信道带来的问题 媒体共享技术 静态划分信道 频分复用 时分复用 波分复用 码分复用 动态媒体接入控制（多点接入）。特点：信道并非在用户通信时固定分配给用户。 随机接入 所有的用户可以随机的发送消息 如果两个或更多的用户在同一时刻发送消息，那么共享媒体上就要产生碰撞(即发生了冲突)，使得这些用户都发送失败，因此必须有解决网络碰撞的协议。 受控接入 特点是用户不能随机的发送信息而必须服从一定的控制。 如多点线路探询 (polling)，或轮询。 1.4.1.1 以太网的两个标准 DIX Ethernet V2 是世界上第一个局域网产品（以太网）的规约。 IEEE 802.3 是第一个 IEEE 的以太网标准。 DIX Ethernet V2 标准与 IEEE 的 802.3 标准只有很小的差别，因此可以将 802.3 局域网简称为“以太网”。 严格说来，“以太网”应当是指符合 DIX Ethernet V2 标准的局域网 。 数据链路层的两个子层 为了使数据链路层能更好地适应多种局域网标准，IEEE 802 委员会就将局域网的数据链路层拆成两个子层： 逻辑链路控制 LLC (Logical Link Control)子层； 媒体接入控制 MAC (Medium Access Control)子层。 与接入到传输媒体有关的内容都放在 MAC子层，而 LLC 子层则与传输媒体无关。 不管采用何种协议的局域网，对 LLC 子层来说都是透明的。 局域网对 LLC 子层是透明的 现在很多厂商生产的适配器上就仅装有MAC协议而没有LCC协议。 一般不考虑 LLC 子层 1.4.1.2 适配器(网卡)的作用 网络接口板又称为通信适配器 (adapter) 或网络接口卡 NIC (Network Interface Card)，或“网卡”。 适配器的重要功能： 进行串行/并行转换。 对数据进行缓存。 在计算机的操作系统安装设备驱动程序。 实现以太网协议。 计算机通过适配器和局域网进行通信 1.4.2 CSMA/CD 协议 最初的以太网是将许多计算机都连接到一根总线上。易于实现广播通信。当初认为这样的连接方法既简单又可靠，因为总线上没有有源器件。 总线的特点是:当一台计算机发送数据时，总线上的所有计算机都能检测到这个数据。这就是广播信道方式，但我们并不总是要在局域网上进行一对多的广播通信。 为了实现一对一通信，将接收站的硬件地址写入帧首部中的目的地址字段中。仅当数据帧中的目的地址与适配器的硬件地址一致时，才能接收这个数据帧。 总线也有缺点。若多台计算机或多个站点同时发送时，会产生发送碰撞或冲突，导致发送失败。同一时间只能允许一台计算机发送数据 人们也常把局域网上的计算机称为主机，工作站,站点或站。 以太网采取了两种重要的措施为了通信的简便，以太网采取了两种重要的措施： (1)采用较为灵活的无连接的工作方式 不必先建立连接就可以直接发送数据。 对发送的数据帧不进行编号，也不要求对方发回确认。 这样做的理由是局域网信道的质量很好，因信道质量产生差错的概率是很小的。 以太网提供的服务 以太网提供的服务是不可靠的交付，即尽最大努力的交付。 当目的站收到有差错的数据帧时就丢弃此帧，其他什么也不做。差错的纠正由高层来决定。(例TCP协议) 如果高层发现丢失了一些数据而进行重传，但以太网并不知道这是一个重传的帧，而是当作一个新的数据帧来发送。 如何避免同时发送产生的碰撞？ 采用 CSMA/CD (2) 以太网发送的数据都使用曼彻斯特 (Manchester) 编码曼彻斯特编码缺点是：它所占的频带宽度比原始的基带信号增加了一倍。 1.4.2.1 CSMA/CD协议要点 CSMA/CD 含义：载波监听多点接入 / 碰撞检测 (Carrier Sense Multiple Access with Collision Detection) 。 “多点接入”表示这是总线型网络，许多计算机以多点接入的方式连接在一根总线上。 “载波监听”是指每一个站在发送数据之前先要检测一下总线上是否有其他计算机在发送数据，如果有，则暂时不要发送数据，以免发生碰撞。 总线上并没有什么“载波”。因此， “载波监听”就是用电子技术检测总线上有没有其他计算机发送的数据信号。 不管是在发送前还是在发送中，每个站必须不停的检测信道。 “碰撞检测”也就是边发送边监听，就是计算机边发送数据边检测信道上的信号电压大小。 当几个站同时在总线上发送数据时，总线上的信号电压摆动值将会增大（互相叠加）。 当一个站检测到的信号电压摆动值超过一定的门限值时，就认为总线上至少有两个站同时在发送数据，表明产生了碰撞。 所谓“碰撞”就是发生了冲突。因此“碰撞检测”也称为“冲突检测”。 检测到碰撞后 在发生碰撞时，总线上传输的信号产生了严重的失真，无法从中恢复出有用的信息来。 每一个正在发送数据的站，一旦发现总线上出现了碰撞，就要立即停止发送，免得继续浪费网络资源，然后等待一段随机时间后再次发送。 CSMA/CD 协议工作流程 为什么要进行碰撞检测？ 因为信号传播时延对载波监听产生了影响A 需要单程传播时延的 2 倍的时间，才能检测到与 B 的发送产生了冲突 争用期 二进制指数类型退避算法 (truncated binary exponential type) * 当参数K&gt;10时，K就不在增大而是一直等于10 例如： 第 1 次冲突重传时： k = 1，r 为 {0，1} 集合中的任何一个数。 第 2 次冲突重传时： k = 2，r 为 {0，1，2，3} 集合中的任何一个数。 第 3 次冲突重传时： k = 3，r 为 {0，1，2，3，4，5，6，7} 集合中的任何一个数。 10 Mbit/s 以太网争用期的长度 这意味着：以太网在发送数据时，若前 64 字节没有发生冲突，则后续的数据就不会发生冲突。 最短有效帧长 如果发生冲突，就一定是在发送的前 64 字节之内。 由于一检测到冲突就立即中止发送，这时已经发送出去的数据一定小于 64 字节。 以太网规定了最短有效帧长为 64 字节，凡长度小于 64 字节的帧都是由于冲突而异常中止的无效帧 64字节并不是固定的，发送越快(千兆网)，网线越长。最短帧越大。因为要让数据充满整个线路。因此,有规定以太网不能超过100m. 覆盖范围 人为干扰信号 `注意：B 也能够检测到冲突，并立即停止发送数据帧，接着就发送干扰信号。这里为了简单起见，只画出 A 发送干扰信号的情况。` 1.4.2.2 CSMA/CD 协议的重要特性 使用 CSMA/CD 协议的以太网不能进行全双工通信而只能进行双向交替通信（半双工通信）。 每个站在发送数据之后的一小段时间内，存在着遭遇碰撞的可能性。 这种发送的不确定性使整个以太网的平均通信量远小于以太网的最高数据率。 CSMA/CD 协议的要点 1.5 使用集线器的星形拓扑 传统以太网使用同轴电缆，采用总线形拓扑结构 采用双绞线的以太网采用星形拓扑，在星形的中心则增加了一种可靠性非常高的设备，叫做集线器 (hub)。 星形以太网 10BASE-T 使用无屏蔽双绞线，采用星形拓扑。 每个站需要用两对双绞线，分别用于发送和接收。 双绞线的两端使用 RJ-45 插头。 集线器使用了大规模集成电路芯片，因此集线器的可靠性提高。 10BASE-T 的通信距离稍短，每个站到集线器的距离不超过 100m。 10BASE-T 以太网在局域网中的统治地位 这种 10 Mbit/s 速率的无屏蔽双绞线星形网的出现，既降低了成本，又提高了可靠性。 具有很高的性价比。10BASE-T 双绞线以太网的出现，是局域网发展史上的一个非常重要的里程碑，它为以太网在局域网中的统治地位奠定了牢固的基础。从此以太网的拓扑就从总线形变为更加方便的星形网络，而以太网也就在局域网中占据了统治地位。 集线器的特点 集线器是使用电子器件来模拟实际电缆线的工作，因此整个系统仍然像一个传统的以太网那样运行。 使用集线器的以太网在逻辑上仍是一个总线网，各工作站使用的还是 CSMA/CD 协议，并共享逻辑上的总线。(更具体的说，各站的适配器执行CSMA/CD协议。网络中的各站必须竞争对传输媒体的控制，并且在同一时刻至多只允许一个站发送数据)。 集线器很像一个多接口的转发器,有很多接口，工作在物理层，它每个接口仅仅是简单的转发比特，收到什么转发什么，不进行碰撞检测(如果有两个接口同时有信号输入(即发生碰撞)，那么所有的接口都将收不到正确的帧)。 集线器采用了专门的芯片，进行自适应串音回波抵消，减少了近端串音。 1.6 以太网的 MAC 层 重点介绍： MAC 层的硬件地址 MAC 帧的格式 1.6.1 MAC 层的硬件地址 在局域网中，硬件地址又称为物理地址，或 `MAC 地址``。 802 标准所说的“地址”严格地讲应当是每一个站的“名字”或标识符。(802中定义名字指出我们所要寻找的那个资源，地址指出那个资源在何处，路由告诉我们如何到达该处) 但鉴于大家都早已习惯了将这种 48 位的“名字”称为“地址”，所以本书也采用这种习惯用法，尽管这种说法并不太严格。 请注意，如果连接在局域网上的主机或路由器安装有多个适配器，那么这样的主机或路由器就有多个“地址”。更准确些说，这种 48 位“地址”应当是某个接口的标识符。48 位的 MAC 地址 IEEE 802 标准规定 MAC 地址字段可采用 6 字节 ( 48位) 或 2 字节 ( 16 位) 这两种中的一种。 IEEE 的注册管理机构 RA 负责向厂家分配地址字段 6 个字节中的前三个字节 (即高位 24 位)，称为组织唯一标识符。 地址字段 6 个字节中的后三个字节 (即低位 24 位) 由厂家自行指派，称为扩展唯一标识符，必须保证生产出的适配器没有重复地址。 一个地址块可以生成 224 个不同的地址。这种 48 位地址称为 MAC-48，它的通用名称是 EUI-48。 生产适配器时，6 字节的 MAC 地址已被固化在适配器的 ROM，因此，MAC 地址也叫做硬件地址 (hardware address) 或物理地址。 “MAC 地址”实际上就是适配器地址或适配器标识符 EUI-48。 单站地址，组地址，广播地址: IEEE 规定地址字段的第一字节的最低位为 I/G 位。I/G 表示 Individual / Group。 当 I/G 位 = 0 时，地址字段表示一个单站地址。 当 I/G 位 = 1 时，表示组地址，用来进行多播（以前曾译为组播）。此时，IEEE 只分配地址字段前三个字节中的 23 位。 当 I/G 位分别为 0 和 1 时，一个地址块可分别生成 223 个单个站地址和 223 个组地址。 所有 48 位都为 1 时，为广播地址。只能作为目的地址使用。 全球管理与本地管理: IEEE 把地址字段第一字节的最低第 2 位规定为 G/L 位，表示 Global / Local。 当 G/L 位 = 0 时，是全球管理（保证在全球没有相同的地址），厂商向 IEEE 购买的 OUI 都属于全球管理。 当 G/L 位 = 1 时， 是本地管理，这时用户可任意分配网络上的地址。 适配器检查 MAC 地址 适配器从网络上每收到一个 MAC 帧就首先用硬件检查 MAC 帧中的 MAC 地址。 如果是发往本站的帧则收下，然后再进行其他的处理。 否则就将此帧丢弃，不再进行其他的处理。 “发往本站的帧”包括以下三种帧： 单播 (unicast) 帧（一对一） 广播 (broadcast) 帧（一对全体） 多播 (multicast) 帧（一对多） 所有的适配器都至少能够识别前两种帧，即能够识别单播地址和广播地址。 有的适配器可用编程方法识别多播地址。 只有目的地址才能使用广播地址和多播地址。 以混杂方式 (promiscuous mode) 工作的以太网适配器只要“听到”有帧在以太网上传输就都接收下来。 1.6.2 MAC 帧的格式 常用的以太网 MAC 帧格式有两种标准： DIX Ethernet V2 标准 IEEE 的 802.3 标准 最常用的 MAC 帧是以太网 V2 的格式。 以太网 V2 的 MAC 帧格式mac帧:实际传送的mac帧: 当一个站在刚开始接受MAC帧时，由于适配器的时钟尚未与到达的比特流达成同步，因此MAC帧的最前面的若干位就无法接收，结果使整个的MAC帧成为无用帧。为了接收端实现迅速同步。从MAC子层向下传到物理层时还要在帧前面插入8字节(由硬件生成)。 前同步码:由七个字节组成(1和0交替码)，它的作用是实现比特(位)同步 帧定界符: 10101011(固定)告诉适配器后面是mac帧 在进行SONET/SDH进行同步传输时，不需要用前同步码，因为，在同步传输是收发双方的位同步总是保持一致的。 插入的8字节(为了达到比特同步) 在帧的前面插入（硬件生成）的 8 字节中，第一个字段共 7 个字节，是前同步码，用来迅速实现 MAC 帧的比特同步。第二个字段 1 个字节是帧开始定界符，表示后面的信息就是 MAC 帧。 目的地址字段 6 字节，源地址字段 6 字节 类型字段 2 字节 类型字段用来标志上一层使用的是什么协议，以便把收到的 MAC 帧的数据上交给上一层的这个协议。 数据字段 46 ~ 1500 字节 数据字段的正式名称是 MAC 客户数据字段。最小长度 64 字节 - 18 字节的首部和尾部 = 数据字段的最小长度（46字节） 当数据字段的长度小于 46 字节时，应在数据字段的后面加入整数字节的填充字段，以保证以太网的 MAC 帧长不小于 64 字节。 FCS 字段 4 字节 FCS。当传输媒体的误码率为 1x10-8 时，MAC 子层可使未检测到的差错小于 1x10-14 。 无效的 MAC 帧 数据字段的长度与长度字段的值不一致； 帧的长度不是整数个字节； 用收到的帧检验序列 FCS 查出有差错； 数据字段的长度不在 46 ~ 1500 字节之间。 有效的 MAC 帧长度为 64 ~ 1518 字节之间。 `对于检查出的无效 MAC 帧就简单地丢弃。以太网不负责重传丢弃的帧。` IEEE 802.3 MAC 帧格式 与以太网 V2 MAC 帧格式相似，区别在于： IEEE 802.3 规定的 MAC 帧的第三个字段是“长度 / 类型”。 当这个字段值大于 0x0600 时（相当于十进制的 1536），就表示“类型”。这样的帧和以太网 V2 MAC 帧完全一样。 当这个字段值小于 0x0600 时才表示“长度”。 当“长度/类型”字段值小于 0x0600 时，数据字段必须装入上面的逻辑链路控制 LLC 子层的 LLC 帧。 现在市场上流行的都是以太网 V2 的 MAC 帧，但大家也常常把它称为 IEEE 802.3 标准的 MAC 帧。帧间最小间隔 1.7 扩展的以太网1.7.1 在物理层扩展以太网 1.7.2 在数据链路层扩展以太网 从共享总线以太网转到交换式以太网时，所有接入设备的软件和硬件、适配器等都不需要做任何改动。 以太网交换机一般都具有多种速率的接口，方便了各种不同情况的用户。 如果两个交换机之间存在回路。假定A向E发送一帧，帧会从S1的5口到S2的5口，然后再从S2的6口到S1的6口。这样想成无限循环，浪费了网络资源。 1.7.3 从总线以太网到星形以太网 局域网存在的问题 局域网存在的以下几个方面的问题： 扩展性 安全性 可管理性 等 广播域 1.7.4 虚拟局域网 虚拟局域网优点 虚拟局域网（VLAN）技术具有以下主要优点： 改善了性能 简化了管理 降低了成本 改善了安全性 划分虚拟局域网的方法 基于交换机端口 基于计算机网卡的MAC地址 基于协议类型 基于IP子网地址 基于高层应用或服务 虚拟局域网使用的以太网帧格式 IEEE 批准了 802.3ac 标准，该标准定义了以太网的帧格式的扩展，以支持虚拟局域网。 虚拟局域网协议允许在以太网的帧格式中插入一个4字节的标识符，称为 VLAN 标记 (tag)，用来指明该帧属于哪一个虚拟局域网。 插入VLAN标记得出的帧称为802.1Q帧或带标记的以太网帧。 1.8 高速以太网一， 100BASE-T 以太网 速率达到或超过 100 Mbit/s 的以太网称为高速以太网。 100BASE-T 在双绞线上传送 100 Mbit/s 基带信号的星形拓扑以太网，仍使用 IEEE 802.3 的 CSMA/CD 协议。 100BASE-T 以太网又称为快速以太网 (Fast Ethernet)。 1995 年IEEE已把 100BASE-T 的快速以太网定为正式标准，其代号为 IEEE 802.3u。 100BASE-T 以太网的特点 二， 吉比特以太网 允许在 1 Gbit/s 下以全双工和半双工两种方式工作。 使用 IEEE 802.3 协议规定的帧格式。 在半双工方式下使用 CSMA/CD 协议，全双工方式不使用 CSMA/CD 协议。 与 10BASE-T 和 100BASE-T 技术向后兼容。吉比特以太网可用作现有网络的主干网，也可在高带宽（高速率）的应用场合中。 半双工方式工作的吉比特以太网 吉比特以太网工作在半双工方式时，就必须进行碰撞检测。为保持 64 字节最小帧长度，以及 100 米的网段的最大长度，吉比特以太网增加了两个功能： 载波延伸 (carrier extension) 分组突发 (packet bursting) 三， 10吉比特以太网和更快的以太网 10 吉比特以太网（10GE）并非把吉比特以太网的速率简单地提高到 10 倍，其主要特点有： 与 10 Mbit/s、100 Mbit/s 和 1 Gbit/s 以太网的帧格式完全相同。 保留了 802.3 标准规定的以太网最小和最大帧长，便于升级。 不再使用铜线而只使用光纤作为传输媒体。 只工作在全双工方式，因此没有争用问题，也不使用 CSMA/CD 协议。 端到端的以太网传输 以太网的工作范围已经从局域网（校园网、企业网）扩大到城域网和广域网，从而实现了端到端的以太网传输。 这种工作方式的好处有： 技术成熟； 互操作性很好； 在广域网中使用以太网时价格便宜； 采用统一的以太网帧格式，简化了操作和管理。 四, 使用以太网进行宽带接入 IEEE 在 2001 年初成立了 802.3 EFM 工作组，专门研究高速以太网的宽带接入技术问题。 以太网宽带接入具有以下特点： 可以提供双向的宽带通信。 可以根据用户对带宽的需求灵活地进行带宽升级。 可以实现端到端的以太网传输，中间不需要再进行帧格式的转换。这就提高了数据的传输效率且降低了传输的成本。 但是不支持用户身份鉴别。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://www.studyz.club/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://www.studyz.club/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"数据链路层","slug":"数据链路层","permalink":"http://www.studyz.club/tags/%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"}]},{"title":"计算机网络······物理层","slug":"计算机网络之物理层","date":"2019-06-22T15:49:03.606Z","updated":"2019-07-01T02:23:41.999Z","comments":true,"path":"posts/1c8680d9/","link":"","permalink":"http://www.studyz.club/posts/1c8680d9/","excerpt":"","text":"1.1 物理层的基本概念 1.1.1 物理层的主要任务 1.2数据通信的基础知识 1.2.1 数据通信系统的模型 一些常用用语 1.2.2 有关信道的几个概念 1.2.3 有关信道的几个基本概念 调制分为两大类 基本的带通调制方法 1.2.4 常用编码方式 1.2.5 信道的极限容量 数字信号通过实际的信道 限制码元在信道上的传输速率的因素 信道能够通过的频率范围 码间串扰 奈氏准则 信噪比 香农公式 香农公式表明的问题 2 物理层下面的传输媒体 2.1 导引型传输媒体 双绞线 光缆 同轴光缆(卫星天线信号线，退出历史舞台) 单模光纤 多模光纤 光纤的优点 3 信道复用技术 3.1 频分复用、时分复用和统计时分复用 3.1.1 频分复用 3.1.2 时分复用 3.2 波分复用 3.3 码分复用 4 数字传输系统 5 宽带接入技术 5.1 ADSL 技术(了解) 5.2 光纤同轴混合网（HFC网）(了解) 5.3 FTTx 技术 1.1 物理层的基本概念 物理层考虑的是怎样才能在连接各种计算机的传输媒体上传输数据比特流，而不是指具体的传输媒体。 物理层的作用是要尽可能地屏蔽掉不同传输媒体和通信手段的差异。 用于物理层的协议也常称为物理层规程 (procedure)。 1.1.1 物理层的主要任务 主要任务：确定与传输媒体的接口的一些特性。 机械特性 ：指明接口所用接线器的形状和尺寸、引线数目和排列、固定和锁定装置等。 电气特性：指明在接口电缆的各条线上出现的电压的范围。 功能特性：指明某条线上出现的某一电平的电压的意义。 过程特性 ：指明对于不同功能的各种可能事件的出现顺序。 1.2 数据通信的基础知识1.2.1 数据通信系统的模型 一个数据传输系统可以划分为三部分，即源系统(或发送端,发送方)，传输系统(或传输网络)和目的系统(或接收端，接收方) 源系统包括两个部分 源点: 源点设备产生要传输的数据，例如，计算机键盘输入汉字，计算机产生的数字比特流。源点又称为源站或 信源。 发送器 通常源点产生的数字比特流经过发送器编码后才能够在传输系统中进行传输。典型的是调制器。 目的系统也包括两部分 接收器 接受传输系统传送过来的信号，并把它转换为能够被目标设备处理的信息。典型的是解调器。，他把来自传输线路上的模拟信号进行解调，提取出在发送端置入的消息，还原出发送端产生的数字比特流。 终点(destination) 终点设备从接收器获取传送来的数字比特流，然后把信息输出。终点站又称为目的站，或信宿。 一些常用用语 数据 (data) —— 运送消息的实体。 信号 (signal) —— 数据的电气的或电磁的表现。 模拟信号 (analogous signal)或连续信号 —— 代表消息的参数的取值是连续的。 例如，用户家中的调制解调器到电话端局之间的用户先上传送的就是模拟信号。 数字信号 (digital signal)或离散信号—— 代表消息的参数的取值是离散的。 例如，用户家中的计算机到调制解调器之间传送的信号。 码元 (code) —— 在使用时间域（或简称为时域）的波形表示数字信号时，代表不同离散数值的基本波形。 1.2.2 有关信道的几个概念 信道(channel):信道和电路并不等同。信道一般是用来表示向某一个方向传送信息的媒体。因此，一条通信电路往往包含一条发送信道和接收信道。 信道 —— 一般用来表示向某一个方向传送信息的媒体。 单向通信（单工通信）——只能有一个方向的通信而没有反方向的交互。例，无线电广播或有线电广播，电视广播。 双向交替通信（半双工通信）——通信的双方都可以发送信息，但不能双方同时发送(当然也就不能同时接收)。例如，对讲机 双向同时通信（全双工通信）——通信的双方可以同时发送和接收信息。例如，手机 单向通信只需要一条信道，而双向交替通信或双向同时通信则都需要两条信道(每个方向各一条)。显然，双向同时通信的效率最高。 有时人们也常用单工表示双向交替通信。如常说的单工电台并不是只能进行单向通信。所以才不用单工，半双工，全双工作为正式名词。 1.2.3 有关信道的几个基本概念 基带信号（即基本频带信号）—— 来自信源的信号。像计算机输出的代表各种文字或图像文件的数据信号都属于基带信号。 基带信号往往包含有较多的低频成分，甚至有直流成分，而许多信道并不能传输这种低频分量或直流分量。因此必须对基带信号进行调制(modulation)。基带信号是数字信号，频带信号是模拟信号 调制分为两大类： 基带调制：仅对基带信号的波形进行变换，使它能够与信道特性相适应。变换后的信号仍然是基带信号。把这种过程称为编码 (coding)。 带通调制：使用载波 (carrier)进行调制，把基带信号的频率范围搬移到较高的频段，并转换为模拟信号，这样就能够更好地在模拟信道中传输（即仅在一段频率范围内能够通过信道） 。 带通信号 ：经过载波调制后的信号。 因此在传输距离较近时，计算机网络都采用基带传输方式。由于近距离范围内基带信号衰减不大，从而信号内容不会发生改变。例如计算机到显示器，打印机等外设信号。 基本的带通调制方法 最基本的二元制调制方法有以下几种： 调幅(AM)：载波的振幅随基带数字信号而变化。例，0和1分别代表无载波和有载波。 调频(FM)：载波的频率随基带数字信号而变化。例，0和1对应的频率不同。 调相(PM) ：载波的初始相位随基带数字信号而变化。例，0和1对应于0度和180度正弦波和余弦波。 为了达到更高的信息传输速率，必须采用技术上更为复杂的多元制的振幅相位混合调制方法。例如，正交振幅调制 QAM (Quadrature Amplitude Modulation) 1.2.4 常用编码方式 不归零制：正电平代表 1，负电平代表 0。 归零制：正脉冲代表 1，负脉冲代表 0。 曼彻斯特编码：位周期中心的向上跳变代表 0，位周期中心的向下跳变代表 1。但也可反过来定义。 差分曼彻斯特编码：在每一位的中心处始终都有跳变。位开始边界有跳变代表 0，而位开始边界没有跳变代表 1。 从信号波形中可以看出，曼彻斯特 (Manchester) 编码和差分曼彻斯特编码产生的信号频率比不归零制高。 从自同步能力来看，不归零制不能从信号波形本身中提取信号时钟频率（这叫做没有自同步能力），而曼彻斯特编码和差分曼彻斯特编码具有自同步能力。 1.2.5 信道的极限容量 任何实际的信道都不是理想的，在传输信号时会产生各种失真以及带来多种干扰。 码元传输的速率越高，或信号传输的距离越远，或传输媒体质量越差，在信道的输出端的波形的失真就越严重。 数字信号通过实际的信道 从概念上讲，限制码元在信道上的传输速率的因素有以下两个： 信道能够通过的频率范围 信噪比 信道能够通过的频率范围 具体的信道所能通过的频率范围总是有限的。信号中的许多高频分量往往不能通过信道。 1924 年，奈奎斯特 (Nyquist) 就推导出了著名的奈氏准则。他给出了在假定的理想条件下，为了避免码间串扰，码元的传输速率的上限值。 上图所示的发送信号是一种典型的矩形脉冲信号如果信号，它包含很丰富的高频分量。如果信号中的高频分量在传输时受到衰减，那么在接收端收到的波形前沿和后沿就变得不那么陡峭，每一个码元所占的时间界限不在是很明确，而是前后都拖了个“尾巴。这样在接收端收到的信号波形就失去了码元之间的清晰界限。这种现象叫码间串扰。 在任何信道中，码元传输的速率是有上限的，否则就会出现码间串扰的问题，使接收端对码元的判决（即识别）成为不可能。如果信道的频带越宽，也就是能够通过的信号高频分量越多，那么就可以用更高的速率传送码元而不出现码间串扰。信噪比 噪声存在于所有的电子设备和通信信道中。 噪声是随机产生的，它的瞬时值有时会很大。因此噪声会使接收端对码元的判决产生错误。 但噪声的影响是相对的。如果信号相对较强，那么噪声的影响就相对较小。 信噪比就是信号的平均功率和噪声的平均功率之比。常记为S/N，并用分贝 (dB) 作为度量单位。即： 1信噪比(dB) &#x3D; 10 log10(S&#x2F;N ) (dB) 例如，当S/N=10时，信噪比为10dB，而当S/N=1000时，信噪比为30dB。 1984年，香农 (Shannon) 用信息论的理论推导出了带宽受限且有高斯白噪声干扰的信道的极限、无差错的信息传输速率（香农公式）。 12345信道的极限信息传输速率 C 可表达为： C &#x3D; W log2(1+S&#x2F;N) (bit&#x2F;s)其中：W 为信道的带宽（以 Hz 为单位）； S 为信道内所传信号的平均功率； N 为信道内部的高斯噪声功率。 香农公式表明的问题 信道的带宽或信道中的信噪比越大，则信息的极限传输速率就越高。 只要信息传输速率低于信道的极限信息传输速率，就一定可以找到某种办法来实现无差错的传输。 若信道带宽 W 或信噪比 S/N 没有上限（当然实际信道不可能是这样的），则信道的极限信息传输速率 C 也就没有上限。 实际信道上能够达到的信息传输速率要比香农的极限传输速率低不少。 (脉冲干扰，在传输中失真等) 对于频带宽度已确定的信道，如果信噪比不能再提高了，并且码元传输速率也达到了上限值，那么还有办法提高信息的传输速率。 这就是：用编码的方法让每一个码元携带更多比特的信息量。 假定我们的基带信号是: 101011000110111010…如果直接传送，则每个码元传送的信息量是1bit。现在将信号中的3个比特编为一组，即101,001,000,111,010，。。(3个比特可以表示8进制数)。3个比特共有8种排列。我们可以用不同的调制方法表示这样的信号。例如，用8种不同的振幅，或/8种不同的频率，或8种不同的相位进行控制。假定我们用频率调制，用频率f1表示000，f2表示001…,则原来的18个码元的信号就可以转换为由6个新的码元(原来的3个bit构成一个新的码元)组成的信号: 101011000110111010=f5f3f0f6f7f2 若以同样的速率发送码元，则同样的时间所传送的信息量就提高了3倍 香农公式与奈氏准则的适用范围 2 物理层下面的传输媒体 传输媒体也称为传输介质或传输媒介，它就是数据传输系统中在发送器和接收器之间的物理通路。 传输媒体可分为两大类，即导引型传输媒体和非导引型传输媒体。 在导引型传输媒体中，电磁波被导引沿着固体媒体（铜线或光纤）传播。 非导引型传输媒体就是指自由空间。在非导引型传输媒体中，电磁波的传输常称为无线传输。 电信领域使用的电磁波的频谱： 2.1 导引型传输媒体双绞线 最常用的传输媒体。 模拟传输和数字传输都可以使用双绞线，其通信距离一般为几到十几公里。 屏蔽双绞线 STP (Shielded Twisted Pair) 带金属屏蔽层 无屏蔽双绞线 UTP (Unshielded Twisted Pair) 1995 年将布线标准更新为 EIA/TIA-568-A。 此标准规定了 `5 个种类的 UTP 标准``（从 1 类线到 5 类线）。 对传送数据来说，现在最常用的 UTP 是5类线（Category 5 或 CAT5）。 无论哪种类别的双绞线,衰减都随频率的升高而增大。使用更粗的导线可以降低衰减，但却增加了导线的重量和价格，信号应该有足够大的振幅，以使在噪声干扰下能够在接受端正确地被检测出来。双绞线的最高速率还与数字信号的编码方法有很大的关系。 光缆 光纤是光纤通信的传输媒体。 由于可见光的频率非常高，约为 108 MHz 的量级，因此一个光纤通信系统的传输带宽远远大于目前其他各种传输媒体的带宽。 当光线从高折射率的媒体射向低折射率的媒体时，其折射角将大于入射角。因此，如果入射角足够大，就会出现全反射，光也就沿着光纤传输下去。 光纤的工作原理: 只要从纤芯中射到纤芯表面的光线的入射角大于某个临界角度，就可产生全反射。 多模光纤 可以存在多条不同角度入射的光线在一条光纤中传输。这种光纤就称为多模光纤。 单模光纤 若光纤的直径减小到只有一个光的波长，则光纤就像一根波导那样，它可使光线一直向前传播，而不会产生多次反射。这样的光纤称为单模光纤。 光脉冲在多模光纤中传输时会逐渐展宽，造成失真。因此多模光纤之适合于近距离传输。单模光纤的直径只有几微米，制造成本高，但传输损耗较小在100Gbit/s的高速率下可传输100公里而不必采用中继站。 光纤的优点 通信容量非常大。 传输损耗小，中继距离长。 抗雷电和电磁干扰性能好。 无串音干扰，保密性好。 体积小，重量轻。 2.2 非导引型传输媒体 将自由空间称为“非导引型传输媒体”。 无线传输所使用的频段很广。 短波通信（即高频通信）主要是靠电离层的反射，但短波信道的通信质量较差，传输速率低。 微波在空间主要是直线传播。 传统微波通信有两种方式： 地面微波接力通信 卫星通信 无线局域网使用的 ISM 频段: 3 信道复用技术 复用 (multiplexing) 是通信技术中的基本概念。它允许用户使用一个共享信道进行通信，降低成本，提高利用率。 3.1 频分复用、时分复用和统计时分复用 最基本的复用就是频分复用FDM (Frequency Division Multiplexing)和时分复用时分复用TDM (Time Division Multiplexing)。频分复用最简单 在通信中，复用器与分用器成对地使用。 3.1.1 频分复用 将整个带宽分为多份，用户在分配到一定的频带后，在通信过程中自始至终都占用这个频带。 频分复用的所有用户在同样的时间占用不同的带宽资源（请注意，这里的“带宽”是频率带宽而不是数据的发送速率）。 3.1.2 时分复用 时分复用则是将时间划分为一段段等长的时分复用帧（TDM帧）。每一个时分复用的用户在每一个 TDM 帧中占用固定序号的时隙。 每一个用户所占用的时隙是周期性地出现（其周期就是TDM帧的长度）的。 TDM 信号也称为等时 (isochronous) 信号。 时分复用的所有用户在不同的时间占用同样的频带宽度。 时分复用可能会造成线路资源的浪费 使用时分复用系统传送计算机数据时，由于计算机数据的突发性质，用户对分配到的子信道的利用率一般是不高的。 为了解决时分复用造成线路资源的浪费–统计时分复用 STDM (Statistic TDM) 统计时分复用又叫异步时分复用，普通时分复用又叫同步时分复用。 由于不是固定的分配给客户，因此两个时隙间必须得空出来一个短时隙用来发送地址信息，这是不可避免的开销。 使用统计时分复用的集中器，也叫做智能复用器，他能提供对整个报文的存储转发能力（但大多数的复用器一次只能存储一个字符或比特）。通过排队方式使各用户更合理地共享信道。 TDM帧与STDM帧都是在物理层传送的比特流中所划分的帧。这种帧与数据链路层中的帧不是一个概念 3.2 波分复用 波分复用 WDM(Wavelength Division Multiplexing) 3.3 码分复用 码分复用 CDM (Code Division Multiplexing) 常用的名词是码分多址 CDMA (Code Division Multiple Access)。 各用户使用经过特殊挑选的不同码型，因此彼此不会造成干扰。 这种系统发送的信号有很强的抗干扰能力，其频谱类似于白噪声，不易被敌人发现。 码分复用的工作原理 每一个比特时间划分为 m 个短的间隔，称为码片 (chip)。通常m的值为64或128。(为方便这里假设为8) 每个站被指派一个唯一的 m bit 码片序列。 如发送比特 1，则发送自己的 m bit 码片序列。 如发送比特 0，则发送该码片序列的二进制反码。 例如，S 站的 8 bit 码片序列是 00011011`。 发送比特 1 时，就发送序列 00011011。 发送比特 0 时，就发送序列 11100100。 我们按惯例将码片中的0写成-1，将1写成+1，则S 站的码片序列：(–1 –1 –1 +1 +1 –1 +1 +1) 。 码片序列实现了扩频 假定S站要发送信息的数据率为 b bit/s。由于每一个比特要转换成 m 个比特的码片，因此 S 站实际上发送的数据率提高到 mb bit/s，同时 S 站所占用的频带宽度也提高到原来数值的 m 倍。 这种通信方式是扩频(spread spectrum)通信中的一种。 扩频通信通常有两大类： 一种是``直接序列扩频DSSS` (Direct Sequence Spread Spectrum)，如上面讲的使用码片序列就是这一类。 另一种是``跳频扩频FHSS` (Frequency Hopping Spread Spectrum)。 CDMA 的重要特点 每个站分配的码片序列不仅必须各不相同，并且还必须互相正交 (orthogonal)。 在实用的系统中是使用伪随机码序列。 码片序列的正交关系 令向量 S 表示站 S 的码片向量，令 T 表示其他任何站的码片向量。 两个不同站的码片序列正交，就是向量 S 和T 的规格化内积 (inner product) 等于 0： 正交关系的另一个重要特性 任何一个码片向量和该码片向量自己的规格化内积都是 1 。 ``一个码片向量和该码片反码的向量的规格化内积值是 –1`。 现在假定在一个CDMA系统中有很多站都在互相通信，每一个站所发送的数据比特和本站的码片序列乘积，因而是本站的码片序列(乘积等于1相当于发送比特1)和该码片序列的二进制反码(乘积等于-1相当于发送比特0)的组合序列，或什么也不发送(乘积等于0相当于没有数据发送)。我们还假定所有站所发送的码片序列都是同步的，即所有的码片序列都在同一时刻开始。 CDMA 的工作原理 设S站要发送的数据是001三个码元，再设CDMA将每一个码元扩展为8个码片，而S站选择的码片序列是(-1 -1 -1 +1 +1 -1 +1 +1)。S站发送的扩频信号时Sx。S站发送的扩频信号Sx中只包含互为反码的两种码片序列。T站选择的码片序列是(-1 -1 +1 -1 +1 +1 +1 -1),T站也发送110三个码元，而T站的扩频信号时Tx。因为所有站都使用相同的频率，因此每个站能够收到所有的站发送的扩频信号。对于我们的例子，所有的站收到的都是叠加信号Sx+Tx。 当接收站打算接收s站发送的信号时，就用s站的码片序列与收到的信号求规格化内积。这相当于S·Sx和S·Tx。显然，S·Sx就是S站发送的比特，因为在计算规格化内积时，按第一个公式相加的各项，或者都是+1，或者都是-1(前面说两个不同站的码片序列正交，就是向量 S 和T 的规格化内积 等于 0，这里的到的结果是不为0，所以是同一个站发送)；而S·Tx一定是0，因为相加的8项中的+1和-1各占一半，一次总和是0. 4 数字传输系统 在早期电话网中，从市话局到用户电话机的用户线是采用最廉价的双绞线电缆，而长途干线采用的是频分复用 FDM 的模拟传输方式。 与模拟通信相比，数字通信无论是在传输质量上还是经济上都有明显的优势。 目前，长途干线大都采用时分复用 PCM 的数字传输方式。 脉码调制 PCM 体制最初是为了在电话局之间的中继线上传送多路的电话。 由于历史上的原因，PCM 有两个互不兼容的国际标准： 北美的 24 路 PCM（简称为 T1） 欧洲的 30 路 PCM（简称为 E1） 我国采用的是欧洲的 E1 标准。 E1 的速率是 2.048 Mbit/s，而 T1 的速率是 1.544 Mbit/s。 当需要有更高的数据率时，可采用复用的方法。 旧的数字传输系统存在许多缺点: 最主要的是以下两个方面： 速率标准不统一如果不对高次群的数字传输速率进行标准化，国际范围的基于光纤高速数据传输就很难实现。 不是同步传输 在过去相当长的时间，为了节约经费，各国的数字网主要采用准同步方式。 当数据传输的速率很高时，收发双方的时钟同步就成为很大的问题。 同步光纤网 SONET 同步光纤网 SONET (Synchronous Optical Network) 的各级时钟都来自一个非常精确的主时钟。SONET 为光纤传输系统定义了同步传输的线路速率等级结构 同步数字系列 SDH ITU-T 以美国标准 SONET 为基础，制订出国际标准同步数字系列 SDH (Synchronous Digital Hierarchy)。 SONET的 OC级 / STS级 与SDH的 STM级 的对应关系 SONET / SDH 标准的意义 使不同的数字传输体制在 STM-1 等级上获得了统一。 第一次真正实现了数字传输体制上的世界性标准。 已成为公认的新一代理想的传输网体制。 SDH 标准也适合于微波和卫星传输的技术体制。 5 宽带接入技术 用户要连接到互联网，必须先连接到某个ISP。 在互联网的发展初期，用户都是利用电话的用户线通过调制解调器连接到ISP的，电话用户线接入到互联网的速率最高仅达到56 kbit/s。 美国联邦通信委员会FCC原来认为只要双向速率之和超过200 kbit/s 就是宽带。但 2015 年重新定义为： 宽带下行速率要达到 25 Mbit/s 宽带上行速率要达到 3 Mbit/s 从宽带接入的媒体来看，可以划分为两大类： 有线宽带接入 无线宽带接入 下面讨论有线的宽带接入。 5.1 ADSL 技术 ``非对称数字用户线 ADSL` (Asymmetric Digital Subscriber Line) 技术就是用数字技术对现有的模拟电话用户线进行改造，使它能够承载宽带业务。 标准模拟电话信号的频带被限制在 3003400 Hz 的范围内，但用户线本身实际可通过的信号频率仍然超过 1 MHz。ADSL 技术就把 04 kHz 低端频谱留给传统电话使用，而把原来没有被利用的高端频谱留给用户上网使用。 DSL 就是数字用户线 (Digital Subscriber Line) 的缩写。 DSL 的几种类型 ADSL (Asymmetric Digital Subscriber Line)：非对称数字用户线 HDSL (High speed DSL)：高速数字用户线 SDSL (Single-line DSL)：1 对线的数字用户线 VDSL (Very high speed DSL)：甚高速数字用户线 DSL (Digital Subscriber Line) ：数字用户线 RADSL (Rate-Adaptive DSL)：速率自适应 DSL，是 ADSL 的一个子集，可自动调节线路速率） ADSL 的传输距离 ADSL 的传输距离取决于数据率和用户线的线径（用户线越细，信号传输时的衰减就越大）。 ADSL 所能得到的最高数据传输速率与实际的用户线上的信噪比密切相关。 例如： 0.5 毫米线径的用户线，传输速率为 1.5~2.0 Mbit/s 时可传送5.5公里，但当传输速率提高到 6.1 Mbit/s 时，传输距离就缩短为 3.7 公里。 如果把用户线的线径减小到 0.4 毫米，那么在 6.1 Mbit/s 的传输速率下就只能传送 2.7 公里。 ADSL 的特点 上行和下行带宽做成不对称的。上行指从用户到 ISP，而下行指从 ISP 到用户。 ADSL 在用户线（铜线）的两端各安装一个 ADSL 调制解调器。 我国目前采用的方案是离散多音调 DMT (Discrete Multi-Tone)调制技术。这里的“多音调”就是“多载波”或“多子信道”的意思。 DMT 技术 DMT 调制技术采用频分复用的方法，把 40 kHz 以上一直到 1.1 MHz 的高端频谱划分为许多子信道，其中 25 个子信道用于上行信道，而 249 个子信道用于下行信道。 每个子信道占据 4 kHz 带宽（严格讲是 4.3125 kHz），并使用不同的载波（即不同的音调）进行数字调制。这种做法相当于在一对用户线上使用许多小的调制解调器并行地传送数据。 5.2 光纤同轴混合网（HFC网） HFC (Hybrid Fiber Coax) 网是在目前覆盖面很广的有线电视网 CATV 的基础上开发的一种居民宽带接入网。 HFC 网除可传送 CATV 外，还提供电话、数据和其他宽带交互型业务。 现有的 CATV 网是树形拓扑结构的同轴电缆网络，它采用模拟技术的频分复用对电视节目进行单向传输。 HFC 网对 CATV 网进行了改造。 HFC 网的主干线路采用光纤 HFC 网将原 CATV 网中的同轴电缆主干部分改换为光纤，并使用模拟光纤技术。 在模拟光纤中采用光的振幅调制AM，这比使用数字光纤更为经济。模拟光纤从头端连接到光纤结点 (fiber node)，即光分配结点 ODN (Optical Distribution Node)。在光纤结点光信号被转换为电信号。在光纤结点以下就是同轴电缆。 每个家庭要安装一个用户接口盒 用户接口盒 UIB (User Interface Box) 要提供三种连接，即： 使用同轴电缆连接到机顶盒 (set-top box)，然后再连接到用户的电视机。 使用双绞线连接到用户的电话机。 使用电缆调制解调器连接到用户的计算机。 电缆调制解调器 (Cable Modem) 电缆调制解调器是为 HFC 网而使用的调制解调器。 电缆调制解调器最大的特点就是传输速率高。 下行速率一般在 3 ~ 10 Mbit/s之间，最高可达 30 Mbit/s。 上行速率一般为 0.2 ~ 2 Mbit/s，最高可达 10 Mbit/s。 电缆调制解调器比在普通电话线上使用的调制解调器要复杂得多，并且不是成对使用，而是只安装在用户端。 5.3 FTTx 技术 FTTx 是一种实现宽带居民接入网的方案，代表多种宽带光纤接入方式。 FTTx 表示 Fiber To The…（光纤到…），例如： 光纤到户 FTTH (Fiber To The Home)：光纤一直铺设到用户家庭，可能是居民接入网最后的解决方法。 光纤到大楼 FTTB (Fiber To The Building)：光纤进入大楼后就转换为电信号，然后用电缆或双绞线分配到各用户。 光纤到路边 FTTC (Fiber To The Curb)：光纤铺到路边，从路边到各用户可使用星形结构双绞线作为传输媒体。 一个家庭用户远远用不了一个光纤的通信容量。为了有效的运用光纤资源，在光纤和广大用户之间，还需要铺设一段中间的转换装置即光配线网(ODN),使多个用户共享一根光纤干线。“无光源”表明在光配线网中无需配备电源，因此基本不用维护，其长期运营成本和管理成本都很低。","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://www.studyz.club/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://www.studyz.club/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"物理层","slug":"物理层","permalink":"http://www.studyz.club/tags/%E7%89%A9%E7%90%86%E5%B1%82/"}]},{"title":"计算机网络······概述","slug":"计算机网络","date":"2019-06-19T12:27:15.568Z","updated":"2019-06-22T15:35:46.990Z","comments":true,"path":"posts/e7e6b974/","link":"","permalink":"http://www.studyz.club/posts/e7e6b974/","excerpt":"","text":"1.1 计算机网络1.2 互联网的核心部分 1.1 计算机网络① 21世纪的重要特征: 数字化，网络化,信息化。是一个以网络为核心的时代② 网络分为 电信网络，有线电视网络，计算机网络。发展最快的并起到核心作用的是计算机网络。③ 三网融合 ④ 互联网具有的两个重要的基本特点:连通性,共享。⑤ 概述局域网: 覆盖范围小，自己购买设备，带宽固定，自己维护(100米以内，带宽百兆，千兆)广域网: 距离较远，花钱买带宽，运营商进行维护 Internet（互联网）: ISP（Internet Service Provider互联网服务提供者即运营商）有自己的机房，对网民提供链接互联网发展的第三个阶段的特点:形成了多层次的ISP结构的互联网 网络把许多计算机连接在一起，而互连网则把许多网络通过路由器连接在一起。与网络相连的计算机常称为主机(host) ⑥ 互联网发展的第三个阶段互联网发展的第三个阶段的特点:形成了多层次的ISP结构的互联网任何机构和个人只要向某个 ISP 交纳规定的费用，就可从该 ISP 获取所需 IP 地址的使用权，并可通过该 ISP 接入到互联网 互联网交换点:IXP(Internet eXchange Point)主要作用是允许两个网络直接相连并交换分组，而不需要通过再通过第三个网络来转发分组，这样减少了分组转发的延迟时间，降低了分组转发的费用。IXP在全球分布是不均匀的。 ⑦ 互联网的组成从互联网的工作方式上看，可以划分为两大块： 边缘部分： 由所有连接在互联网上的主机组成。这部分是用户直接使用的，用来进行通信（传送数据、音频或视频）和资源共享。 核心部分：由大量网络和连接这些网络的路由器组成。这部分是为边缘部分提供服务的（提供连通性和交换）。 处在互联网边缘的部分就是连接在互联网上的所有的主机。这些主机又称为端系统 (end system)。端系统在功能上可能有很大的差别： 小的端系统可以是一台普通个人电脑，具有上网功能的智能手机，甚至是一个很小的网络摄像头。 大的端系统则可以是一台非常昂贵的大型计算机。 端系统的拥有者可以是个人，也可以是单位（如学校、企业、政府机关等），当然也可以是某个 ISP。 端系统之间通信的含义 端系统之间的两种通信方式1. 客户——服务器方式（C/S方式）即 Client/Server 方式，简称为 C/S 方式。服务请求方和服务提供方都要使用网络核心部分所提供的服务* 客户 (client) 和服务器 (server) 都是指通信中所涉及的两个应用进程。 * 客户——服务器方式所描述的是进程之间服务和被服务的关系。 * 客户是服务的请求方，服务器是服务的提供方。 客户: 被用户调用后运行，在打算通信时主动向远地服务器发起通信（请求服务）。因此，客户程序必须知道服务器程序的地址。 不需要特殊的硬件和很复杂的操作系统。 服务器: 一种专门用来提供某种服务的程序，可同时处理多个远地或本地客户的请求。 系统启动后即自动调用并一直不断地运行着，被动地等待并接受来自各地的客户的通信请求。因此，服务器程序不需要知道客户程序的地址。 一般需要强大的硬件和高级的操作系统支持。 客户与服务器的通信关系建立后，通信可以是双向的，客户和服务器都可发送和接收数据。 2.对等方式（P2P方式）即 Peer—to—Peer 方式 ，简称为 P2P 方式。对等连接工作方式可支持大量对等用户（如上百万个）同时工作。 对等连接 (peer-to-peer，简写为 P2P ) 是指两个主机在通信时并不区分哪一个是服务请求方还是服务提供方。 只要两个主机都运行了对等连接软件 ( P2P 软件) ，它们就可以进行平等的、对等连接通信。双方都可以下载对方已经存储在硬盘中的共享文档。 对等连接方式从本质上看仍然是使用客户服务器方式，只是对等连接中的每一个主机既是客户又是服务器。 例如主机 C 请求 D 的服务时，C 是客户，D 是服务器。但如果 C 又同时向 F提供服务，那么 C 又同时起着服务器的作用。 1.2 互联网的核心部分 网络核心部分是互联网中最复杂的部分。网络中的核心部分要向网络边缘中的大量主机提供连通性，使边缘部分中的任何一个主机都能够向其他主机通信（即传送或接收各种形式的数据）。在网络核心部分起特殊作用的是路由器 (router)。路由器是实现分组交换 (packet switching) 的关键构件，其任务是转发收到的分组，这是网络核心部分最重要的功能。 三种交换方式:电路交换，报文交换，分组交换。互联网的核心部分采用了分组交换技术。1.2.1 电路交换!电路交换](计算机网络/dianlv.png “电路交换”) 电路交换的特点: 电路交换必定是面向连接的。 电路交换分为三个阶段： 建立连接：建立一条专用的物理通路，以保证双方通话时所需的通信资源在通信时不会被其他用户占用； 通信：主叫和被叫双方就能互相通电话； 释放连接：释放刚才使用的这条专用的物理通路（释放刚才占用的所有通信资源）。 电路交换的缺点: 计算机数据具有突发性。 这导致在传送计算机数据时，通信线路的利用率很低（用来传送数据的时间往往不到 10% 甚至不到 1% ）。 1.2.2 分组交换 通常我们把要发送的整块数据称为一个报文(message)。在发送报文之前，先把较长的一段报文划分成为一个个更小的等长数据段，例如，每个数据段为1024bit①。在每一个数据段前面，加上一些由必要的控制信息组成的首部(header)后，就构成了一个分组(packet)。分组又称为‘包’，而分组的首部也可以称为‘包头’。分组是在互联网中传送数据的单元。 ①bit译为‘比特’，在计算机中也译为‘位’，在很多时候两者译名是可以通用的。bit在表示信息量或信息传输速率的时候不能译为位。 分组交换的特点 分组交换则采用存储转发技术。 在发送端，先把较长的报文划分成较短的、固定长度的数据段。 分组交换网以“分组”作为数据传输单元。 依次把各分组发送到接收端（假定接收端在左边）。 接收端收到分组后剥去首部还原成原来的报文。 分组交换的传输过程 每一个分组的首部都含有地址（诸如目的地址和源地址）等控制信息。 分组交换网中的结点交换机根据收到的分组首部中的地址信息，把分组转发到下一个结点交换机。 每个分组在互联网中独立地选择传输路径。 用这样的存储转发方式，最后分组就能到达最终目的地。 主机和路由器的区别 位于网络边缘的主机和位于网络核心的部分的路由器都是计算机，但他们的作用却不同。主机是为用户进行信息处理的，并且可以和其他主机通过网络交换信息。路由器则是用来转发数分组的，即进行分组交换。 路由器 在路由器中的输入和输出端口之间没有直接连线。 路由器处理分组的过程是： 把收到的分组先放入缓存（暂时存储）； 查找转发表，找出到某个目的地址应从哪个端口转发； 把分组送到适当的端口转发出去。 分组交换的优缺点 优点 所采用的手段 高效 在分组传输的过程中动态分配传输带宽，对通信链路是逐段占用。 灵活 为每一个分组独立地选择最合适的转发路由。 迅速 以分组作为传送单位，可以不先建立连接就能向其他主机发送分组。 可靠 保证可靠性的网络协议；分布式多路由的分组交换网，使网络有很好的生存性。 缺点 问题 分组在各结点存储转发时需要排队，这就会造成一定的时延。 分组必须携带的首部（里面有必不可少的控制信息）也造成了一定的开销 1.2.3 报文交换(message switching)报文交换的时延较大现已不用。 1.2.4 三种交换方式的比较 若要连续传送大量的数据，且其传送时间远大于连接建立时间，则电路交换的传输速率较快。 报文交换和分组交换不需要预先分配传输带宽，在传送突发数据时可提高整个网络的信道利用率。 由于一个分组的长度往往远小于整个报文的长度，因此分组交换比报文交换的时延小，同时也具有更好的灵活性。 1.3.1 计算机网络的定义 较好的定义：计算机网络主要是由一些通用的、可编程的硬件互连而成的，而这些硬件并非专门用来实现某一特定目的（例如，传送数据或视频信号）。这些可编程的硬件能够用来传送多种不同类型的数据，并能支持广泛的和日益增长的应用。 根据这个定义：计算机网络所连接的硬件，并不限于一般的计算机，而是包括了智能手机等。计算机网络并非专门用来传送数据，而是能够支持很多种的应用（包括今后可能出现的各种应用）。上述的“可编程的硬件”表明这种硬件一定包含有中央处理机 (CPU)。 1.3.2 按照网络的作用范围进行分类 广域网 WAN (Wide Area Network)：作用范围通常为几十到几千公里。 城域网 MAN (Metropolitan Area Network)：作用距离约为 5~50 公里。 局域网 LAN (Local Area Network) ：局限在较小的范围（如 1 公里左右）。 个人区域网 PAN (Personal Area Network) ：范围很小，大约在 10 米左右。 若中央处理机之间的距离非常近（如仅 1 米的数量级甚至更小些），则一般就称之为多处理机系统，而不称它为计算机网络。 1.3.3 按照网络的使用者进行分类 公用网 (public network) 按规定交纳费用的人都可以使用的网络。因此也可称为公众网。 专用网 (private network) 为特殊业务工作的需要而建造的网络。 公用网和专用网都可以提供多种服务。如传送的是计算机数据，则分别是公用计算机网络和专用计算机网络。 1.3.4 用来把用户接入到互联网的网络 接入网 AN (Access Network)，它又称为本地接入网或居民接入网。 接入网是一类比较特殊的计算机网络，用于将用户接入互联网。 接入网本身既不属于互联网的核心部分，也不属于互联网的边缘部分。 接入网是从某个用户端系统到互联网中的第一个路由器（也称为边缘路由器）之间的一种网络。 从覆盖的范围看，很多接入网还是属于局域网。 从作用上看，接入网只是起到让用户能够与互联网连接的“桥梁”作用。 1.4.1 计算机网络的性能指标速率 比特（bit）来源于 binary digit，意思是一个“二进制数字”，因此一个比特就是二进制数字中的一个 1 或 0。 速率是计算机网络中最重要的一个性能指标，指的是数据的传送速率，它也称为数据率 (data rate) 或比特率 (bit rate) 速率的单位是 bit/s，或 kbit/s、Mbit/s、 Gbit/s等。例如4  1010 bit/s 的数据率就记为 40 Gbit/s。 速率往往是指额定速率或标称速率，非实际运行速率。因为是按字节算的，所以除以8是实际速率。 带宽两种不同意义： “带宽”(bandwidth) 本来是指信号具有的频带宽度，其单位是赫（或千赫、兆赫、吉赫等）。 在计算机网络中，带宽用来表示网络中某通道传送数据的能力。表示在单位时间内网络中的某信道所能通过的“最高数据率”。单位是 bit/s，即 “比特每秒”。 在“带宽”的上述两种表述中，前者为频域称谓，而后者为时域称谓，其本质是相同的。也就是说，一条通信链路的“带宽”越宽，其所能传输的“最高数据率”也越高。 吞吐量 吞吐量 (throughput) 表示在单位时间内通过某个网络（或信道、接口）的数据量。 吞吐量更经常地用于对现实世界中的网络的一种测量，以便知道实际上到底有多少数据量能够通过网络。 吞吐量受网络的带宽或网络的额定速率的限制。 时延 时延 (delay 或 latency) 是指数据（一个报文或分组，甚至比特）从网络（或链路）的一端传送到另一端所需的时间。 有时也称为延迟或迟延。 网络中的时延由以下几个不同的部分组成： 发送时延 也称为传输时延。发送数据时，数据帧从结点进入到传输媒体所需要的时间。也就是从发送数据帧的第一个比特算起，到该帧的最后一个比特发送完毕所需的时间。 传播时延 电磁波在信道中需要传播一定的距离而花费的时间。 发送时延与传播时延有本质上的不同。 信号发送速率和信号在信道上的传播速率是完全不同的概念。 处理时延 主机或路由器在收到分组时，为处理分组（例如分析首部、提取数据、差错检验或查找路由）所花费的时间。 排队时延 分组在路由器输入输出队列中排队等待处理所经历的时延。 排队时延的长短往往取决于网络中当时的通信量。 总时延 = 发送时延+ 传播时延+ 处理时延+ 排队时延 时延带宽积 链路的时延带宽积又称为以比特为单位的链路长度。 往返时间 互联网上的信息不仅仅单方向传输，而是双向交互的。因此，有时很需要知道双向交互一次所需的时间。 往返时间 RTT (round-trip time) 表示从发送方发送数据开始，到发送方收到来自接收方的确认，总共经历的时间。 在互联网中，往返时间还包括各中间结点的处理时延、排队时延以及转发数据时的发送时延。 当使用卫星通信时，往返时间 RTT 相对较长，是很重要的一个性能指标。 利用率 分为信道利用率和网络利用率。 信道利用率=有数据通过的时间/(有+无)数据通过的时间。 完全空闲的信道的利用率是零。 网络利用率则是全网络的信道利用率的加权平均值。 信道利用率并非越高越好。当某信道的利用率增大时，该信道引起的时延也就迅速增加。(这也是为什么运营商标的速率要比实际速率大的原因之一) 时延与利用率之间的关系 根据排队论的理论，当某信道的利用率增大时，该信道引起的时延也就迅速增加。 若令 D0 表示网络空闲时的时延，D 表示网络当前的时延，则在适当的假定条件下，可以用下面的简单公式表示 D 和 D0 之间的关系：其中：U 是网络的利用率，数值在 0 到 1 之间。 1.4.2 计算机网络的非性能指标主要包括:费用,质量,标准化,可靠性,可扩展性和可升级性 ,易于管理和维护。 1.5.1 计算机网络的体系结构 计算机网络是个非常复杂的系统。相互通信的两个计算机系统必须高度协调工作才行，而这种“协调”是相当复杂的。分层可将庞大而复杂的问题，转化为若干较小的局部问题，而这些较小的局部问题就比较易于研究和处理。 为了使不同体系结构的计算机网络都能互连，国际标准化组织 ISO 于 1977 年成立了专门机构研究该问题。他们提出了一个试图使各种计算机在世界范围内互连成网的标准框架，即著名的开放``系统互连基本参考模型 OSI/RM(Open Systems Interconnection Reference Model)，简称为OSI`。 OSI 只获得了一些理论研究的成果，在市场化方面却失败了。原因包括： OSI 的专家们在完成 OSI 标准时没有商业驱动力； OSI 的协议实现起来过分复杂，且运行效率很低； OSI 标准的制定周期太长，因而使得按 OSI 标准生产的设备无法及时进入市场； OSI 的层次划分也不太合理，有些功能在多个层次中重复出现。 法律上的 (de jure) 国际标准 OSI 并没有得到市场的认可。非国际标准 TCP/IP 却获得了最广泛的应用。TCP/IP 常被称为事实上的 (de facto) 国际标准。 1.5.2 协议与划分层次 网络协议 (network protocol)，简称为协议，是为进行网络中的数据交换而建立的规则、标准或约定。 计算机网络中的数据交换必须遵守事先约定好的规则。 这些规则明确规定了所交换的数据的格式以及有关的同步问题（同步含有时序的意思）。 网络协议的三个组成要素 语法：数据与控制信息的结构或格式 。 语义：需要发出何种控制信息，完成何种动作以及做出何种响应。 同步：事件实现顺序的详细说明。 由此可见，网络协议是计算机网络的不可缺少的组成部分。 协议的两种形式 一种是使用便于人来阅读和理解的文字描述。 另一种是使用让计算机能够理解的程序代码。 这两种不同形式的协议都必须能够对网络上信息交换过程做出精确的解释。 对于非常复杂的计算机网络协议，其结构应该是层次式的。 分层的好处与缺点好处: 各层之间是独立的。 灵活性好。 结构上可分割开。 易于实现和维护。 能促进标准化工作。 坏处: 降低效率。 有些功能会在不同的层次中重复出现，因而产生了额外开销。 层数太少，就会使每一层的协议太复杂。 层数太多，又会在描述和综合各层功能的系统工程任务时遇到较多的困难。 各层之间需要实现的功能 差错控制：使相应层次对等方的通信更加可靠。 流量控制：发送端的发送速率必须使接收端来得及接收，不要太快。 分段和重装：发送端将要发送的数据块划分为更小的单位，在接收端将其还原。 复用和分用：发送端几个高层会话复用一条低层的连接，在接收端再进行分用。 连接建立和释放：交换数据前先建立一条逻辑连接，数据传送结束后释放连接。 计算机网络的体系结构 计算机网络的体系结构 (architecture) 是计算机网络的各层及其协议的集合。 体系结构就是这个计算机网络及其部件所应完成的功能的精确定义。 实现 (implementation) 是遵循这种体系结构的前提下用何种硬件或软件完成这些功能的问题。 体系结构是抽象的，而实现则是具体的，是真正在运行的计算机硬件和软件。 OSI 的七层协议体系结构的概念清楚，理论也较完整，但它既复杂又不实用。 TCP/IP 是四层体系结构：应用层、运输层、网际层和网络接口层。但最下面的网络接口层并没有具体内容。 因此往往采取折中的办法，即综合 OSI 和 TCP/IP 的优点，采用一种只有五层协议的体系结构 。 1.5.3 五层协议的体系结构 应用层 (application layer) 体系结构的最高层。 任务是通过应用进程间的交互来完成特定网络应用。 应用层协议定义的是应用进程间通信和交互的规则。这里进程就是指主机中正在运行的程序。应用层的交互的数据单元称为报文(message)。 运输层 (transport layer) 运输层的任务是负责向两台主机中进程之间的通信提供通用的数据传输服务。(通用指多种应用可以使用同一个运输层服务) 应用层程序利用该服务传送应用层报文。 由于一台主机可以同时运行多个进程，因此运输层具有复用和分用的功能 运输层主要有一下两种协议: 传输控制协议 TCP(Transmisson Control Protocol)–&gt;提供面向连接的，可靠的数据传输服务，其数据传输最大的单位是报文段(message)。 用户数据报协议 UDP(User Datagram Protocol)–&gt;提供无连接的，尽最大努力(best-effort)的数据传输服务(不保证数据传输的可靠性，及不必考虑目标能否接受到等)，其数据传输的单位是用户数据报。 网络层 (network layer) 网络层负责为分组交换网上的不同主机提供通信服务。在发送数据时，网络层把运输层产生的报文或用户数据报封装成分组或包进行传送。 在TCP/IP体系结构中，由于网络层使用IP协议，因此分组也叫做IP数据报，或简称数据报。 运输层的UDP数据报和网络层的IP数据报不同，无论在哪一层传送的数据单元，都可以笼统的用分组来表示 互联网使用的网络层协议是无连接的网际协议 IP(Internet Protocol)和许多路由选择协议。因此网络层也叫网际层或IP层 数据链路层 (data link layer) 数据链路层将从网络层交下来的IP数据报组装成帧(framing),在两个相邻节点的链路上传送帧。每一帧包含数据和必要的控制信息（如同步信息，地址信息，差错控制等） 在接收数据时，控制信息能让接收端知道一个帧从哪个bit开始到那个比特结束，这样数据链路层在接收到一个后就可以提取出数据部分，上交给网络层。 控制信息还使接收端能够检测到所收到的数据有无差错，如果发现差错，链路层就会丢弃这个帧，避免浪费网络资源。 物理层 (physical layer) 物理层所传数据的单位是比特。 规定了接口标准，电器标准(用几伏电压代表0和1)，如何在物理线路上传输的更快。 数据在五层协议中的传输过程主机一向主机二发送数据: OSI 参考模型把对等层次之间传送的数据单位称为该层的协议数据单元 PDU (Protocol Data Unit)。这个名词现已被许多非 OSI 标准采用。 任何两个同样的层次把数据（即数据单元加上控制信息）通过水平虚线直接传递给对方。这就是所谓的“对等层”(peer layers)之间的通信。 各层协议实际上就是在各个对等层之间传递数据时的各项规定。 1.5.4 实体、协议、服务和服务访问点 实体 (entity) 表示任何可发送或接收信息的硬件或软件进程。 协议是控制两个对等实体进行通信的规则的集合。 在协议的控制下，两个对等实体间的通信使得本层能够向上一层提供服务。要实现本层协议，还需要使用下层所提供的服务。 协议与服务的区别 协议的实现保证了能够向上一层提供服务。 本层的服务用户只能看见服务而无法看见下面的协议。即下面的协议对上面的服务用户是透明的。 协议是“水平的”，即协议是控制对等实体之间通信的规则。 服务是“垂直的”，即服务是由下层向上层通过层间接口提供的。 上层使用服务原语获得下层所提供的服务。 1.5.5 TCP/IP 的体系结构 在互联网使用的各种协议中，最重要的和最知名的就是TCP和IP协议，现在人们经常提到的TCP/IP并不一定是单指TCP和IP这两个具体的协议。而往往是表示互联网所使用的的整个TCP/IP 协议簇(protocol suite) TCP/IP协议可以为各式各样的应用提供服务(所谓的everything over IP)，同时TCP/IP协议也允许IP协议在各式各样的网络构成的互联网上运行(所谓的IP over everything) 在应用层的客户进程和服务器进程交互 主机C的两个服务器进程分别向A和B的客户进程提供服务 1.5.6 数据的封装过程","categories":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://www.studyz.club/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://www.studyz.club/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"hadoop-mapreduce-源码分析","slug":"hadoop-mapreduce-源码分析","date":"2019-06-07T06:40:49.380Z","updated":"2019-06-19T12:14:26.355Z","comments":true,"path":"posts/b072ecf/","link":"","permalink":"http://www.studyz.club/posts/b072ecf/","excerpt":"","text":"通过分析之前手写的Wordcount方法了解mr运行的过程。 一, 客户端client的源码分析 一, 客户端client的源码分析① job.waitForCompletion1job.waitForCompletion(true); waitForCompletion: submit:submitJobInternal: writeSplits: writeNewSplits: getInputFormatClass:Ctrl键–&gt;Open Implementation发现没有子类，看他的父类JobContextImplJobContextImpl： TextInputFormat.class: Ctrl+t","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/categories/hadoop/"},{"name":"mapreduce","slug":"hadoop/mapreduce","permalink":"http://www.studyz.club/categories/hadoop/mapreduce/"}],"tags":[{"name":"mapreduce","slug":"mapreduce","permalink":"http://www.studyz.club/tags/mapreduce/"}]},{"title":"MapReduce实例-wordcount","slug":"MapReduce实例-wordcount","date":"2019-06-05T03:01:34.700Z","updated":"2019-06-19T07:07:58.112Z","comments":true,"path":"posts/409713d5/","link":"","permalink":"http://www.studyz.club/posts/409713d5/","excerpt":"","text":"一，hadoop自带实例worldcount二,编程实现wordcount ① 实现原理 ② 注意事项 ③ 源码 ④ 制作jar包 ⑤ 命令一，hadoop自带实例worldcountjar包:在这个目录下有计算需要用到jar包12cd &#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F; 命令:12hadoop jar &#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-2.7.3.jar wordcount &#x2F;usr&#x2F;root&#x2F;helloworld.txt &#x2F;data&#x2F;wc&#x2F;output hadoop jar是用来做classpath的也就是说让计算机知道jar包在哪。/usr/root/helloworld.txt是输入目录之前上传的/data/wc/output是输出目录注意这个目录必须是不存在的或者为空目录，否则程序会暂停。 运行结果: 12345[root@master ~]# hdfs dfs -ls &#x2F;data&#x2F;wc&#x2F;outputFound 2 items-rw-r--r-- 3 root supergroup 0 2019-06-05 13:29 &#x2F;data&#x2F;wc&#x2F;output&#x2F;_SUCCESS-rw-r--r-- 3 root supergroup 40 2019-06-05 13:29 &#x2F;data&#x2F;wc&#x2F;output&#x2F;part-r-00000 第一行为标志文件大小为0，后面SUCCESS表示执行成公。第二行为执行的最终结果。命名规则: part-r-00000 r–&gt;代表Reduce，m代表map输出也可以只生成map文件。 00000–&gt;o号Reduce，如果有多个Reduce就会出现00001,000002等 二,编程实现wordcount① 实现原理123456789输入：hello worldhello world 。。。。。。很多行输出：Hello 110817World 110817WorldHello 62（hello 110817个 world 110817个 WorldHello 62个） Map过程示意图: 用户没有定义Combiner时的Reduce过程示意图: 用户有定义Combiner时的Reduce过程示意图: ② 注意事项要注意eclipse的jre版本要不然制成jar包运行或者在eclipse里运行都会报错(hadoop项目上有个黄色的感叹号)123456789101112131415161718192021222324[root@master usr]# hadoop jar &#x2F;usr&#x2F;MyWorldcount.jar MyWorldcount.MyWordcount&#39;&#39;19&#x2F;06&#x2F;07 15:57:02 INFO client.ConfiguredRMFailoverProxyProvider: Failing over to rm219&#x2F;06&#x2F;07 15:57:03 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.19&#x2F;06&#x2F;07 15:57:03 INFO mapreduce.JobSubmitter: Cleaning up the staging area &#x2F;tmp&#x2F;hadoop-yarn&#x2F;staging&#x2F;root&#x2F;.staging&#x2F;job_1559890033713_0006Exception in thread &quot;main&quot; java.io.IOException: No input paths specified in job at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:239) at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:387) at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:301) at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:318) at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:196) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.hadoop.mapreduce.Job.submit(Job.java:1287) at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1308) at MyWorldcount.MyWordcount.main(MyWordcount.java:50) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136) 解决办法:1Window--&gt;Preferences--&gt;java--&gt;Installed JRES--&gt;Execution Environments选择与自己虚拟机里配置相同版本的jre ③ 源码 MyWordcount.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package MyWorldcount;import java.io.IOException;import java.io.ObjectOutput;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapred.lib.db.DBInputFormat;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class MyWordcount &#123; public static void main(String[] args) throws Exception &#123; &#x2F;&#x2F;读取配置信息 Configuration conf &#x3D; new Configuration(true); &#x2F;&#x2F;HDFS中客户端抽象成了ifsystem，在MR中抽象成了job &#x2F;&#x2F;在job上Ctrl可以看源码，鼠标放在Job上可以获得帮助，下面的代码复制来的-_- Job job &#x3D; Job.getInstance(conf); &#x2F;&#x2F; Create a new Job &#x2F;&#x2F;Job job &#x3D; Job.getInstance(); job.setJarByClass(MyWordcount.class); &#x2F;&#x2F; Specify various job-specific parameters job.setJobName(&quot;ocxx&quot;); &#x2F;* job.setInputPath(new Path(&quot;in&quot;)); job.setOutputPath(new Path(&quot;out&quot;)); *&#x2F; Path input &#x3D; new Path(&quot;&#x2F;usr&#x2F;helloworld.txt&quot;); FileInputFormat.addInputPath(job, input); Path output &#x3D; new Path(&quot;&#x2F;usr&#x2F;wd&#x2F;mywc&quot;); if (output.getFileSystem(conf).exists(output)) &#123; output.getFileSystem(conf).delete(output, true); &#125; FileOutputFormat.setOutputPath(job, output); job.setMapperClass(MyMapper.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setReducerClass(MyReducer.class); &#x2F;&#x2F; Submit the job, then poll for progress until the job is complete job.waitForCompletion(true); &#125;&#125; MyReducer:1234567891011121314151617181920212223242526package MyWorldcount;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;public class MyReducer extends Reducer&lt;Text,IntWritable, Text, IntWritable&gt;&#123; &#x2F;&#x2F;相同的Key为一组。。调用一次reduce方法，在方法内迭代这一组数据，进行计算:sum count max min ...... private IntWritable result &#x3D; new IntWritable(); public void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; int sum &#x3D; 0; for (IntWritable val : values) &#123; sum +&#x3D; val.get(); &#125; result.set(sum); context.write(key, result); &#125;&#125; MyMapper:1234567891011121314151617181920212223242526package MyWorldcount;import java.io.IOException;import java.util.StringTokenizer;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;&#x2F;&#x2F;mapreduce不能使用基本数据类型public class MyMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;&#123; private final static IntWritable one &#x3D; new IntWritable(1); private Text word &#x3D; new Text(); &#x2F;&#x2F;key放的偏移量，value放的是那一行字符串 public void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123; StringTokenizer itr &#x3D; new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) &#123; word.set(itr.nextToken()); context.write(word, one); &#125; &#125;&#125; ④ 制作jar包1项目上右键--&gt;Export--&gt;Java--&gt;JAR file--&gt;next这里只要选MyWorldcount包里的三个java文件就可以了。 ⑤ 命令12[root@master usr]# hadoop jar &#x2F;usr&#x2F;MyWorldcount.jar MyWorldcount.MyWordcount /usr/MyWorldcount.jar –&gt;是本地文件的jar包位置 MyWorldcount.MyWordcount –&gt;MyWorldcount是包名 MyWordcount是类名 由于这里在程序里将输入文件和输出文件写死了所以并不需要写输入输出文件路径","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/categories/hadoop/"},{"name":"mapreduce","slug":"hadoop/mapreduce","permalink":"http://www.studyz.club/categories/hadoop/mapreduce/"}],"tags":[{"name":"mapreduce","slug":"mapreduce","permalink":"http://www.studyz.club/tags/mapreduce/"},{"name":"wordcount","slug":"wordcount","permalink":"http://www.studyz.club/tags/wordcount/"}]},{"title":"win7-eclipse-hdfs部署","slug":"win7-eclipse-hdfs","date":"2019-06-03T05:10:28.557Z","updated":"2019-06-19T07:09:42.039Z","comments":true,"path":"posts/52c052c8/","link":"","permalink":"http://www.studyz.club/posts/52c052c8/","excerpt":"","text":"一，安装包,环境配置 win7-Hadoop 配置系统变量 eclipse链接HDFS eclipse做hadoop的User Libraries二，有在不同集群编译的需求 先将集群中的hdfs-site.xml和core-site.xml复制到本地 将配置文件放到项目中三，注意事项 一，安装包,环境配置① win7-Hadoop将下载的hadoop安装包及源码包解压并创建一个hadoop-lib的目录:将这个文件夹里的内容复制到hadoop-lib中: 1D:\\hadoop\\hadoop-2.7.3\\share\\hadoop 将：hadoop.dll文件 1D:\\hadoop\\hadoop-2.7.3\\bin 放到: 1C:\\Windows\\System32 ② 配置系统变量 ③ eclipse链接HDFS ④ eclipse做hadoop的User Libraries先导入libs:然后新建一个java project:在java project右键–&gt;Build Path–&gt;Add Libraries–&gt;User Libraries选中hadoop2.7.3就OK了另外再用同样的方法导入一个JUnit单元测试 二，有在不同集群编译的需求① 先将集群中的hdfs-site.xml和core-site.xml复制到本地然后拷贝到文件目录: 将配置文件放到项目中 这样编译的时候配置文件也会被编译了此时就可以在src目录中创建java文件了 三，注意事项在执行如下代码创建目录时: 12345678910111213141516171819202122232425262728293031323334package hadoop;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.junit.After;import org.junit.Before;public class Test &#123; Configuration conf; &#x2F;&#x2F; FileSystem fs;&#x2F;&#x2F;准备客户端 用这个类表示 @Before public void conn() throws Exception &#123; &#x2F;&#x2F;准备连接 conf &#x3D; new Configuration(true);&#x2F;&#x2F;为true表示读导入配置文件hdfs-site.xml fs &#x3D; FileSystem.get(conf); &#125; &#x2F;&#x2F; 关闭 @After public void close() throws Exception &#123; fs.close(); &#125; &#x2F;&#x2F;创建目录 @org.junit.Test public void mkdir() throws Exception &#123; Path ifile &#x3D; new Path(&quot;&#x2F;ooxx&quot;); if (fs.exists(ifile)) &#123;&#x2F;&#x2F;判断这个文件是否存在，如果存在删掉重新创建 fs.delete(ifile, true);&#x2F;&#x2F;递归删除 &#125; fs.mkdirs(ifile);&#x2F;&#x2F;创建一个名为ooxx的文件 &#125;&#125; 报错: 1232019-06-03 23:38:33,480 WARN hdfs.DFSUtil (DFSUtil.java:getAddressesForNameserviceId(688)) - Namenode for mycluster remains unresolved for ID nn1. Check your hdfs-site.xml file to ensure namenodes are configured properly.2019-06-03 23:38:45,054 WARN hdfs.DFSUtil (DFSUtil.java:getAddressesForNameserviceId(688)) - Namenode for mycluster remains unresolved for ID nn2. Check your hdfs-site.xml file to ensure namenodes are configured properly. 解决办法：将映射写入Windows中，eclipse链接的时候将IP地址改为主机名。原因: 之前配置hdfs-site.xml时写的是master，在win上拿IP地址去访问会出问题。 1 hadoop项目的文件夹同级目录下会出现\\usr\\local\\hadoop-2.7.3\\ha 的文件夹存放mr运行日志 目录与配置有关 1234C:\\Windows\\System32\\drivers\\etc\\hosts 1234192.168.43.159 master192.168.43.160 slave1192.168.43.161 slave2192.168.43.162 slave3","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/categories/hadoop/"},{"name":"eclipse","slug":"hadoop/eclipse","permalink":"http://www.studyz.club/categories/hadoop/eclipse/"}],"tags":[{"name":"hdfs","slug":"hdfs","permalink":"http://www.studyz.club/tags/hdfs/"},{"name":"win7","slug":"win7","permalink":"http://www.studyz.club/tags/win7/"},{"name":"eclipse","slug":"eclipse","permalink":"http://www.studyz.club/tags/eclipse/"}]},{"title":"Hadoop-HDFS-API","slug":"Hadoop-HDFS-API","date":"2019-06-03T01:27:48.002Z","updated":"2019-06-19T07:03:54.590Z","comments":true,"path":"posts/6b11fb4d/","link":"","permalink":"http://www.studyz.club/posts/6b11fb4d/","excerpt":"","text":"一，创建目录二，hdfs命令行命令三，HDFS分布式计算分析 一，创建目录1234567891011121314151617181920212223242526272829303132333435363738394041424344package hadoop;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.junit.After;import org.junit.Before;public class Test &#123; Configuration conf; &#x2F;&#x2F; FileSystem fs;&#x2F;&#x2F;准备客户端 用这个类表示 @Before public void conn() throws Exception &#123; &#x2F;&#x2F;准备连接 conf &#x3D; new Configuration(true);&#x2F;&#x2F;为true表示读导入配置文件hdfs-site.xml fs &#x3D; FileSystem.get(conf); &#125; &#x2F;&#x2F; 关闭 @After public void close() throws Exception &#123; fs.close(); &#125; &#x2F;&#x2F;创建目录 @org.junit.Test public void mkdir() throws Exception &#123; Path ifile &#x3D; new Path(&quot;&#x2F;ooxx&quot;); if (fs.exists(ifile)) &#123;&#x2F;&#x2F;判断这个文件是否存在，如果存在删掉重新创建 fs.delete(ifile, true);&#x2F;&#x2F;递归删除 &#125; fs.mkdirs(ifile);&#x2F;&#x2F;创建一个名为ooxx的文件 &#125;&#125;&#96;&#96;&#96; 上传文件局部代码: 将nginx文件写进tt.txt //上传文件 @org.junit.Test public void upload() throws Exception { Path fPath = new Path(“/ooxx/tt.txt”); FSDataOutputStream outputStream = fs.create(fPath); InputStream inputStream = new BufferedInputStream(new FileInputStream(new File(&quot;D:\\\\用户目录\\\\大数据\\\\尚学堂笔记\\\\nginx\\\\01资料\\\\nginx&quot;))); IOUtils.copyBytes(inputStream, outputStream, conf,true); &#125; 12HDFS文件块信息: // HDFS的块信息 @org.junit.Test public void biks() throws Exception { Path i = new Path(&quot;/usr/root/test.txt&quot;); FileStatus iStatus =fs.getFileStatus(i); BlockLocation[] blksBlockLocations= fs.getFileBlockLocations(iStatus, 0, iStatus.getLen()); //取块的位置 for (BlockLocation b : blksBlockLocations) &#123; System.out.println(b); &#125; &#125; 1上传文件命令:(这里我上传的时候没有设置块大小1048576字节，所以第一个块的大小是134217728) hdfs dfs -D dfs.blocksize=1048576 -put /opt/jdk-8u91-linux-x64.rpm /usr/root 1运行结果: 0,134217728,slave2,slave1,slave3134217728,25944853,slave2,slave3,slave1 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354![块信息](Hadoop-HDFS-API&#x2F;kuai.png)### &lt;h id&#x3D;&quot;2&quot;&gt;二，hdfs命令行命令&lt;&#x2F;h&gt;（1）查看帮助 hdfs dfs -help （2）查看当前目录信息 hdfs dfs -ls &#x2F; （3）上传文件 hdfs dfs -put &#x2F;本地路径 &#x2F;hdfs路径 （4）剪切文件 hdfs dfs -moveFromLocal a.txt &#x2F;aa.txt （5）下载文件到本地 hdfs dfs -get &#x2F;hdfs路径 &#x2F;本地路径 （6）合并下载 hdfs dfs -getmerge &#x2F;hdfs路径文件夹 &#x2F;合并后的文件 （7）创建文件夹 hdfs dfs -mkdir &#x2F;hello （8）创建多级文件夹 hdfs dfs -mkdir -p &#x2F;hello&#x2F;world （9）移动hdfs文件 hdfs dfs -mv &#x2F;hdfs路径 &#x2F;hdfs路径 （10）复制hdfs文件 hdfs dfs -cp &#x2F;hdfs路径 &#x2F;hdfs路径 （11）删除hdfs文件 hdfs dfs -rm &#x2F;aa.txt （12）删除hdfs文件夹 hdfs dfs -rm -r &#x2F;hello （13）查看hdfs中的文件 hdfs dfs -cat &#x2F;文件 hdfs dfs -tail -f &#x2F;文件 （14）查看文件夹中有多少个文件 hdfs dfs -count &#x2F;文件夹 （15）查看hdfs的总空间 hdfs dfs -df &#x2F; hdfs dfs -df -h &#x2F; （16）修改副本数 hdfs dfs -setrep 1 &#x2F;a.txt### &lt;h id&#x3D;&quot;3&quot;&gt;三，HDFS分布式计算分析&lt;&#x2F;h&gt;##### ① HDFS的上层执行了计算，下层是一小块一小块的块文件。上层计算不能绕过HDFS访问下层的块文件。HDFS从存储块的DN处拿到块再以流的方式传递到上层文件。 // HDFS的块信息 @org.junit.Test public void biks() throws Exception { Path i = new Path(&quot;/usr/root/helloworld.txt&quot;); FileStatus iStatus =fs.getFileStatus(i); BlockLocation[] biks= fs.getFileBlockLocations(iStatus, 0, iStatus.getLen()); //取块的位置 for (BlockLocation b : biks) &#123; System.out.println(b); &#125; //上层只能通过流打开i文件 FSDataInputStream in = fs.open(i); for (int j = 0; j &lt; 15; j++) &#123; System.out.println((char)in.readByte()); &#125; &#125; 123456![取流运行结果](Hadoop-HDFS-API&#x2F;yunxingjieguo.png &quot;取流运行结果&quot;)![helloworld.txt文件内容](Hadoop-HDFS-API&#x2F;wenjianneirong.png &quot;helloworld.txt文件内容&quot;)##### ② 结果分析: * 打开HDFS文件读的流的第一个字节，就是HDFS存这个文件的第一个字节##### ③ 流可以调到任意块，随意切换位置 //上层计算只能通过流打开i文件 FSDataInputStream in = fs.open(i); for (int j = 0; j &lt; 4; j++) &#123; System.out.println((char)in.readByte()); &#125; in.seek(1048576);//跳到第二个块，第二个块的偏移量是1048576 for (int j = 0; j &lt; 8; j++) &#123; System.out.println((char)in.readByte()); &#125; &#125; 结果分析: ![分割快](Hadoop-HDFS-API/jieguo.png &quot;分割快&quot;) ##### ④ 距离概念 #### 计算向数据移动","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/categories/hadoop/"},{"name":"hdfs","slug":"hadoop/hdfs","permalink":"http://www.studyz.club/categories/hadoop/hdfs/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/tags/hadoop/"},{"name":"hdfs","slug":"hdfs","permalink":"http://www.studyz.club/tags/hdfs/"},{"name":"api","slug":"api","permalink":"http://www.studyz.club/tags/api/"}]},{"title":"YARN--新一代的资源调度框架","slug":"YARN----新一代的资源调度框架","date":"2019-05-31T07:26:00.044Z","updated":"2019-06-19T07:10:12.122Z","comments":true,"path":"posts/19780d58/","link":"","permalink":"http://www.studyz.club/posts/19780d58/","excerpt":"","text":"1. MapReduce 1.X版本的缺陷2. YARN设计思路3. YARN体系架构 3.1 ResourceManager 3.2 ApplicationMaster 3.3 NodeManager 4. YARN运行过程 4.1 YARN集群部署 4.2 YARN运行步骤 5. MRv2: On YARN 角色分析 5.1 YARN：解耦资源与计算 5.2 MR 5.3 Client 6. YARN的单点配置 6.1 YARN的一般配置 6.2 YARN的高可用配置(HA) 1. MapReduce 1.X版本的缺陷 存在单点故障,即唯一的JobTracker出现故障会导致系统不可用。 JobTracker既要负责作业的调度失败恢复，又要负责资源管理分配。JobTracker“大包大揽”导致任务过重（任务多时内存开销大，上限4000节点） 容易出现内存溢出（分配资源只考虑MapReduce任务数，不考虑CPU、内存）当两个具有较大内存消耗的任务被分配到同一个TaskTracker上时，很容易发生内存溢出。 资源划分不合理（强制划分为“槽” slot ，进一步划分为Map slot和Reduce slot供Map任务和Reduce任务使用）彼此之间不能使用分配给对方的槽，当Map任务已经用完Map槽时，即使系统中还有大量剩余的Reduce槽，也不能拿来运行Map任务，反之亦然。当系统中存在单一的Map或Reduce任务时就会造成资源的浪费。 2. YARN设计思路 YARN(Yet Another Resource Negotiator)的基本思路是“放权”，即不让JobTracker承担过多的功能。 YARN ResourceManager–&gt;负责资源管理 ApplicationMaster–&gt;负责任务调度和监控 NodeManager–&gt;负责执行原TaskTracker的任务 MapReduce1.0既是一个计算框架，也是一个资源管理调度框架 到了Hadoop2.0以后，MapReduce1.0中的资源管理调度功能，被单独分离出来形成了YARN，它是一个纯粹的资源管理调度框架，而不是一个计算框架 被剥离了资源管理调度功能的MapReduce 框架就变成了MapReduce2.0，它是运行在YARN之上的一个纯粹的计算框架，不再自己负责资源调度管理服务，而是由YARN为其提供资源管理调度服务 3. YARN体系架构 3.1 ResourceManager* 处理客户端请求 * 启动/监控ApplicationMaster * 监控NodeManager * 资源分配与调度 ResourceManager（RM）是一个全局的资源管理器，负责整个系统的资源管理和分配，主要包括两个组件，即调度器（Scheduler）和应用程序管理器（Applications Manager） 调度器接收来自ApplicationMaster的应用程序资源请求，把集群中的资源以“容器”的形式分配给提出申请的应用程序，容器的选择通常会考虑应用程序所要处理的数据的位置，进行就近选择，从而实现“计算向数据靠拢” 容器（Container）作为动态资源分配单位，每个容器中都封装了一定数量的CPU、内存、磁盘等资源，从而限定每个应用程序可以使用的资源量 调度器被设计成是一个可插拔的组件，YARN不仅自身提供了许多种直接可用的调度器，也允许用户根据自己的需求重新设计调度器 应用程序管理器（Applications Manager）负责系统中所有应用程序的管理工作，主要包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动等 3.2 ApplicationMaster 为应用程序申请资源，并分配给内部任务 任务调度、监控与容错 ResourceManager接收用户提交的作业，按照作业的上下文信息以及从NodeManager收集来的容器状态信息，启动调度过程，为用户作业启动一个ApplicationMaster 当用户作业提交时，ApplicationMaster与ResourceManager协商获取资源，ResourceManager会以容器的形式为ApplicationMaster分配资源； 把获得的资源进一步分配给内部的各个任务（Map任务或Reduce任务），实现资源的“二次分配”； 与NodeManager保持交互通信进行应用程序的启动、运行、监控和停止，监控申请到的资源的使用情况，对所有任务的执行进度和状态进行监控，并在任务发生失败时执行失败恢复（即重新申请资源重启任务）； 定时向ResourceManager发送“心跳”消息，报告资源的使用情况和应用的进度信息； 当作业完成时，ApplicationMaster向ResourceManager注销容器，执行周期完成。 3.3 NodeManager* 单个节点上的资源管理 * 处理来自ResourceManger的命令 * 处理来自ApplicationMaster的命令 NodeManager是驻留在一个YARN集群中的每个节点上的代理，主要负责： 容器生命周期管理 监控每个容器的资源（CPU、内存等）使用情况 跟踪节点健康状况 以“心跳”的方式与ResourceManager保持通信 向ResourceManager汇报作业的资源使用情况和每个容器的运行状态 接收来自ApplicationMaster的启动/停止容器的各种请求 需要说明的是，NodeManager主要负责管理抽象的容器，只处理与容器相关的事情，而不具体负责每个任务（Map任务或Reduce任务）自身状态的管理，因为这些管理工作是由ApplicationMaster完成的，ApplicationMaster会通过不断与NodeManager通信来掌握各个任务的执行状态 4. YARN运行过程4.1 YARN集群部署 在集群部署方面，YARN的各个组件是和Hadoop集群中的其他组件进行统一部署的 4.2 YARN运行步骤 步骤1：用户编写客户端应用程序，向YARN提交应用程序，提交的内容包括ApplicationMaster程序、启动ApplicationMaster的命令、用户程序等 步骤2：YARN中的ResourceManager负责接收和处理来自客户端的请求，为应用程序分配一个容器，在该容器中启动一个ApplicationMaster 步骤3：ApplicationMaster被创建后会首先向ResourceManager注册 步骤4：ApplicationMaster采用轮询的方式向ResourceManager申请资源 步骤5：ResourceManager以“容器”的形式向提出申请的ApplicationMaster分配资源 步骤6：在容器中启动任务（运行环境、脚本） 步骤7：各个任务向ApplicationMaster汇报自己的状态和进度 步骤8：应用程序运行完成后，ApplicationMaster向ResourceManager的应用程序管理器注销并关闭自己 5. MRv2: On YARN 角色分析 5.1 YARN：解耦资源与计算 ResourceManager 主，核心 集群节点资源管理 NodeManager 与RM汇报资源 管理Container生命周期 计算框架中的角色都以Container表示 Container：【节点NM，CPU,MEM,I/O大小，启动命令】 默认NodeManager启动线程监控Container大小，超出申请资源额度，kill 支持Linux内核的Cgroup(资源管理) 5.2 MR ： MR-ApplicationMaster-Container 作业为单位，避免单点故障，负载到不同的节点 创建Task需要和RM申请资源（Container /MR 1024MB） Task-Container 5.3 Client： RM-Client：请求资源创建AM AM-Client：与AM交 6. YARN的单点配置 6.1 YARN的一般配置① mapred-site.xml12345&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt; &lt;value&gt;yarn&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; ② yarn-site.xml12345&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt; &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; 6.2 YARN的高可用配置(HA)链接：ResourceManager高可用性 ② yarn-site.xml12345678910111213141516171819202122232425262728293031323334353637&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;&#x2F;name&gt; &lt;value&gt;master&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt; &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt; &lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;&#x2F;name&gt; &lt;value&gt;true&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;&#x2F;name&gt; &lt;value&gt;cluster1&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;&#x2F;name&gt; &lt;value&gt;rm1,rm2&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;&#x2F;name&gt; &lt;value&gt;slave2&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;&#x2F;name&gt; &lt;value&gt;slave3&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;&#x2F;name&gt; &lt;value&gt;slave1:2181,slave2:2181,slave3:2181&lt;&#x2F;value&gt; &lt;&#x2F;property&gt;&lt;&#x2F;configuration&gt; 启动命令: 1234先在master机执行命令:start-yarn.sh然后再在刚才配置的slave2和slave3上启动resourcemanager:yarn-daemon.sh start resourcemanager web界面: 1slave2:8088","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/categories/hadoop/"},{"name":"yarn","slug":"hadoop/yarn","permalink":"http://www.studyz.club/categories/hadoop/yarn/"}],"tags":[{"name":"yarn","slug":"yarn","permalink":"http://www.studyz.club/tags/yarn/"}]},{"title":"Hadoop-HDFS-HA配置","slug":"Hadoop-HDFS-HA配置","date":"2019-05-27T15:29:37.907Z","updated":"2019-11-12T10:34:36.512Z","comments":true,"path":"posts/c1acd5ec/","link":"","permalink":"http://www.studyz.club/posts/c1acd5ec/","excerpt":"","text":"官方文档(HDFS High Availability Using the Quorum Journal Manager) 四，角色介绍 领导者（leader），负责进行投票的发起和决议，更新系统状态（数据同步），发送心跳。 学习者（learner），包括跟随者（follower）和观察者（observer）。 跟随者（follower），用于接受客户端请求、向客户端返回结果，在选主过程中参与投票。 观察者（Observer），可以接受客户端请求，将写请求转发给leader，但observer不参加投票过程，只同步leader的状态，observer的目的是为了扩展系统，提高读取速度。1）leader失效后会在follower中重新选举新的leader2）每个follower都和leader有连接，接受leader的数据更新操作3）客户端可以连接到每个server，每个server的数据完全相同4）每个节点的服务Server，记录事务日志和快照到持久存储 五，工作原理Zookeeper的核心是原子广播，这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab协议。Zab协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。 恢复模式：当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，恢复模式不接受客户端请求，当领导者被选举出来，且大多数Server完成了和leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和Server具有相同的系统状态。 广播模式：一旦Leader已经和多数的Follower进行了状态同步后，他就可以开始广播消息了，即进入广播状态。这时候当一个Server加入ZooKeeper服务中，它会在恢复模式下启动，发现Leader，并和Leader进行状态同步。待到同步结束，它也参与消息广播。ZooKeeper的广播状态一直到Leader崩溃了或者Leader失去了大部分的Followers支持。 1）Zookeeper节点数据操作流程写操作：1）在Client向Follwer 或 Observer 发出一个写的请求2）Follwer 或 Observer 把请求发送给Leader；3）Leader接收到以后向所有follower发起提案；4）Follwer收到提案后执行写操作，然后把操作结果发送给Leader；5）当多数follower返回提案结果后，leader会commit该提议，通知其他Follower 和 Observer 同步信息；6）Follwer 或Observer把请求结果返回给Client。 读操作：1）在Client向Follwer 或 Observer 发出一个读的请求；2）Follwer 或 Observer 把请求结果返回给Client； 六，主要特点最终一致性：client不论连接到哪个Server，展示给它都是同一个视图，这是zookeeper最重要的特性；可靠性：具有简单、健壮、良好的性能，如果消息被某一台服务器接受，那么它将被所有的服务器接受；实时性：Zookeeper保证客户端将在一个时间间隔范围内获得服务器的更新信息，或者服务器失效的信息。 但由于网络延时等原因，Zookeeper不能保证两个客户端能同时得到刚更新的数据，如果需要最新数据，应该在读数据之前调用sync()接口；等待无关(wait-free)：慢的或者失效的client，不得干预快速的client的请求，使得每个client都能有效的等待；原子性：更新只能成功或者失败，没有中间状态；顺序性 ：按照客户端发送请求的顺序更新数据； 一，先将zookeeper传入准备作为第二个NameNode的节点上。二，环境变量配置123456789[root@slave1 zookeeper]# vi &#x2F;etc&#x2F;profile添加如下变量：export ZOOKEEPER_HOME&#x3D;&#x2F;opt&#x2F;zookeeperexport PATH&#x3D;$PATH:&#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;sbin:$ZOOKEEPER_HOME&#x2F;bin然后重新载入配置文件：[root@slave1 zookeeper]# . &#x2F;etc&#x2F;profile 随便进个目录输入zk然后Tab键能提示就说明环境变量配置完成。 1cd - &#x2F;&#x2F;回到之前路径 三，修改配置文件123[root@slave1 zookeeper]# cd conf&#x2F;[root@slave1 conf]# cp zoo_sample.cfg zoo.cfg[root@slave1 conf]# vi zoo.cfg 调整路径不要放在临时目录：dataDir=/usr/local/hadoop-2.7.3/zk 1234567891011121314151617181920212223242526272829303132# The number of milliseconds of each ticktickTime&#x3D;2000# The number of ticks that the initial# synchronization phase can takeinitLimit&#x3D;10# The number of ticks that can pass between# sending a request and getting an acknowledgementsyncLimit&#x3D;5# the directory where the snapshot is stored.# do not use &#x2F;tmp for storage, &#x2F;tmp here is just# example sakes.dataDir&#x3D;&#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;zk# the port at which the clients will connectclientPort&#x3D;2181# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns&#x3D;60## Be sure to read the maintenance section of the# administrator guide before turning on autopurge.## http:&#x2F;&#x2F;zookeeper.apache.org&#x2F;doc&#x2F;current&#x2F;zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount&#x3D;3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge feature#autopurge.purgeInterval&#x3D;1server.1&#x3D;192.168.43.160:2888:3888server.2&#x3D;192.168.43.161:2888:3888server.3&#x3D;192.168.43.162:2888:3888 server为要布置为zookeeper的节点zookeeper运行时有两个状态 所以有两个端口正常运行 和出错是选出新的Leader（zookeeper也选用主从模型主：Leader）一个是leader，剩余的是Followerid最大的为leader 七，创建目录123[root@slave1 conf]# mkdir -p &#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;zk[root@slave1 conf]# echo 1 &gt; &#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;zk&#x2F;myid&#x2F;&#x2F;1为刚才server后面的字符 八，将配置好的zookeeper文件分发到刚才写server的节点中123456[root@slave1 opt]# scp -r .&#x2F;zookeeper&#x2F; slave2:&#96;pwd&#96;[root@slave1 opt]# scp -r .&#x2F;zookeeper&#x2F; slave3:&#96;pwd&#96;在分发的节点中添加自己的myid:[root@slave2 opt]# mkdir -p &#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;zk[root@slave2 opt]# echo 2 &gt; &#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;zk&#x2F;myid 九，启动在刚开始配置的slave1中：123456789[root@slave1 opt]# zkServer.sh startZooKeeper JMX enabled by defaultUsing config: &#x2F;opt&#x2F;zookeeper&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfgStarting zookeeper ... STARTED[root@slave1 opt]# zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: &#x2F;opt&#x2F;zookeeper&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfgError contacting service. It is probably not running. 此时发现并未启动这是因为需要集群中启动zookeeper的节点过半才行 十，注意事项这里要注意java的环境配置1234567export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_91export JRE_HOME&#x3D;$JAVA_HOME&#x2F;jreexport PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin:$JRE_HOME&#x2F;binexport CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar:$JRE_HOME&#x2F;libexport ZOOKEEPER_HOME&#x3D;&#x2F;opt&#x2F;zookeeperexport PATH&#x3D;$PATH:&#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;bin:&#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;sbin:$ZOOKEEPER_HOME&#x2F;bin:PATH 还要手动放行端口1vi &#x2F;etc&#x2F;sysconfig&#x2F;iptables &#x2F;&#x2F;进到配置文件 输入如下内容： 12345#zookeeper-A INPUT -p tcp -m state --state NEW -m tcp --dport 2181 -j ACCEPT-A INPUT -p tcp -m state --state NEW -m tcp --dport 2888 -j ACCEPT-A INPUT -p tcp -m state --state NEW -m tcp --dport 3888 -j ACCEPT 1service iptables restart &#x2F;&#x2F;重启防火墙 运行状态12[root@slave1 ~]# zkServer.sh status 十一，NameNode配置① 进入haoop的etc12[root@master ~]# cd &#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;etc&#x2F; ② 将hadoop文件夹拷贝一份123[root@master etc]# cp -r hadoop haoop-full&#x2F;&#x2F; 原来的hadoop文件用来做HA&#x2F;&#x2F;复制出来的hadoop-full用来做之前的普通分布式方便以后不用HA时切换 注意有HDFS HA就没有SecondaryNameNode③ 配置hdfs-site.xml12[root@master hadoop]# vi hdfs-site.xml 内容如下// master–&gt;node01, slave1–&gt;node02以此类推 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;&#x2F;name&gt; &lt;value&gt;3&lt;&#x2F;value&gt; &lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;dfs.nameservices&lt;&#x2F;name&gt; &lt;value&gt;mycluster&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;&#x2F;name&gt; &lt;value&gt;nn1,nn2&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;&#x2F;name&gt; &lt;value&gt;master:8020&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;&#x2F;name&gt; &lt;value&gt;slave1:8020&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;&#x2F;name&gt; &lt;value&gt;master:50070&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;&#x2F;name&gt; &lt;value&gt;slave1:50070&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;&#x2F;name&gt; &lt;value&gt;qjournal:&#x2F;&#x2F;master:8485;slave1:8485;slave2:8485&#x2F;mycluster&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;&#x2F;name&gt; &lt;value&gt;&#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;ha&#x2F;jn&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;&#x2F;name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;&#x2F;name&gt; &lt;value&gt;shell(&#x2F;bin&#x2F;true)&lt;&#x2F;value&gt; &#x2F;&#x2F;这里不写成这样可能会导致kill某一个NameNode时另一个出于standby的节点不能被拉起。配置的时候不要忘了把这个注释删掉。 ！！！&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;&#x2F;name&gt; &lt;value&gt;&#x2F;root&#x2F;.ssh&#x2F;id_rsa&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; 另外再添加(hdfs-site.xml) 12345&lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;&#x2F;name&gt; &lt;value&gt;true&lt;&#x2F;value&gt;&lt;&#x2F;property&gt; ④ core-site.xml123456789101112131415161718&lt;configuration&gt;&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt; &lt;value&gt;hdfs:&#x2F;&#x2F;mycluster&lt;&#x2F;value&gt;&lt;&#x2F;property&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt; &lt;value&gt;&#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;ha&lt;&#x2F;value&gt; &lt;description&gt;Abase for other temporary directories.&lt;&#x2F;description&gt;&lt;&#x2F;property&gt;&lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;&#x2F;name&gt; &lt;value&gt;slave1:2181,slave2:2181,slave3:2181&lt;&#x2F;value&gt; &lt;&#x2F;property&gt;&lt;&#x2F;configuration&gt; 十二，分发修改的配置文件到各个节点( hdfs-site.xml core-site.xml)123[root@master hadoop]# scp hdfs-site.xml core-site.xml slave1:&#96;pwd&#96;[root@master hadoop]#scp hdfs-site.xml core-site.xml slave2:&#96;pwd&#96;[root@master hadoop]#scp hdfs-site.xml core-site.xml slave3:&#96;pwd&#96; 注意：要做master级与zookeeper所在节点的免秘钥使zookeeper节点``能够免密访问master`① 在slave1中1234[root@slave1 ~]# ssh-keygen -t rsa -P &#39;&#39; -f ~&#x2F;.ssh&#x2F;id_rsa[root@slave1 ~]# cd .ssh[root@slave1 .ssh]# cat id_rsa.pub &gt;&gt; authorized_keys &#x2F;&#x2F;追加密钥 十三， 部署① 手动启动journalnode先启动master机 12[root@master hadoop]# hadoop-daemon.sh start journalnode 再启动slave机 1[root@slave1 ~]# hadoop-daemon.sh start journalnode 三个slave机都要启动这里不一一列举了，可以用jps验证是否启动 ② 格式化在master机执行 12[root@master hadoop]# hdfs namenode -format 如果报错了要记得打开端口具体操作前面有 见到INFO common.Storage: Storage directory /usr/local/hadoop-2.7.3/ha/dfs/name has been successfully formatted.才算成功 123456789101112131415161718192021222324252627282930192.168.43.159:8485: false1 exceptions thrown:192.168.43.161:8485: No Route to Host from master&#x2F;192.168.43.159 to slave2:8485 failed on socket timeout exception: java.net.NoRouteToHostException: 没有到主机的路由; For more details see: http:&#x2F;&#x2F;wiki.apache.org&#x2F;hadoop&#x2F;NoRouteToHost at org.apache.hadoop.hdfs.qjournal.client.QuorumException.create(QuorumException.java:81) at org.apache.hadoop.hdfs.qjournal.client.QuorumCall.rethrowException(QuorumCall.java:223) at org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.hasSomeData(QuorumJournalManager.java:232) at org.apache.hadoop.hdfs.server.common.Storage.confirmFormat(Storage.java:901) at org.apache.hadoop.hdfs.server.namenode.FSImage.confirmFormat(FSImage.java:184) at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:988) at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1434) at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1559)19&#x2F;06&#x2F;02 13:13:11 INFO ipc.Client: Retrying connect to server: slave1&#x2F;192.168.43.160:8485. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries&#x3D;10, sleepTime&#x3D;1000 MILLISECONDS)19&#x2F;06&#x2F;02 13:13:11 ERROR namenode.NameNode: Failed to start namenode.org.apache.hadoop.hdfs.qjournal.client.QuorumException: Unable to check if JNs are ready for formatting. 1 successful responses:192.168.43.159:8485: false1 exceptions thrown:192.168.43.161:8485: No Route to Host from master&#x2F;192.168.43.159 to slave2:8485 failed on socket timeout exception: java.net.NoRouteToHostException: 没有到主机的路由; For more details see: http:&#x2F;&#x2F;wiki.apache.org&#x2F;hadoop&#x2F;NoRouteToHost at org.apache.hadoop.hdfs.qjournal.client.QuorumException.create(QuorumException.java:81) at org.apache.hadoop.hdfs.qjournal.client.QuorumCall.rethrowException(QuorumCall.java:223) at org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.hasSomeData(QuorumJournalManager.java:232) at org.apache.hadoop.hdfs.server.common.Storage.confirmFormat(Storage.java:901) at org.apache.hadoop.hdfs.server.namenode.FSImage.confirmFormat(FSImage.java:184) at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:988) at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1434) at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1559)19&#x2F;06&#x2F;02 13:13:11 INFO util.ExitUtil: Exiting with status 119&#x2F;06&#x2F;02 13:13:11 INFO namenode.NameNode: SHUTDOWN_MSG:&#x2F;************************************************************SHUTDOWN_MSG: Shutting down NameNode at master&#x2F;192.168.43.159************************************************************&#x2F; 1-A INPUT -p tcp -m state --state NEW -m tcp --dport 8485 -j ACCEPT ④ 启动NameNode(在master机操作)12[root@master hadoop]# hadoop-daemon.sh start namenode ⑤ 在第二台(slave1)12[root@slave1 ~]# hdfs namenode -bootstrapStandby 此时如果报错123456789101112131415161759:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries&#x3D;10, sleepTime&#x3D;1000 MILLISECONDS)19&#x2F;06&#x2F;02 13:55:31 INFO ipc.Client: Retrying connect to server: master&#x2F;192.168.43.159:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries&#x3D;10, sleepTime&#x3D;1000 MILLISECONDS)19&#x2F;06&#x2F;02 13:55:32 INFO ipc.Client: Retrying connect to server: master&#x2F;192.168.43.159:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries&#x3D;10, sleepTime&#x3D;1000 MILLISECONDS)19&#x2F;06&#x2F;02 13:55:33 INFO ipc.Client: Retrying connect to server: master&#x2F;192.168.43.159:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries&#x3D;10, sleepTime&#x3D;1000 MILLISECONDS)19&#x2F;06&#x2F;02 13:55:34 INFO ipc.Client: Retrying connect to server: master&#x2F;192.168.43.159:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries&#x3D;10, sleepTime&#x3D;1000 MILLISECONDS)19&#x2F;06&#x2F;02 13:55:35 INFO ipc.Client: Retrying connect to server: master&#x2F;192.168.43.159:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries&#x3D;10, sleepTime&#x3D;1000 MILLISECONDS)19&#x2F;06&#x2F;02 13:55:36 INFO ipc.Client: Retrying connect to server: master&#x2F;192.168.43.159:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries&#x3D;10, sleepTime&#x3D;1000 MILLISECONDS)19&#x2F;06&#x2F;02 13:55:37 INFO ipc.Client: Retrying connect to server: master&#x2F;192.168.43.159:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries&#x3D;10, sleepTime&#x3D;1000 MILLISECONDS)19&#x2F;06&#x2F;02 13:55:38 INFO ipc.Client: Retrying connect to server: master&#x2F;192.168.43.159:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries&#x3D;10, sleepTime&#x3D;1000 MILLISECONDS)19&#x2F;06&#x2F;02 13:55:39 INFO ipc.Client: Retrying connect to server: master&#x2F;192.168.43.159:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries&#x3D;10, sleepTime&#x3D;1000 MILLISECONDS)19&#x2F;06&#x2F;02 13:55:39 FATAL ha.BootstrapStandby: Unable to fetch namespace information from active NN at master&#x2F;192.168.43.159:8020: No Route to Host from slave1&#x2F;192.168.43.160 to master:8020 failed on socket timeout exception: java.net.NoRouteToHostException: 没有到主机的路由; For more details see: http:&#x2F;&#x2F;wiki.apache.org&#x2F;hadoop&#x2F;NoRouteToHost19&#x2F;06&#x2F;02 13:55:39 INFO util.ExitUtil: Exiting with status 219&#x2F;06&#x2F;02 13:55:39 INFO namenode.NameNode: SHUTDOWN_MSG:&#x2F;************************************************************SHUTDOWN_MSG: Shutting down NameNode at slave1&#x2F;192.168.43.160************************************************************&#x2F; 需要手动打开8020端口，具体方法前面有123vi &#x2F;etc&#x2F;sysconfig&#x2F;iptables &#x2F;&#x2F;进到配置文件-A INPUT -p tcp -m state --state NEW -m tcp --dport 8020 -j ACCEPTservice iptables restart &#x2F;&#x2F;重启防火墙 ⑥ 在master机格式化zookeeper12hdfs zkfc -formatZK 成功提示: 12345619&#x2F;06&#x2F;02 19:51:13 INFO ha.ActiveStandbyElector: Recursively deleting &#x2F;hadoop-ha&#x2F;mycluster from ZK...19&#x2F;06&#x2F;02 19:51:13 INFO ha.ActiveStandbyElector: Successfully deleted &#x2F;hadoop-ha&#x2F;mycluster from ZK.19&#x2F;06&#x2F;02 19:51:13 INFO ha.ActiveStandbyElector: Successfully created &#x2F;hadoop-ha&#x2F;mycluster in ZK.19&#x2F;06&#x2F;02 19:51:13 INFO zookeeper.ZooKeeper: Session: 0x200002767560000 closed19&#x2F;06&#x2F;02 19:51:13 INFO zookeeper.ClientCnxn: EventThread shut down ⑦ 验证(在slave3)打开客户端12[root@slave3 ~]# zkCli.sh 查看根目录可以看到格式化创建的文件: 12[zk: localhost:2181(CONNECTING) 1] ls &#x2F; 进入到hadoop-ha目录查看[mycluster] 123[zk: localhost:2181(CONNECTED) 4] ls &#x2F;hadoop-ha[mycluster] 十四，启动hdfs① 在master机执行:1[root@master ~]# start-dfs.sh 得到如下反馈: 12345678910111213141519&#x2F;06&#x2F;02 20:10:47 WARN fs.FileSystem: &quot;hdfs:mycluster&quot; is a deprecated filesystem name. Use &quot;hdfs:&#x2F;&#x2F;hdfs:mycluster&#x2F;&quot; instead.Starting namenodes on [master slave1]master: namenode running as process 2982. Stop it first.slave1: starting namenode, logging to &#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;hadoop-root-namenode-slave1.outslave3: starting datanode, logging to &#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;hadoop-root-datanode-slave3.outslave2: starting datanode, logging to &#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;hadoop-root-datanode-slave2.outslave1: starting datanode, logging to &#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;hadoop-root-datanode-slave1.outStarting journal nodes [master slave1 slave2]master: journalnode running as process 2868. Stop it first.slave2: journalnode running as process 2875. Stop it first.slave1: journalnode running as process 2909. Stop it first.Starting ZK Failover Controllers on NN hosts [master slave1]master: starting zkfc, logging to &#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;hadoop-root-zkfc-master.outslave1: starting zkfc, logging to &#x2F;usr&#x2F;local&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;hadoop-root-zkfc-slave1.out ② 输入jps验证:(DFSZKFailoverController为zookeeper进程)123456[root@master ~]# jps2868 JournalNode2982 NameNode5271 Jps5052 DFSZKFailoverController slave1: 12345678[root@slave1 ~]# jps5081 Jps3403 QuorumPeerMain4588 DataNode4716 DFSZKFailoverController2909 JournalNode4509 NameNode slave2: 123456[root@slave2 ~]# jps4817 Jps3303 QuorumPeerMain4394 DataNode2875 JournalNode ③ slave3中此时mycluster中有东西了:123[zk: localhost:2181(CONNECTED) 7] ls &#x2F;hadoop-ha&#x2F;mycluster[ActiveBreadCrumb, ActiveStandbyElectorLock] 在slave3中用get命令取节点的数据: 123456789101112131415[zk: localhost:2181(CONNECTED) 9] get &#x2F;hadoop-ha&#x2F;mycluster&#x2F;ActiveStandbyElectorLock myclusternn1master �&gt;(�&gt;cZxid &#x3D; 0x500000008ctime &#x3D; Sun Jun 02 20:11:14 CST 2019mZxid &#x3D; 0x500000008mtime &#x3D; Sun Jun 02 20:11:14 CST 2019pZxid &#x3D; 0x500000008cversion &#x3D; 0dataVersion &#x3D; 0aclVersion &#x3D; 0ephemeralOwner &#x3D; 0x200002767560001dataLength &#x3D; 30numChildren &#x3D; 0 myclusternn1master �&gt;(�&gt;说明master写的锁 进入master机hdfs的web管理界面: 1http:&#x2F;&#x2F;192.168.43.159:50070 进入slave1机hdfs的web管理界面: 十五，HA验证①由上面我们知道master是active,用jps看到在master机中NameNode对应的进程号是2982 NameNode接下来杀死master机上的NameNode进程: 123456[root@master ~]# kill -9 2982[root@master ~]# jps7185 Jps2868 JournalNode5052 DFSZKFailoverController 此时master机hdfs的web管理界面已经加载不出来了。slave1机hdfs的web管理界面: 然后用这个命令单独启动进程(start后面跟要启动的进程): 12[root@master hadoop]# hadoop-daemon.sh start namenode ② 还可以继续验证在slave1中:kill zkfc 123456789[root@slave1 hadoop]# jps12529 Jps10887 JournalNode3403 QuorumPeerMain10814 DataNode10959 DFSZKFailoverController11295 NameNode[root@slave1 hadoop]# kill -9 10959 这时会发现aster机hdfs的web管理界面:而slave1机hdfs的web管理界面:重新启动zkfc: 12[root@slave1 hadoop]# hadoop-daemon.sh start zkfc","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/categories/hadoop/"},{"name":"hdfs","slug":"hadoop/hdfs","permalink":"http://www.studyz.club/categories/hadoop/hdfs/"}],"tags":[{"name":"hdfs","slug":"hdfs","permalink":"http://www.studyz.club/tags/hdfs/"},{"name":"ha","slug":"ha","permalink":"http://www.studyz.club/tags/ha/"}]},{"title":"Hadoop-HDFS-HA模式原理","slug":"Hadoop-HDFS-HA模式原理","date":"2019-05-27T03:49:24.809Z","updated":"2019-06-19T07:04:39.546Z","comments":true,"path":"posts/93a16b6c/","link":"","permalink":"http://www.studyz.club/posts/93a16b6c/","excerpt":"","text":"1. Hadoop 2.0产生的背景 Hadoop 1.0中的HDFS和MapReduce在高可用，扩展等方面存在问题 HDFS存在的问题 NameNode单点故障，难以应用于在线场景 HA NameNode压力过大，且内存受限，影响扩展性 F MapReduce存在的问题 JobTracker访问压力过大，影响系统扩展性 难以支持除MapReduce之外的计算框架，比如Spark，Storm等 2. Hadoop 1.X与Hadoop 2.X Hadoop 2.x由HDFS、MapReduce和YARN三个分支构成； HDFS：NN `Federation（联邦）``、HA； 2.X:只支持2个节点HA，3.0实现了一主多从 MapReduce：运行在YARN上的MR；离线计算，基于磁盘I/O计算 YARN：资源管理系统 3. HDFS 2.X (解决方案) 解决HDFS 1.0中单点故障和内存受限问题。 解决单点故障 HDFS HA：通过主备NameNode解决 如果主NameNode发生故障，则切换到备NameNode上 解决内存受限问题 HDFS Federation(联邦) 水平扩展，支持多个NameNode； （2）每个NameNode分管一部分目录； （1）所有NameNode共享所有DataNode存储资源 2.x仅是架构上发生了变化，使用方式不变 对HDFS使用者透明 HDFS 1.x中的命令和API仍可以使用 4. HDFS 2.X HA","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/categories/hadoop/"},{"name":"hdfs","slug":"hadoop/hdfs","permalink":"http://www.studyz.club/categories/hadoop/hdfs/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/tags/hadoop/"},{"name":"hdfs","slug":"hdfs","permalink":"http://www.studyz.club/tags/hdfs/"},{"name":"ha","slug":"ha","permalink":"http://www.studyz.club/tags/ha/"}]},{"title":"Hadoop-HDFS","slug":"Hadoop-HDFS","date":"2019-05-23T14:45:40.912Z","updated":"2019-06-19T07:06:33.962Z","comments":true,"path":"posts/39223cc/","link":"","permalink":"http://www.studyz.club/posts/39223cc/","excerpt":"","text":"TAGS: HDFS Hadoop 1. HDFS存储模型：字块（重点） 文件线性切割成块（Block）:偏移量offset（byte） Block`分散``存储在集群节点中 单一文件Block大小一致，文件与文件可以不一致 Block可以设置副本数，副本分散在不同节点中。 副本数不要超过节点数 文件上传可以设置Block大小和副本数 已上传的文件Block副本数可以调，大小不能再改变 只支持一次写入多次读取，同时刻只一个写入者 注：分布式计算不只有HDFS,HDFS是为了更好的支撑上层分布式计算。 可以append追加数据 2. 架构模型–主从架构 文件元数据MetaData，文件数据 元数据 数据本身 （主）NameNode节点保存文件元数据: 单节点 posix (会存在一个虚拟目录结构记录的是文件结构/名) (从)DataNode节点保存文件Block数据:多节点 DataNode与NameNode保持心跳，提交Block列表 hdfsClient（客户端）与NameNode交互元数据信息 HdfsClient与DataNode交互文件Block数据 3. NameNode–操作都在内存中（NN） 基于内存存储:不会和磁盘发生交换 只存在内存中（断电丢失） 持久化 NameNode主要功能： 接收客户端的读写服务 收集DataNode汇报的Block列表信息 NameNode保存metadata（元数据）信息包括 文件owership和permissions 文件大小和时间 （Block列表：block偏移量），位置信息–&gt;文件位置 Block每副本位置（由DataNode上报） NameNode持久化 NameNode的metadata信息在启动后会加载到内存 metadata存储到磁盘文件名为FsImage Block的位置信息不会保存到FsImage EditLog记录对metadata的操作日志..增删改等1. 在HDFS1.0体系结构中，在整个HDFS集群中只有一个命名空间，并且只有唯一一个名称节点，该节点负责对这个命名空间进行管理2.HDFS使用的是传统的分级文件体系，因此，用户可以像使用普通文件系统一样，创建、删除目录和文件，在目录间转移文件，重命名文件等 3.1 FsImage文件 FsImage文件包含文件系统中所有目录和文件inode的序列化形式。每个inode是一个文件或目录的元数据的内部表示，并包含此类信息：文件的复制等级、修改和访问时间、访问权限、块大小以及组成文件的块。对于目录，则存储修改时间、权限和配额元数据 FsImage文件没有记录块存储在哪个数据节点。而是由名称节点把这些映射保留在内存中，当数据节点加入HDFS集群时，数据节点会把自己所包含的块列表告知给名称节点，此后会定期执行这种告知操作，以确保名称节点的块映射是最新的。 3.2 名称节点启动过程 在名称节点启动的时候，它会将FsImage（格式化的时候会产生一个空的）文件中的内容加载到内存中，之后再执行EditLog文件中的各项操作，使得内存中的元数据和实际的同步，存在内存中的元数据支持客户端的读操作。 一旦在内存中成功建立文件系统元数据的映射，则创建一个新的FsImage文件和一个空的EditLog文件 此刻NameNode运行在安全模式。即NameNode的文件系统对与客户端来说是只读的。（显示目录，显示文件内容等。写，删除，重命名都会失败） 名称节点起来之后，HDFS中的更新操作会重新写到EditLog文件中，因为FsImage（如果没有SNN则只有在启动的时候才会更新并且只会更新一次）文件一般都很大（GB级别的很常见），如果所有的更新操作都往FsImage文件中添加，这样会导致系统运行的十分缓慢，但是，如果往EditLog文件里面写就不会这样，因为EditLog 要小很多。每次执行写操作之后，且在向客户端发送成功代码之前，edits文件都需要同步更新 3.3 问题:名称节点运行期间EditLog不断变大 在名称节点运行期间，HDFS的所有更新操作都是直接写到EditLog中，久而久之， EditLog文件将会变得很大 虽然这对名称节点运行时候是没有什么明显影响的，但是，当名称节点重启的时候，名称节点需要先将FsImage里面的所有内容映像到内存中，然后再一条一条地执行EditLog中的记录，当EditLog文件非常大的时候，会导致名称节点启动操作非常慢，而在这段时间内HDFS系统处于安全模式，一直无法对外提供写操作，影响了用户的使用解决办法 SecondaryNameNode 4. DataNode（DN） 本地磁盘目录存储数据（Block），文件形式。数据节点是分布式文件系统HDFS的工作节点，负责数据的存储和读取，会根据客户端或者是名称节点的调度来进行数据的存储和检索，并且向名称节点定期发送自己所存储的块的列表。 同时存储Block的元数据文件 启动DataNode时会向NameNode汇报block信息 通过NameNode发送心跳保持与其联系（3秒一次），如果NameNode10分钟没有收到DataNode的心跳，则认为其已经lost，并copy其上（（曾经上传的block信息然后从别的DataNode中再复制一份）的block到其他的DataNode。 5. HDFS优缺点HDFS优点 高容错性 数据自动保存多个副本 副本丢失后，自动恢复 适合批处理 移动计算而非数据 数据位置暴露给计算框架（block偏移量） 适合大数据处理 GB，TB，甚至PB 百万规模以上的文件数量 10K+节点 可构建在廉价机器上 通过副本提高可靠性 提供了容错和恢复机制 HDFS缺点 低延迟数据访问 比如毫秒级 低延迟与高吞吐率 小文件存取 占用NameNode的大量内存(文件元数据会存储在NameNode中，这样小文件占用的NameNode的空间和大文件占用的几乎一样这样就导致整个集群的DataNode存不了很多文件) 寻道时间超过读取时间 并发写入，文件随机修改 一个文件只能有一个读写者 仅支持append 6. SecondaryNameNode（SNN） 第二名称节点是HDFS架构中的一个组成部分，它是用来保存名称节点中对HDFS 元数据信息的备份，并减少名称节点重启的时间。SecondaryNameNode一般是单独运行在一台机器上 SNN执行合并的时机 根据配置文件设置的时间间隔fs.checkpoint.period默认3600秒 根据配置文件设置editlog大小fs.checkpoint.size规定edits文件的最大默认值是64MB SecondaryNameNode工作流程 SecondaryNameNode会定期和NameNode通信，请求其停止使用EditLog文件，暂时将新的写操作写到一个新的文件edit.new上来，这个操作是瞬间完成，上层写日志的函数完全感觉不到差别； SecondaryNameNode通过HTTP GET方式从NameNode上获取到FsImage和EditLog文件，并下载到本地的相应目录下； SecondaryNameNode将下载下来的FsImage载入到内存，然后一条一条地执行EditLog文件中的各项更新操作，使得内存中的FsImage保持最新；这个过程就是EditLog和FsImage文件合并； SecondaryNameNode执行完（3）操作之后，会通过post方式将新的FsImage文件发送到NameNode节点上 NameNode将从SecondaryNameNode接收到的新的FsImage替换旧的FsImage文件，同时将edit.new替换EditLog文件，通过这个过程EditLog就变小了 7. Block的副本放置策略 第一个副本: 放置在上传文件的DN;如果是集群外提交，则随机挑选一台磁盘不太满，CPU不太忙的节点。 第二个副本：放置在于第一个副本不同机架的节点上。 第三个副本: 与第二个副本相同机架的其他节点。 更多副本: 随机节点。 7.1 对于客户端的定义 客户端是用户操作HDFS最常用的方式，HDFS在部署时都提供了客户端 HDFS客户端是一个库，暴露了HDFS文件系统接口，这些接口隐藏了HDFS实现中的大部分复杂性 客户端可以支持打开、读取、写入等常见的操作，并且提供了类似Shell的命令行方式来访问HDFS中的数据 HDFS也提供了Java API，作为应用程序访问文件系统的客户端编程接口 8. HDFS的写流程 核心部分: Pipeline of datanodes以及节点之间如何相互传递DN如何与NN心跳同步。9. HDFS的读流程 距离优先–NameNode会根据客户端的位置和三个副本存放的位置做一个排序。先本机，再同机架 由于NameNode存在文件列表，使I/O拥有对文件有任意位置单独读取的能力。（可以单独读到指定的块） 流程：客户端先跟NameNode交互要读的文件（距离优先，读取的时候可以读任何一个块不必每次都从0开始读），HDFS分布式文件系统很好的支持了计算层的本地化读取。 10. HDFS文件的权限–POSIX 阻止好人做错事的意思是：为了防止不小心删除了别的用户的文件。 不是阻止坏人做坏事的意思是：如果新安装一台机器到集群中，对于HDFS来说它就具有root权限，如果在这台机器中启动一个与A用户同名的HDFS客户端。那么集群会认为这台机器就是A。此时可以执行删除等操作。。。。因此为了安全就需要结合其他的权限系统kerberos–节点，用户，权限多重认证。 11. HDFS的通信协议 所有的HDFS通信协议都是构建在TCP/IP协议基础之上的。 客户端通过一个可配置的端口向名称节点主动发起TCP连接，并使用客户端协议与名称节点进行交互。 名称节点和数据节点之间则使用数据节点协议进行交互。 客户端与数据节点的交互是通过RPC（Remote Procedure Call）来实现的。在设计上，名称节点不会主动发起RPC，而是响应来自客户端和数据节点的RPC请求","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/categories/hadoop/"},{"name":"hdfs","slug":"hadoop/hdfs","permalink":"http://www.studyz.club/categories/hadoop/hdfs/"}],"tags":[{"name":"hdfs","slug":"hdfs","permalink":"http://www.studyz.club/tags/hdfs/"}]},{"title":"Hadoop","slug":"hadoop","date":"2019-05-23T12:43:12.641Z","updated":"2019-06-19T07:06:59.780Z","comments":true,"path":"posts/b3349d42/","link":"","permalink":"http://www.studyz.club/posts/b3349d42/","excerpt":"","text":"1. 简介 Hadoop的思想之源：Google Openstack: NASA 面对的数据和计算难题-大量的网页怎么存储-搜索算法 带给我们的关键技术和思想-GFS-Map-Reduce-Bigtale Hadoop包含的模块（Modules）Hadoop项目网址The project includes these modules: Hadoop Common: 支持其他Hadoop模块的常用实用程序。 Hadoop Distributed File System (HDFS™): 一种分布式文件系统，可提供对应用程序数据的高吞吐量访问。 Hadoop YARN: 作业调度和集群资源管理的框架。 Hadoop MapReduce: 基于YARN的系统，用于并行处理大型数据集。 Hadoop Ozone: Hadoop的对象存储。 Hadoop Submarine: Hadoop的机器学习引擎。 Hadoop相关项目Apache的其他Hadoop相关项目包括: Ambari™：基于Web的工具，用于配置，管理和监控Apache Hadoop集群，包括对Hadoop HDFS，Hadoop MapReduce，Hive，HCatalog，HBase，ZooKeeper，Oozie，Pig和Sqoop的支持。Ambari还提供了一个用于查看群集运行状况的仪表板，例如热图和能够直观地查看MapReduce，Pig和Hive应用程序以及以用户友好的方式诊断其性能特征的功能。 Avro™：数据序列化系统。 Cassandra™：可扩展的多主数据库，没有单点故障。 Chukwa™：用于管理大型分布式系统的数据收集系统。 HBase™：可扩展的分布式数据库，支持大型表的结构化数据存储。 Hive™：一种数据仓库基础架构，提供数据汇总和即席查询。 Mahout™：可扩展的机器学习和数据挖掘库。 Pig™：用于并行计算的高级数据流语言和执行框架。 Spark™：用于Hadoop数据的快速通用计算引擎。Spark提供了一种简单而富有表现力的编程模型，支持广泛的应用程序，包括ETL，机器学习，流处理和图形计算。 Tez™：基于Hadoop YARN的通用数据流编程框架，它提供了一个功能强大且灵活的引擎来执行任意DAG任务，以处理批量和交互式用例的数据。Tez正在被Hadoop生态系统中的Hive™，Pig™和其他框架以及其他商业软件（例如ETL工具）采用，以取代Hadoop™MapReduce作为底层执行引擎。 ZooKeeper™：用于分布式应用程序的高性能协调服务。","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/categories/hadoop/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/tags/hadoop/"}]},{"title":"Map-Reduce原理、体系架构和工作机制","slug":"Mapreduce","date":"2019-05-22T23:05:28.444Z","updated":"2019-06-19T07:07:24.963Z","comments":true,"path":"posts/4678b149/","link":"","permalink":"http://www.studyz.club/posts/4678b149/","excerpt":"","text":"1. 并行计算框架MPI, PVM, CUDA, BOINC, Map-Reduce MapReduce将复杂的、运行于大规模集群上的并行计算过程高度地抽象到了两个函数：Map和Reduce 编程容易，不需要掌握分布式并行编程细节，也可以很容易把自己的程序运行在分布式系统上，完成海量数据的计算 MapReduce采用“分而治之”策略，一个存储在分布式文件系统中的大规模数据集，会被切分成许多独立的分片（split），这些分片可以被多个Map任务并行处理 MapReduce设计的一个理念就是“计算向数据靠拢”，而不是“数据向计算靠拢”，因为，移动数据需要大量的网络传输开销 MapReduce框架采用了Master/Slave架构，包括一个Master和若干个Slave。Master上运行JobTracker，Slave上运行TaskTracker Hadoop框架是用Java实现的，但是，MapReduce应用程序则不一定要用Java来写 2. MapReduce的体系结构MapReduce体系结构主要由四个部分组成，分别是： 2.1 Client: 用户编写的MapReduce程序通过Client提交到JobTracker端 用户可通过Client提供的一些接口查看作业运行状态 2.2 JobTracker(作业追踪器) JobTracker负责资源监控和作业调度 JobTracker 监控所有TaskTracker与Job的健康状况，一旦发现失败，就将相应的任务转移到其他节点 JobTracker 会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器（TaskScheduler），而调度器会在资源出现空闲时，选择合适的任务去使用这些资源 2.3 TaskTracker(任务跟踪器) TaskTracker 会周期性地通过“心跳”将本节点上资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker 发送过来的命令并执行相应的操作（如启动新任务、杀死任务等） TaskTracker 使用“slot”等量划分本节点上的资源量（CPU、内存等）。一个Task 获取到一个slot 后才有机会运行，而Hadoop调度器的作用就是将各个TaskTracker上的空闲slot分配给Task使用。slot 分为Map slot 和Reduce slot 两种，分别供MapTask 和Reduce Task 使用 2.4 Task Task 分为Map Task 和Reduce Task 两种，均由TaskTracker 启动 MapReduce的体系结构1.X 3. MapReduce运行分析MapTask 和 ReduceTask 先执行Map再执行Reduce。不同的Map任务之间不会进行通信且不同的Reduce任务之间也不会发生任何信息交换。所有的数据交换都是通过MapReduce框架自身去实现的。 Map任务可以有多个。 Reduce的个数根据实际情况要输出的结果确定。比如说要求输入男女数，如果只有一个Reduce则要执行两遍统计，如果有两个Reduce则可并行统计男女效率高一倍。—-&gt;所以数量是根据人决定的(软件，硬件) -_-| 。 每个Map前面有一个split 一个Map对应一个split。 split（切片）：描述的是Map处理的数据量的大小，是一个逻辑概念从哪处理到哪结束，默认切片等于块–&gt;默认Map处理的就是一个块。但是可以人为将切片调的很小。 Map的数量由split决定。 原语（重要）： 相同”的key为一组，调用一次reduce方法，方法内迭代这一组数据进行计算 (类似的sql)4. 小结 block &gt; split 1:1 N:1 1:N split &gt; map 1:1 map &gt; reduce N:1 N:N 1:1 1:N group(key)&gt;partition 1:1 N:1 N:N 1:N? &gt;违背了原语 partition &gt; outputfile 5. MapReduce各个阶段执行阶段 5.1 Split 如上所述: HDFS 以固定大小的block 为基本单位存储数据，而对于MapReduce 而言，其处理单位是split。split 是一个逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。它的划分方法完全由用户自己决定。 5.2 MapReduce任务的数量 Hadoop为每个split创建一个Map任务，split 的多少决定了Map任务的数目。大多数情况下，理想的分片大小是一个HDFS块 5.3 Reduce任务数量 最优的Reduce任务个数取决于集群中可用的reduce任务槽(slot)的数目 通常设置比reduce任务槽数目稍微小一些的Reduce任务个数（这样可以预留一些系统资源处理可能发生的错误） 6. Shuffle(洗牌)过程详解 Shuffle——&gt;对Map输出结果进行分区，排序，合并等处理并交给Reduce的过程。因此Shuffle分为 Map端操作 Reduce端操作 6.1 Map端的Shuffle过程 (1) 输入数据和执行Map任务 Map的输出数据一班保存在分布式文件系统的文件块中，文件快的格式是任意的可以是文档，也可以是二进制文件。Map任务接受&lt;key,Value&gt;作为输入后，按一定的映射规则转换成一批&lt;Key，Value&gt;作为输出。 (2) 写入缓存 如果每个Map结果都直接写入缓存，会引入很多的次寻址开销。一次Map的结果会先写入缓存，待缓存到一定数据量后再写入磁盘，这样就可以一次寻址，连续写入，大大降减少对磁盘I/O的影响。在写入缓存前，Key和Value值都会被序列化成字节数组 每个Map任务分配一个缓存,MapReduce默认100MB缓存。eg:设置溢写（Spill）比例0.8即当100M大小的缓存被填满80M时，就启动溢写过程，把已经缓存的80M写入磁盘，剩余的20M供Map结果继续写入。但是在溢写到磁盘前，缓存中的数据首先会被分区（Partition）。这些键值对最终需要交给不同的Reduce任务并行处理。MapReduce通过Partition接口对这些键值对进行分区。 分区默认采用的是Hash函数对Key进行哈希后再用Reduce任务的数量进行取模，可以表示成hash(Key) mod R,R–&gt;Reduce任务数量。这样就可以吧Map的输出结果均匀的分配给R个Reduce任务去处理了。（用户也可以通过重载Partition接口来自定义分区方式） 对于每个分区内的键值对，后台线程会根据Key对它们进行内存排序(Sort),排序是MapReduce的默认操作。 排序结束之后有一个可选的操作–合并(Combine)。如果用户事先没有定义Combine则不进行合并操作。如果定义了则在这个时候会执行合并操作。从而减少需要溢写到磁盘的数据量。eg: 两个键值对&lt;“a”,1&gt;和&lt;“a”,1&gt;，如果合并，会得到&lt;“a”,2&gt;,减少了键值对的数量。由于发生在Map端，，所以只能称之为“合并”，从而有别于Reduce.使用Combine决不能改变Reducer任务最终的计算结果。一般而言，累加，最大值等场景可以使用合并操作 在经过分区，排序以及可能发生的合并操作后，这些缓存中的键值对就可以被写入磁盘，并清空缓存。每次溢写操作都会在磁盘中生成一个新的溢写文件，写入溢写文件中的所有键值对是经过分区和排序的（内部有序外部无序） (4) 文件归并 在Map任务结束之前，系统会对所有溢写文件中的数据进行归并(Merge)，生成一个大的溢写文件，这个大的溢写文件中的所有键值对也是经过分区和排序的。所谓归并即两个键值对&lt;“a”,1&gt;和&lt;“a”,1&gt;如果归并，会得到&lt;“a”,&lt;1,1&gt;&gt;。 在进行归并时，如果磁盘中已生成的溢写文件的数量超过参数min.num.spills.for.combine的值时(默认值是3，用户可以自己修改)，那么就可以再次运行Combiner,对数据进行合并操作，从而减少写入磁盘的数据量。但是,如果磁盘中只要一两个溢写文件时，执行合并操作就会“得不偿失”，因为执行合并操作本身也需要代价，因此不会运行Combiner (5) 后续 经过上述的四个步骤后，Map端的Shuffle过程全部完成，最终生成的一个大文件会被存放在本地磁盘上。这个大文件中的数据是被分区的，不同的分区会发送到不同的Reduce任务进行并行处理。JobTracker会一直监测Map任务的执行，当监测到一个Map任务完成后，就会立即通知相关的Reduce任务来“领取”数据，然后开始Reduce端的Shuffle过程。 6.2 Reduce端的Shuffle过程 （1） 领取（Fetch）数据 Reduce任务通过RPC向JobTracker询问Map任务是否已经完成，若完成，则领取数据 Reduce任务通过RPC向JobTracker询问Map任务是否已经完成，若完成，则到该Map任务所在机器上把属于自己处理的分区数据领取(Fetch)到本地磁盘中。一般系统中会存在多个Map机器，因此Reduce任务会使用多线程同时从多个Map机器中领回数据。 （2） 归并数据 Reduce领取数据先放入缓存，来自不同Map机器，先归并，再合并，写入磁盘 多个溢写文件归并成一个或多个大文件，文件中的键值对是排序的 当数据很少时，不需要溢写到磁盘，直接在缓存中归并，然后输出给Reduce Reduce领取数据先放入缓存，如果缓存放满就会像Map端一样被溢写到磁盘。由于此时Reduce阶段的Shuffle还没有真正开始，所以可以把大部分的空间分配给Shuffle过程作为缓存。由于Reduce从多个Map机器中领回属于自己处理的那些分区数据，因此缓存中的数据是来自不同Map机器的，一般会存在很多可以合并（Combine）的键值对。当溢写过程启动的时候，具有相同Key值得键值对会被归并（Merge），如果用户定义了Combine，则归并后的数据还可以执行合并操作，减少写入磁盘的数据量。每个溢写过程结束后，都会在磁盘中生成一个溢写文件，因此磁盘中会存在多个溢写文件。当所有的Map端都被领回时，这些溢写文件会被归并成一个大文件，在归并的同时对键值对进行排序，从而使最终的大文件中的键值对是有序的。在数据很少的情况下，缓存可以存储所有的数据，就不需要吧数据溢写到磁盘上，而是直接在内存中执行归并操作，然后输出给Reduce任务。在磁盘上的多个溢写文件归并成一个大文件可能需要执行多伦归并操作。每轮归并操作可以归并的文件数量由参数io.sort.factor的值控制（默认值是10，可以修改）。假设磁盘中有50个溢写文件，每轮归并10个，则需要归并5轮，得到5个归并后的大文件. (3) 把数据输入给Reduce任务 磁盘中经过多轮归并后得到若干大文件，不会继续归并成一个新的大文件，而是直接输入给Reduce任务，这样可以减少磁盘读写的开销。由此，整个Shuffle过程顺利结束。接下来Reduce会执行Reduce函数中定义 的各种映射，输出最终结果，并保存到分布式文件系统中。 7. 理解Map: 读懂数据 映射为`KV模型`` 并行分布式 计算向数据移动 Reduce： 数据全量/分量加工（partition/group） Reduce中可以包含不同的key 相同的Key汇聚到一个Reduce中 相同的Key调用一次reduce方法排序实现key的汇聚 K,V使用自定义数据类型: 作为参数传递，节省开发成本，提高程序自由度 Writable序列化：使能分布式程序数据交互 Comparable比较器：实现具体排序（字典序，数值序等） MapReduce 1.X版本角色 JobTracker 核心，主，单点 调度所有作业 监控整个集群的资源负载 TaskTracker 从，自身节点资源管理 和JobTracker心跳，汇报资源，获取Task Client 作业单位 规划作业计算分布 提交作业资源到HDFS 最终提交作业到JobTracker MapReduce1.X版本弊端 JobTracker：负载过重，单点故障 资源管理与计算调度强耦合，其他计算框架需要重复实现资源管理 不同框架对资源不能全局管理","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/categories/hadoop/"},{"name":"mapreduce","slug":"hadoop/mapreduce","permalink":"http://www.studyz.club/categories/hadoop/mapreduce/"}],"tags":[{"name":"mapreduce","slug":"mapreduce","permalink":"http://www.studyz.club/tags/mapreduce/"}]},{"title":"vim基本操作","slug":"vim基本操作命令","date":"2019-05-21T15:38:30.929Z","updated":"2019-06-19T07:08:53.632Z","comments":true,"path":"posts/6595f1f0/","link":"","permalink":"http://www.studyz.club/posts/6595f1f0/","excerpt":"","text":"Vim 是 Linux 系统上的最著名的文本/代码编辑器，也是早年的 Vi 编辑器的加强版，而 gVim 则是其 Windows 版。它的最大特色是完全使用键盘命令进行编辑，脱离了鼠标操作虽然使得入门变得困难，但上手之后键盘流的各种巧妙组合操作却能带来极为大幅的效率提升。因此 Vim 和现代的编辑器（如 Sublime Text）有着非常巨大的差异，而且入门学习曲线陡峭，需要记住很多按键组合和命令，如今被看作是高手、Geek们专用的编辑器。尽管 Vim 已经是古董级的软件，但还是有无数新人迎着困难去学习使用，可见其经典与受欢迎程度。另外，由于 Vim 的可配置性非常强，各种插件、语法高亮配色方案等多不胜数，无论作为代码编辑器或是文稿撰写工具都非常给力…… Vim的三种模式 > 1.正常（normal）模式，缺省的编辑模式；下面如果不加特殊说明，提到的命令都直接在正常模式下输入；任何其它模式中都可以通过键盘上的 Esc 键回到正常模式。 2.命令（command）模式，用于执行较长、较复杂的命令；在正常模式下输入“:”（一般命令）、“/”（正向搜索）或“?”（反向搜索）即可进入该模式；命令模式下的命令要输入回车键（Enter）才算完成。 3.插入（insert）模式，输入文本时使用；在正常模式下键入“i”（insert）或“a”（append）即可进入插入模式（也有另外一些命令，如“c”，也可以进入插入模式，但这些命令有其它的作用）。 4.可视（visual）模式，用于选定文本块；可以在正常模式下输入“v”（小写）来按字符选定，输入“V”（大写）来按行选定，或输入“Ctrl-V”来按方块选定。 选择（select）模式，与普通的 Windows 编辑器较为接近的选择文本块的方式；在以可视模式和选择模式之一选定文本块之后，可以使用“Ctrl-G”切换到另一模式——该模式很少在 Linux 上使用。 1. 基本操作1.1 编辑–&gt;输入：i: 在当前光标所在字符的前面，转为输入模式； a: 在当前光标所在字符的后面，转为输入模式； o: 在当前光标所在行的下方，新建一行，并转为输入模式； I：在当前光标所在行的行首，转换为输入模式 A：在当前光标所在行的行尾，转换为输入模式 O：在当前光标所在行的上方，新建一行，并转为输入模式； 1.2 输入–&gt;编辑：ESC 1.3 编辑–&gt;末行：(英文输入法冒号): 1.4 末行–&gt;编辑：ESC, ESC 注：输入模式和末行模式之间不能直接切换，默认处于编辑模式 1.5 关闭文件末行模式关闭文件 123456:q 退出:wq 保存并退出:q! 不保存并退出:w 保存:w! 强行保存:wq --&gt; :x 1.6 保存文件12:w newfile 此时Vi将把当前文件的内容保存到指定的newfile中。:w! newfile 用文件的当前内容替换newfile中原有内容 1.7 使用vim编辑多个文件1234567vim FILE1 FILE2 FILE3:next 切换至下一个文件:prev 切换至前一个文件:last 切换至最后一个文件:first 切换至第一个文件:q退出当前文件:qa 全部退出 1.8 复制,粘贴与删除12345yy|2yy 复制行dd|2dd 删除、剪切行p粘贴到下一行: 1,10d 删除1行至10行。: 1,.d 删除从第一行到光标所在行。 1.9 快速移动123456789101112131415zz 把当前行置为屏幕正中央。zt 把当前行置于屏幕顶端。t意为top。zb 把当前行置于屏幕底端。b意为bottom。gg 到文档首行G 直接跳转到文件结尾行（这是大写G）:58 也是直接到58行^|$定位到行首，行尾dG从当前行删到文档最后一行w 下一个wordW 下一个word(跳过标点符号)b 前一个worde 跳到目前word的尾端0 跳到目前行的开头^ 跳到目前行第一个非空白字元$ 跳到行尾 2.0 搜寻与查找123456&#x2F;application 查找application出现在第几行？&#x2F;xxxx 搜寻xxxx# 往前搜寻目前游标所在的字(word)* 往後搜寻目前游标所在的字(word)fx 在目前行往后搜寻字元xgd 跳到目前游标所在的字(word)的定义位置 2.1 撤销与替换1234:16,31s/application/mei/gc 把16~31行之间的application替换为mei，并且在替换的时候给提示:16,31s/application/mei/g 不给提示替换(c表示confirm)u 撤销（反悔了，撤销操作）；或者:q!不保存，那么所有的操作都不保存了&lt;Ctrl&gt;-R - 重做，反转撤消，即它是对撤消的撤消。 2.1 分割视窗:split 分割視窗(可加檔名順便開啟另一檔案) :diffsplit xxx 以分割視窗和檔案xxx做比較(diff) Ctrl-W p 跳到前一個分割視窗(在兩個分割窗來回切換) Ctrl-W j 跳到下面的分割窗 Ctrl-W h 跳到左邊的分割窗 Ctrl-W k 跳到上面的分割窗 Ctrl-W l 跳到右邊的分割窗 12#### 2.2 路径 cd .. //回到上一路径 cd . cd //回到根目录 cd - //回到上一个目录类似于撤回操作 ```","categories":[{"name":"vim","slug":"vim","permalink":"http://www.studyz.club/categories/vim/"}],"tags":[{"name":"vim","slug":"vim","permalink":"http://www.studyz.club/tags/vim/"}]},{"title":"markdown语法","slug":"Markdown基本语法","date":"2019-05-21T15:38:30.927Z","updated":"2019-06-19T07:08:35.816Z","comments":true,"path":"posts/32f3548e/","link":"","permalink":"http://www.studyz.club/posts/32f3548e/","excerpt":"","text":"这篇文章将会介绍简单的md语法 Markdown语法的基本介绍1.1标题123456# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题 预览： 一级标题二级标题三级标题四级标题五级标题六级标题1.2标签语法：TAGS：内容 或直接标签：内容 ，即 在编辑区任意行的列首位置输入tags/标签：关键词1 关键词2 ， 多个用空格隔开。 示例: 123标签：教程 Markdown或TAGS: 教程 Markdown 1.3 目录（大纲）导出语法 在段落中填写 [TOC] （toc小写也可以）以显示各标题为结构的目录（大纲）导航。示例： 1[TOC] 1.4 强调或突出1.4.1斜体语法： 斜体 或 斜体 在文字两边加 单下底线 或 单星号示例：World World 1.5 删除线语法:~~ ~~ 在文字两边加 双波浪号(快速记忆：单斜双粗两波删)示例: 删除线 1~~删除线~~ 1.6 下划线语法:在空白行下方添加三条“-”横线。（前面讲过在文字下方添加“-”，实现的2级标题）示例: 123空白行下面才可以，不然会变成二级标题。--- 1.7 段落与分割1.7.1 普通段落的分割语法:用空行分隔，连着多个空行也是一个效果，只能空一行的间距。 无论空几行有几个空格只显示一个，与html类似。 1.7.2 水平分割线语法: 个及以上的 星号***或 减号—或 下底线___来表示。示例: 123***---___ 1.7.3 强制换行语法: 在行尾输入两个空格示例:jjffdd 123jjff //这里留了两个空行dd 2.0 Markdown中级语法教程2.1 链接2.1.1 网页链接（内联风格）1##### 语法: [链接文字](http://链接地址&quot;（空格）鼠标悬停显示文字（可选）&quot;)，&quot;（空格）鼠标悬停显示文字&quot;非必须项，在pc浏览器中能看到效果，手机移动端因无鼠标效果无法直接体现。 示例:小智的博客 1[小智的博客](http://studyz.club &quot;千万不要点呀！&quot;) 2.1.2 链接的快速定义语法:1234567前面文档中引用链接的语法为：[链接文字][id]前后id要匹配一致，一般为数字，这样可以在文档的其他任意地方快速插入这个链接。后面定义链接的语法为：[id]:（空格）http://链接地址（空格）&quot;鼠标悬停显示文字（可选）&quot;后面的语法必须在列首并单独一行，适合前期不知道网址，后面再加入的情况。 示例: 阿智的博客 123[阿智的博客][1]//注意中间有东西[1]: http://studyz.club &quot;千万不要点啊!&quot; 2.13 链接的自动展示语法: 你也可以直接将这个链接打出来示例:http://studyz.club 2.2 列表2.2.1 有序列表语法: 直接数字加英文句点加空格（最多三个空格）和文字。示例: 列表1 列表2 121. 列表1 //这后面加两个空格就会变成层次列表2. 列表2 列表1 1列表1.1 2.2.2 无序列表语法: 文字前面加 星号空格*（空格）或 短横空格-（空格）或 加号+（空格）表示。示例: 列表1 列表2 列表3 123* 列表1- 列表2+ 列表3 2.2.3 多重列表语法: 单层列表项缩进1个空格就可以往后面叠加子层的列表示例： 首层列表项（有序） 第二层列表项1(嵌套的列表可以是有序的 ) 第二层列表项2(格式和正常的有序、无序列表没有差异) 第三层列表项1 第三层列表项2 第四层列表项1 第五层列表项2 12345671. 首层列表项（有序）2. 第二层列表项1(嵌套的列表可以是有序的 )3. 第二层列表项2(格式和正常的有序、无序列表没有差异)4. 第三层列表项15. 第三层列表项2 //这里有两个空格1.第四层列表项1 //这里有两个空格1.第五层列表项2 2.3 引用2.3.1 文字引用语法: 段落前面添加大于号和空格&gt; （空格），就能够形成引用段落。示例1: 湖心亭看雪 张岱 崇祯五年十二月，余住西湖。大雪三日，湖中人鸟声俱绝。是日更定矣，余拏一小舟，拥毳衣炉火，独往湖心亭看雪。雾凇沆砀，天与云与山与水，上下一白。湖上影子，惟长堤一痕、湖心亭一点，与余舟一芥、舟中人两三粒而已。 到亭上，有两人铺毡对坐，一童子烧酒炉正沸。见余，大喜曰：“湖中焉得更有此人！”拉余同饮。余强饮三大白而别。问其姓氏，是金陵人，客此。及下船，舟子喃喃曰：“莫说相公痴，更有痴似相公者！” 1234&gt; &lt;center&gt;湖心亭看雪 张岱&lt;/center&gt;崇祯五年十二月，余住西湖。大雪三日，湖中人鸟声俱绝。是日更定矣，余拏一小舟，拥毳衣炉火，独往湖心亭看雪。雾凇沆砀，天与云与山与水，上下一白。湖上影子，惟长堤一痕、湖心亭一点，与余舟一芥、舟中人两三粒而已。到亭上，有两人铺毡对坐，一童子烧酒炉正沸。见余，大喜曰：“湖中焉得更有此人！”拉余同饮。余强饮三大白而别。问其姓氏，是金陵人，客此。及下船，舟子喃喃曰：“莫说相公痴，更有痴似相公者！” 2.3.2 多层引用 这是第一层的文字引用 这是第二层嵌套的引用 这是第三层的嵌套的引用 123&gt; 这是第一层的文字引用 &gt; &gt; 这是第二层嵌套的引用&gt; &gt; &gt; 这是第三层的嵌套的引用 2.4 添加影像图片2.4.1 插入图片语法:1使用 ![图片描述](图片网络/本地图片地址（空格）“鼠标悬停显示文字（可选）”) 示例： 1![红猪](Markdown基本语法/timg.jpg &quot;千万不要啊！&quot;) 2.4.2 图片链接的定义语法:12345[id]:（空格）/url/to/img.jpg&quot;鼠标悬停显示文字（可选）&quot;后面定义链接的语法为：![图片描述][id]：本地/网络图片地址 &quot;鼠标悬停显示文字（可选）&quot;后面的语法必须在列首并单独一行 12[2]: Markdown基本语法/timg.jpg &quot;千万要啊!&quot;![红猪][2] 2.5 代码块2.5.1 行内(内联)代码语法: 使用内容 表示行内 代码块。即 在文字左右使用反引号包含（键盘数字“1”前面的波浪号英文输入状态时能打出）。示例:莫说相公痴，更有痴似相公者! 1莫说相公痴，更有`痴`似相公者! 2.5.2 段落文本代码块语法: 每行缩进4个空格或 1个 Tab表示这是一个代码块，此行左侧有四个不可见的空格。这是代码块，会自动添加行号，注意要与前面文字空行，不然没有引用效果，就是这么简单，非常实用呀。 2.5.3 代码块语法:1上下三个反引号tab键上面英文输入时可打出，上面三个可以填语言类型。 2.6 表格语法: 第一行文字粗体表示每列名称，在下方加至少3个横杠-符。用|分隔各列，不需要手动对齐。表格中的元素可以使用粗体、斜体、高亮等。使用反斜杠\\可以对符号转义。三个或以上下划线_或者|中划线-可以画一条横线，但是中划线要注意与二级标题区分开。 一行一列标题 一行二列标题 一行三列标题 二行一列 二行二列 二行三列 三行一列 三行二列 三行三列 1234一行一列标题|一行二列标题|一行三列标题---|---|---二行一列|二行二列|二行三列三行一列|三行二列|三行三列 3. 制作页内跳转，目录12345678目录用这种格式[5.3 Client](#1)标题用### &lt;h2 id&#x3D;&quot;1&quot;&gt;5.3 Client&lt;&#x2F;h2&gt;### &lt;a id&#x3D;&quot;3&quot;&gt;5.1 YARN：解耦资源与计算&lt;&#x2F;a&gt;","categories":[{"name":"markdown","slug":"markdown","permalink":"http://www.studyz.club/categories/markdown/"}],"tags":[{"name":"markdown","slug":"markdown","permalink":"http://www.studyz.club/tags/markdown/"}]},{"title":"CentOS 7.4配置ssh免密登录及出现的问题","slug":"CentOS 7.4配置ssh免密登录及出现的问题","date":"2019-05-21T15:38:30.924Z","updated":"2019-06-19T06:06:46.801Z","comments":true,"path":"posts/bd44a056/","link":"","permalink":"http://www.studyz.club/posts/bd44a056/","excerpt":"","text":"1 配置思路通过RSA加密算生成了密钥，包括私钥和公钥，我们把公钥追加到用来认证授权的key中去。每台机器配置本地免密登录，然后将其余每台机器生成的~/.ssh/id_dsa.pub公钥内容追加到其中一台主机的authorized_keys中，然后将这台机器中包括每台机器公钥的authorized_keys文件发送到集群中所有的服务器。这样集群中每台服务器都拥有所有服务器的公钥，这样集群间任意两台机器都可以实现免密登录了。配置四台虚拟机主机： master从机： slave1从机： slave2从机： slave3这四台虚拟机可以在VMware中配置好一台后进行复制，本篇所采用的的机器已经配置完jdk注：如果机器配置不够，仅仅为了学习的话可以不用配置那么多虚拟机 2 第一台机器(master)的配置注意留意命令所处的文件夹！！要不然容易出错。（.ssh目录）2.1 生成公钥和密钥12// 生成公钥和密钥ssh-keygen -t rsa ![生成公钥和密钥](CentOS 7.4配置ssh免密登录及出现的问题/生成公钥和密钥.png) 2.2 把公钥文件放入授权文件中12// 把公钥文件放入授权文件中cat id_rsa.pub &gt;&gt; authorized_keys ![把公钥文件放入授权文件中](CentOS 7.4配置ssh免密登录及出现的问题/把公钥文件放入授权文件中.png) 2.3 写入认证文件12// 写入认证文件scp ~/.ssh/authorized_keys slave1:~/.ssh/ 注意这里写的是slave1不是主机名master如果是在slave1这个节点为初始状态时（未进行任何的ssh操作之前，根目录下没有.ssh目录）此时需要手动在slave1节点上，进行一次ssh操作，如：ssh slave1![在slave1中创建.ssh文件](CentOS 7.4配置ssh免密登录及出现的问题/在slave1中创建ssh文件.png)然后再执行这个命令会得到![查看认证文件](CentOS 7.4配置ssh免密登录及出现的问题/查看认证文件.png) 3 第二台机器(node1)3.1写入公钥和密钥1ssh-keygen -t rsa ![生成公钥和密钥](CentOS 7.4配置ssh免密登录及出现的问题/生成公钥和密钥2.png) 3.2 同上设置12cat id_rsa.pub &gt;&gt; authorized_keysscp ~/.ssh/authorized_keys slave2:~/.ssh/ 此时可能会出现同上问题 4 第三台机器(node2)1234//同样的设置 也会出现相同的问题 就会有相同的解决方法 此时你应该能熟练的解决这个问题了ssh-keygen -t rsacat id_rsa.pub &gt;&gt; authorized_keysscp ~/.ssh/authorized_keys slave3:~/.ssh/ 5 第四台机器(node3)1234567ssh-keygen -t rsacat id_rsa.pub &gt;&gt; authorized_keysscp ~/.ssh/authorized_keys master:~/.ssh/scp ~/.ssh/authorized_keys slave1:~/.ssh/scp ~/.ssh/authorized_keys slave2:~/.ssh/ 6 免密检查12//注意自己当前所处的目录在哪cat /.ssh/known_hosts ![免密检查](CentOS 7.4配置ssh免密登录及出现的问题/免密检查.png)如果没有的话，比如master没有，则需要在该机器上重新执行一下ssh master命令，让其生成以上信息即可。![ssh连接主机可生成known_hosts文件内容](CentOS 7.4配置ssh免密登录及出现的问题/ssh连接主机可生成known_hosts文件内容.png) 7 在node4中测试连接![链接测试](CentOS 7.4配置ssh免密登录及出现的问题/链接测试.png) 8 可能出现的问题 8.1 如果# ssh master出现ssh: Could not resolve hostname ./id_rsa: Name or service not known 12//检查能否ping从机的IP地址ping 192.168.56.202 如果能ping通说明配置是没问题的 1vi /etc/hosts 在hosts文件中加入IP地址和主机名，实现两者的映射关系 123127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.56.202 slave2 8.2 如果用ssh时 提示WARNING: POSSIBLE DNS SPOOFING DETECTED 123456789101112131415161718192021@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: POSSIBLE DNS SPOOFING DETECTED! @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@The RSA host key for www.163.net has changed,and the key for the according IP address 158.252.167.117is unknown. This could either mean thatDNS SPOOFING is happening or the IP address for the hostand its host key have changed at the same time.@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!Someone could be eavesdropping on you right now (man-in-the-middle attack)!It is also possible that the RSA host key has just been changed.The fingerprint for the RSA key sent by the remote host is11:8e:a8:b4:91:95:8b:15:82:b8:71:de:a7:c1:c0:aa.Please contact your system administrator.Add correct host key in &#x2F;home&#x2F;fuck&#x2F;.ssh&#x2F;known_hosts to get rid of this message.Offending key in &#x2F;home&#x2F;fuck&#x2F;.ssh&#x2F;known_hosts:1RSA host key for www.163.net has changed and you have requested strict checking.Host key verification failed. 找到.ssh文件里的known_hosts文件 12//在.ssh目录下执行删除操作然后再ssh即可rm known_hosts 8.3 各虚拟机之间不能够免密登录 123//在.ssh目录下将密钥拷贝到各个机器中[root@node2 .ssh]# ssh-copy-id hadoop","categories":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/categories/hadoop/"},{"name":"ssh","slug":"hadoop/ssh","permalink":"http://www.studyz.club/categories/hadoop/ssh/"}],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/tags/hadoop/"},{"name":"ssh免密登录","slug":"ssh免密登录","permalink":"http://www.studyz.club/tags/ssh%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/"}]}],"categories":[{"name":"算法练习","slug":"算法练习","permalink":"http://www.studyz.club/categories/%E7%AE%97%E6%B3%95%E7%BB%83%E4%B9%A0/"},{"name":"二叉树","slug":"算法练习/二叉树","permalink":"http://www.studyz.club/categories/%E7%AE%97%E6%B3%95%E7%BB%83%E4%B9%A0/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"name":"Pythong实战-使用GUI框架","slug":"Pythong实战-使用GUI框架","permalink":"http://www.studyz.club/categories/Pythong%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8GUI%E6%A1%86%E6%9E%B6/"},{"name":"Pythong实战-操作数据库","slug":"Pythong实战-操作数据库","permalink":"http://www.studyz.club/categories/Pythong%E5%AE%9E%E6%88%98-%E6%93%8D%E4%BD%9C%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"大数据流处理框架Storm","slug":"大数据流处理框架Storm","permalink":"http://www.studyz.club/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6Storm/"},{"name":"内存计算框架spark","slug":"内存计算框架spark","permalink":"http://www.studyz.club/categories/%E5%86%85%E5%AD%98%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6spark/"},{"name":"数据采集与清洗","slug":"数据采集与清洗","permalink":"http://www.studyz.club/categories/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E4%B8%8E%E6%B8%85%E6%B4%97/"},{"name":"Scala","slug":"Scala","permalink":"http://www.studyz.club/categories/Scala/"},{"name":"IDEA","slug":"IDEA","permalink":"http://www.studyz.club/categories/IDEA/"},{"name":"win10+php环境搭建","slug":"win10-php环境搭建","permalink":"http://www.studyz.club/categories/win10-php%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"},{"name":"hadoop-hive-DDL-DML","slug":"hadoop-hive-DDL-DML","permalink":"http://www.studyz.club/categories/hadoop-hive-DDL-DML/"},{"name":"Hive","slug":"Hive","permalink":"http://www.studyz.club/categories/Hive/"},{"name":"Hadoop-Hive","slug":"Hive/Hadoop-Hive","permalink":"http://www.studyz.club/categories/Hive/Hadoop-Hive/"},{"name":"mysql","slug":"mysql","permalink":"http://www.studyz.club/categories/mysql/"},{"name":"centos7-mysql8","slug":"mysql/centos7-mysql8","permalink":"http://www.studyz.club/categories/mysql/centos7-mysql8/"},{"name":"hadoop-hive","slug":"hadoop-hive","permalink":"http://www.studyz.club/categories/hadoop-hive/"},{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/categories/hadoop/"},{"name":"hbase-Hive","slug":"hadoop/hbase-Hive","permalink":"http://www.studyz.club/categories/hadoop/hbase-Hive/"},{"name":"hbase-","slug":"hbase","permalink":"http://www.studyz.club/categories/hbase/"},{"name":"hbase-MapReduce","slug":"hadoop/hbase-MapReduce","permalink":"http://www.studyz.club/categories/hadoop/hbase-MapReduce/"},{"name":"hbaseAPI","slug":"hadoop/hbaseAPI","permalink":"http://www.studyz.club/categories/hadoop/hbaseAPI/"},{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"},{"name":"Maven项目本地仓库配置","slug":"软件开发框架/Maven项目本地仓库配置","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/Maven%E9%A1%B9%E7%9B%AE%E6%9C%AC%E5%9C%B0%E4%BB%93%E5%BA%93%E9%85%8D%E7%BD%AE/"},{"name":"SSM","slug":"软件开发框架/SSM","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/SSM/"},{"name":"利用SpringMVC做一个符合REST风格的CRUD","slug":"软件开发框架/利用SpringMVC做一个符合REST风格的CRUD","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/%E5%88%A9%E7%94%A8SpringMVC%E5%81%9A%E4%B8%80%E4%B8%AA%E7%AC%A6%E5%90%88REST%E9%A3%8E%E6%A0%BC%E7%9A%84CRUD/"},{"name":"SpringMVC-视图解析","slug":"软件开发框架/SpringMVC-视图解析","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/SpringMVC-%E8%A7%86%E5%9B%BE%E8%A7%A3%E6%9E%90/"},{"name":"SpringMVC-数据输出","slug":"软件开发框架/SpringMVC-数据输出","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/SpringMVC-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%87%BA/"},{"name":"SpringMVC-请求处理","slug":"软件开发框架/SpringMVC-请求处理","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/SpringMVC-%E8%AF%B7%E6%B1%82%E5%A4%84%E7%90%86/"},{"name":"SpringMVC-REST","slug":"软件开发框架/SpringMVC-REST","permalink":"http://www.studyz.club/categories/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/SpringMVC-REST/"},{"name":"hbase","slug":"hadoop/hbase","permalink":"http://www.studyz.club/categories/hadoop/hbase/"},{"name":"hbase基本操作","slug":"hadoop/hbase/hbase基本操作","permalink":"http://www.studyz.club/categories/hadoop/hbase/hbase%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"},{"name":"Python数据分析与应用","slug":"Python数据分析与应用","permalink":"http://www.studyz.club/categories/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%BA%94%E7%94%A8/"},{"name":"numpy","slug":"Python数据分析与应用/numpy","permalink":"http://www.studyz.club/categories/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%BA%94%E7%94%A8/numpy/"},{"name":"数据挖据概念与技术","slug":"数据挖据概念与技术","permalink":"http://www.studyz.club/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8D%AE%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%8A%80%E6%9C%AF/"},{"name":"分布式数据库","slug":"分布式数据库","permalink":"http://www.studyz.club/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"计算机网络","slug":"计算机网络","permalink":"http://www.studyz.club/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"mapreduce","slug":"hadoop/mapreduce","permalink":"http://www.studyz.club/categories/hadoop/mapreduce/"},{"name":"eclipse","slug":"hadoop/eclipse","permalink":"http://www.studyz.club/categories/hadoop/eclipse/"},{"name":"hdfs","slug":"hadoop/hdfs","permalink":"http://www.studyz.club/categories/hadoop/hdfs/"},{"name":"yarn","slug":"hadoop/yarn","permalink":"http://www.studyz.club/categories/hadoop/yarn/"},{"name":"vim","slug":"vim","permalink":"http://www.studyz.club/categories/vim/"},{"name":"markdown","slug":"markdown","permalink":"http://www.studyz.club/categories/markdown/"},{"name":"ssh","slug":"hadoop/ssh","permalink":"http://www.studyz.club/categories/hadoop/ssh/"}],"tags":[{"name":"算法练习","slug":"算法练习","permalink":"http://www.studyz.club/tags/%E7%AE%97%E6%B3%95%E7%BB%83%E4%B9%A0/"},{"name":"二叉树","slug":"二叉树","permalink":"http://www.studyz.club/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"name":"Pythong实战-使用GUI框架","slug":"Pythong实战-使用GUI框架","permalink":"http://www.studyz.club/tags/Pythong%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8GUI%E6%A1%86%E6%9E%B6/"},{"name":"Pythong实战-操作数据库","slug":"Pythong实战-操作数据库","permalink":"http://www.studyz.club/tags/Pythong%E5%AE%9E%E6%88%98-%E6%93%8D%E4%BD%9C%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"大数据流处理框架Storm","slug":"大数据流处理框架Storm","permalink":"http://www.studyz.club/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%B5%81%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6Storm/"},{"name":"内存计算框架spark","slug":"内存计算框架spark","permalink":"http://www.studyz.club/tags/%E5%86%85%E5%AD%98%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6spark/"},{"name":"数据采集与清洗","slug":"数据采集与清洗","permalink":"http://www.studyz.club/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E4%B8%8E%E6%B8%85%E6%B4%97/"},{"name":"Scala","slug":"Scala","permalink":"http://www.studyz.club/tags/Scala/"},{"name":"Scala学习基础知识","slug":"Scala学习基础知识","permalink":"http://www.studyz.club/tags/Scala%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"name":"IDEA常用快捷键","slug":"IDEA常用快捷键","permalink":"http://www.studyz.club/tags/IDEA%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/"},{"name":"Scala学习-环境的搭建和入门","slug":"Scala学习-环境的搭建和入门","permalink":"http://www.studyz.club/tags/Scala%E5%AD%A6%E4%B9%A0-%E7%8E%AF%E5%A2%83%E7%9A%84%E6%90%AD%E5%BB%BA%E5%92%8C%E5%85%A5%E9%97%A8/"},{"name":"win10+php环境搭建","slug":"win10-php环境搭建","permalink":"http://www.studyz.club/tags/win10-php%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"},{"name":"-hive","slug":"hive","permalink":"http://www.studyz.club/tags/hive/"},{"name":"Hadoop-Hive","slug":"Hadoop-Hive","permalink":"http://www.studyz.club/tags/Hadoop-Hive/"},{"name":"centos7-mysql8","slug":"centos7-mysql8","permalink":"http://www.studyz.club/tags/centos7-mysql8/"},{"name":"-hbase - hbase-Hive","slug":"hbase-hbase-Hive","permalink":"http://www.studyz.club/tags/hbase-hbase-Hive/"},{"name":"集群的时间同步","slug":"集群的时间同步","permalink":"http://www.studyz.club/tags/%E9%9B%86%E7%BE%A4%E7%9A%84%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5/"},{"name":"-hbase","slug":"hbase","permalink":"http://www.studyz.club/tags/hbase/"},{"name":"-hbase - hbase-Mapreduce","slug":"hbase-hbase-Mapreduce","permalink":"http://www.studyz.club/tags/hbase-hbase-Mapreduce/"},{"name":"-hbase - hbaseAPI","slug":"hbase-hbaseAPI","permalink":"http://www.studyz.club/tags/hbase-hbaseAPI/"},{"name":"软件开发框架","slug":"软件开发框架","permalink":"http://www.studyz.club/tags/%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/"},{"name":"Maven项目本地仓库配置","slug":"Maven项目本地仓库配置","permalink":"http://www.studyz.club/tags/Maven%E9%A1%B9%E7%9B%AE%E6%9C%AC%E5%9C%B0%E4%BB%93%E5%BA%93%E9%85%8D%E7%BD%AE/"},{"name":"SSM","slug":"SSM","permalink":"http://www.studyz.club/tags/SSM/"},{"name":"利用SpringMVC做一个符合REST风格的CRUD","slug":"利用SpringMVC做一个符合REST风格的CRUD","permalink":"http://www.studyz.club/tags/%E5%88%A9%E7%94%A8SpringMVC%E5%81%9A%E4%B8%80%E4%B8%AA%E7%AC%A6%E5%90%88REST%E9%A3%8E%E6%A0%BC%E7%9A%84CRUD/"},{"name":"SpringMVC-视图解析","slug":"SpringMVC-视图解析","permalink":"http://www.studyz.club/tags/SpringMVC-%E8%A7%86%E5%9B%BE%E8%A7%A3%E6%9E%90/"},{"name":"SpringMVC-数据输出","slug":"SpringMVC-数据输出","permalink":"http://www.studyz.club/tags/SpringMVC-%E6%95%B0%E6%8D%AE%E8%BE%93%E5%87%BA/"},{"name":"SpringMVC-请求处理","slug":"SpringMVC-请求处理","permalink":"http://www.studyz.club/tags/SpringMVC-%E8%AF%B7%E6%B1%82%E5%A4%84%E7%90%86/"},{"name":"SpringMVC-REST","slug":"SpringMVC-REST","permalink":"http://www.studyz.club/tags/SpringMVC-REST/"},{"name":"-hbase - hbase基本操作","slug":"hbase-hbase基本操作","permalink":"http://www.studyz.club/tags/hbase-hbase%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"},{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://www.studyz.club/tags/SpringMVC/"},{"name":"Python数据分析与应用","slug":"Python数据分析与应用","permalink":"http://www.studyz.club/tags/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%BA%94%E7%94%A8/"},{"name":"Python numpy","slug":"Python-numpy","permalink":"http://www.studyz.club/tags/Python-numpy/"},{"name":"Python","slug":"Python","permalink":"http://www.studyz.club/tags/Python/"},{"name":"-数据挖掘概念与技术  -引论","slug":"数据挖掘概念与技术-引论","permalink":"http://www.studyz.club/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%8A%80%E6%9C%AF-%E5%BC%95%E8%AE%BA/"},{"name":"hbase","slug":"hbase","permalink":"http://www.studyz.club/tags/hbase/"},{"name":"-ssm -MyBatis - MyBatis缓存机制","slug":"ssm-MyBatis-MyBatis缓存机制","permalink":"http://www.studyz.club/tags/ssm-MyBatis-MyBatis%E7%BC%93%E5%AD%98%E6%9C%BA%E5%88%B6/"},{"name":"-分布式数据库  -Hbase简介","slug":"分布式数据库-Hbase简介","permalink":"http://www.studyz.club/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93-Hbase%E7%AE%80%E4%BB%8B/"},{"name":"-ssm -MyBatis - MyBatis动态sql","slug":"ssm-MyBatis-MyBatis动态sql","permalink":"http://www.studyz.club/tags/ssm-MyBatis-MyBatis%E5%8A%A8%E6%80%81sql/"},{"name":"-ssm -Spring MyBatis的关联映射","slug":"ssm-Spring-MyBatis的关联映射","permalink":"http://www.studyz.club/tags/ssm-Spring-MyBatis%E7%9A%84%E5%85%B3%E8%81%94%E6%98%A0%E5%B0%84/"},{"name":"-ssm -Spring -MyBatis","slug":"ssm-Spring-MyBatis","permalink":"http://www.studyz.club/tags/ssm-Spring-MyBatis/"},{"name":"-ssm -Spring - spring-事务管理","slug":"ssm-Spring-spring-事务管理","permalink":"http://www.studyz.club/tags/ssm-Spring-spring-%E4%BA%8B%E5%8A%A1%E7%AE%A1%E7%90%86/"},{"name":"-ssm -Spring - spring-jdbc","slug":"ssm-Spring-spring-jdbc","permalink":"http://www.studyz.club/tags/ssm-Spring-spring-jdbc/"},{"name":"ssm","slug":"ssm","permalink":"http://www.studyz.club/tags/ssm/"},{"name":"Spring","slug":"Spring","permalink":"http://www.studyz.club/tags/Spring/"},{"name":"AOP","slug":"AOP","permalink":"http://www.studyz.club/tags/AOP/"},{"name":"Bean","slug":"Bean","permalink":"http://www.studyz.club/tags/Bean/"},{"name":"计算机网络","slug":"计算机网络","permalink":"http://www.studyz.club/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"网络层","slug":"网络层","permalink":"http://www.studyz.club/tags/%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"name":"数据链路层","slug":"数据链路层","permalink":"http://www.studyz.club/tags/%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"name":"物理层","slug":"物理层","permalink":"http://www.studyz.club/tags/%E7%89%A9%E7%90%86%E5%B1%82/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://www.studyz.club/tags/mapreduce/"},{"name":"wordcount","slug":"wordcount","permalink":"http://www.studyz.club/tags/wordcount/"},{"name":"hdfs","slug":"hdfs","permalink":"http://www.studyz.club/tags/hdfs/"},{"name":"win7","slug":"win7","permalink":"http://www.studyz.club/tags/win7/"},{"name":"eclipse","slug":"eclipse","permalink":"http://www.studyz.club/tags/eclipse/"},{"name":"hadoop","slug":"hadoop","permalink":"http://www.studyz.club/tags/hadoop/"},{"name":"api","slug":"api","permalink":"http://www.studyz.club/tags/api/"},{"name":"yarn","slug":"yarn","permalink":"http://www.studyz.club/tags/yarn/"},{"name":"ha","slug":"ha","permalink":"http://www.studyz.club/tags/ha/"},{"name":"vim","slug":"vim","permalink":"http://www.studyz.club/tags/vim/"},{"name":"markdown","slug":"markdown","permalink":"http://www.studyz.club/tags/markdown/"},{"name":"ssh免密登录","slug":"ssh免密登录","permalink":"http://www.studyz.club/tags/ssh%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/"}]}